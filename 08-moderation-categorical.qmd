```{r setup, include=FALSE}
source("R_setup.R")
```

# Regression Analysis And A Categorical Moderator {#sec-moderationcat}

> Key concepts: regression equation, dummy variables, normally distributed residuals, linearity, homoscedasticity, independent observations, statistical diagram, interaction variable, covariate, common support, simple slope, conditional effect.

Watch this micro lecture on regression analysis with a categorical moderator for an overview of the chapter (@vid-intro-moderation).

::: {#vid-intro-moderation}

{{< video https://www.youtube.com/embed/lDkGyTvPzOY
    width="100%"
    height="360" 
>}}

Introduction to moderation with categorical variables.
:::

### Summary {.unnumbered}

::: {.callout-important appearance="simple"}

My dependent variable is numerical but at least one predictor is also numerical, so I cannot apply analysis of variance. How can I investigate moderation with regression analysis?

:::

The linear regression model is a powerful and very popular tool for predicting a numerical dependent variable from one or more independent variables. In this chapter, we use regression analysis to evaluate the effects of an anti-smoking campaign. We predict attitude towards smoking from exposure to the anti-smoking campaign (numerical), time spent with smokers (numerical), and the respondent's smoking status (categorical).

Regression coefficients, that is, the slopes of regression lines, are the effects in a regression model. They show the predicted difference in the dependent variable for a one unit difference in the independent variable (exposure, time spent with smokers) or the predicted mean difference for two categories (smokers versus non-smokers).

But what if the predictive effect is not the same in all contexts? For example, exposure to an anti-smoking campaign may generally generate a more negative attitude towards smoking. The effect, however, is probably different for people who smoke than for people who do not smoke. In this case, the effect of campaign exposure on attitude towards smoking is moderated by context: Whether or not the person exposed to the campaign is a smoker.

Different effect sizes for different contexts are different regression coefficients for different contexts. We need different regression lines for different groups of people. We can use an interaction variable as an independent variable in a regression model to accommodate for moderation as different effects. An interaction variable is just the product of the predictor variable and the moderator variable.

As an independent variable in the model, the regression coefficient of an interaction variable (interaction effect for short) has a confidence interval and a *p* value. The confidence interval tells us the plausible values for the size of the interaction effect in the population. The *p* value tests the null hypothesis that there is no interaction effect at all in the population.

To interpret the interaction effect, we must determine the size of the effect of the predictor on the dependent variable for each group of the moderator. For example, the effect of campaign exposure on smoking attitude for smokers and the effect for non-smokers.

An interaction effect in a regression model closely resembles an interaction effect in analysis of variance. The effect of a single predictor that is involved in an interaction effect in a regression model, however, is not a main effect as in analysis of variance. It is a conditional effect, namely the effect for one particular value of the moderator, that is, the effect within one particular context. To understand this, we must pay close attention to the regression equation.

## Regression Analysis {#sec-regression-equation}

In the social sciences, we usually expect that a particular outcome has several causes. Investigating the effects of an anti-smoking campaign, for instance, we would not assume that a person's attitude towards smoking depends only on exposure to a particular anti-smoking campaign. It is easy to think of other and perhaps more influential causes such as personal smoking status, contact with people who do or do not smoke, susceptibility to addiction, and so on. Regression analysis with multiple predictor variables is called a *multiple regression analysis*. 


```{r}
#| label: fig-concept-smoke
#| fig-cap: "A conceptual model with some hypothesized causes of attitude towards smoking."
#| echo: false
#| warning: false

# Draw conceptual diagram: Attitude towards smoking predicted by Exposure, Smoking status, and Contact with smokers.
library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(0.3, 0.3, 0.3, 0.7), 
                        y = c(.1, .3, .5, .3),
                        label = c("Exposure", "Smoking Status", "Contact with Smokers", "Attitude"))
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = x[1], y = y[1], xend = x[4] - 0.04, yend = y[4] - 0.02), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[2], y = y[2], xend = x[4] - 0.04, yend = y[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[3], y = y[3], xend = x[4] - 0.04, yend = y[4] + 0.02), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.05, 0.55)) +
  theme_void()
# Cleanup.
rm(variables)
```


@fig-concept-smoke summarizes some hypothesized causes of the attitude towards smoking. Attitude towards smoking is measured as a scale, so it is a numerical variable. In linear regression, the dependent variable ($y$) must be numerical and in principle continuous. There are regression models for other types of dependent variables, for instance, logistic regression for a dichotomous (0/1) dependent variable and Poisson regression for a count dependent variable, but we will not discuss these models.

### The regression equation

A regression model translates this conceptual diagram into a statistical model. The statistical regression model is a mathematical function with the dependent variable (also known as the outcome variable, usually referred to with the letter $y$) as the sum of a constant, the effects ($b$) of independent variables or predictors ($x$), which are *predictive effects*, and an error term ($e$), which is also called the *residuals*, see Equation @eq-regression.

$$
\small
  y = constant + b_1*x_1 + b_2*x_2 + b_3*x_3 + e 
\normalsize
$$ {#eq-regression}

If we want to predict the dependent variable ($y$), we ignore the error term ($e$) in the equation, and we indicate the prediction by adding a hat to outcome variable $\hat{y}$. The equation without the error term [Eq. @eq-regressionpred] represents the regression line that we visualize and interpret in the following subsections. We use the error term only when we discuss the assumptions for statistical inference on a regression model in @sec-regr-inference.

$$
\small
  \hat{y} = constant + b_1*x_1 + b_2*x_2 + b_3*x_3 
\normalsize
$$ {#eq-regressionpred}

Though it is conceptually relevant to understand that the error term represents the difference between what the model predicts and what we actually observe. 

$$
\small
  \epsilon = y - \hat{y} 
\normalsize
$$ {#eq-error}

From @eq-error we can see that the error, also called the residual, is the difference between the observed value of the dependent variable ($y$) and the predicted value ($\hat{y}$). Thus, the better our model predicts the dependent outcome variable, the smaller the residuals.

The following sections will explain the regression equation in more detail. We will first discuss the regression equation with a single numerical predictor, then we will discuss the regression equation with a single dichotomous predictor, and finally we will discuss the regression equation with a categorical predictor. The latter requires some tricks to be able to include it in a regression model.

### A numerical predictor

Let us first have a close look at a *simple regression equation*, that is, a regression equation with just one predictor ($x$). Let us try to predict attitude towards smoking from exposure to an anti-smoking campaign.

::: {#fig-regression-continuous .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/regression-continuous/" width="100%" height="408px" style="border:none;">
</iframe>
``` 
Predicting attitude towards smoking from exposure to an anti-smoking campaign. The orange dot represents the predicted attitude for the selected value of exposure.
:::


Good understanding of the regression equation is necessary for understanding moderation in regression models. So let us have a close look at an example equation [Eq. @eq-regrexample]. In this example, the dependent variable attitude towards smoking is predicted from a constant and one independent variable, namely exposure to an anti-smoking campaign.

$$
\small
  attitude = constant + b*exposure 
\normalsize
$$ {#eq-regrexample}

The constant is the predicted attitude if a person scores zero on all independent variables. To see this, plug in (replace) zero for the predictor in the equation (Eq. @eq-regsmokedummy) and remember that zero times something yields zero. This reduces the equation to the constant.

$$
\small
\begin{split}
  attitude &= constant + b*0 \\ 
  attitude &= constant + 0 \\
  attitude &= constant
\end{split}
\normalsize
$$ {#eq-regsmokedummy}

For all persons scoring zero on exposure, the predicted attitude equals the value of the regression constant. This interpretation only makes sense if the predictor can be zero. If, for example, exposure had been measured on a scale ranging from one to seven, nobody can have zero exposure, so the constant has no straightforward meaning.

The unstandardized regression coefficient $b$ represents the predicted difference in the dependent variable for a difference of one unit in the independent variable. For example, plug in the values 1 and 0 for the *exposure* variable in the equation. If we take the difference of the two equations, we are left with $b$. Other terms in the two equations cancel out.

$$
\small
\begin{split}
  attitude = constant + b*1 \\ 
  \underline{- \mspace{20mu} attitude = constant + b*0} \\
  attitude \mspace{4mu} difference = b*1 - b*0 = b - 0 = b
\end{split}
\normalsize
$$ {#eq-regweight}

::: {.callout-important appearance="simple"}

The unstandardized regression coefficient $b$ represents the predicted difference in the dependent variable for a difference of one unit in the independent variable.

It is the slope of the regression line.

:::

Whether this predicted difference is small or large depends on the practical context. Is the predicted decrease in attitude towards smoking worth the effort of the campaign? In the example shown in @fig-regression-continuous, one additional unit of exposure decreases the predicted attitude by 0.6. This seems to be quite a substantial change on a scale from -5 to 5.

In the data, the smallest exposure score is (about) zero, predicting a positive attitude of 1.6. The largest observed exposure score is around eight, predicting a negative attitude of -3.2. If exposure causes the predicted differences in attitude, the campaign would have interesting effects. It may change a positive attitude into a rather strong negative attitude.

If we want to apply a rule of thumb for the strength of the effect, we usually look at the standardized regression coefficient ($b^*$ according to APA, *Beta* in SPSS output). See @sec-assoc-size for some rules of thumb for effect size interpretation.

Note that the regression coefficient is calculated for predictor values that occur within the data set. For example, if the observed exposure scores are within the range zero to eight, these values are used to predict attitude towards smoking.

We cannot see this in the regression equation, which allows us to plug in -10, 10, or 100 as exposure values. But the values for attitude that we predict from these exposure values are probably nonsensical (if possible at all: -10 exposure?) Our data do not tell us anything about the relation between exposure and anti-smoking attitude for predictor values outside the observed zero to eight range. We should not pretend to know the effects of exposure levels outside this range. It is good practice to check the actual range of predictor values.

### Dichotomous predictor {#sec-dichpredictor}

Instead of a numerical independent variable, we can use a dichotomy as an independent variable in a regression model. The dichotomy is preferably coded as 1 versus 0, for example, 1 for smokers and 0 for non-smokers among our respondents.

::: {#fig-regression-dichotomy .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/regression-dichotomy/" width="100%" height="330px" style="border:none;">
</iframe>
``` 
What is the difference in attitude between non-smokers and smokers?
:::

The interpretation of the effect of a dichotomous independent variable in a regression model is quite different from the interpretation of a numerical independent variable's effect.

It does not make sense to interpret the unstandardized regression coefficient of, for example, smoking status as predicted difference in attitude for a difference of one 'more' smoking status. After all, the 0 and 1 scores do not mean that there is one unit 'more' smoking. Instead, the coefficient indicates that we are dealing with different groups: smokers versus non-smokers.

If smoking status is coded as smoker (1) versus non-smoker (0), we effectively have two versions of the regression equation. The first equation @eq-regdicho1 represents all smokers, so their smoking status score is 1. The smoking status of this group has a fixed contribution to the predicted average attitude, namely $b$.

$$
\small
\begin{split}
  attitude &= constant + b*status \\
  attitude_{smokers} &= constant + b*1 \\
  attitude_{smokers} &= constant + b
\end{split}
\normalsize
$$ {#eq-regdicho1}

Regression equation @eq-regdicho0 represents all non-smokers. Their smoking status score is 0, so the smoking status effect drops from the model.

$$
\small
\begin{split}
  attitude &= constant + b*status \\
  attitude_{non-smokers} &= constant + b*0 \\
  attitude_{non-smokers} &= constant + 0 
\end{split}
\normalsize
$$ {#eq-regdicho0}

If you compare the final equations for smokers [Eq. @eq-regdicho1] and non-smokers [Eq. @eq-regdicho0], the only difference is $b$, which is present for smokers but absent for non-smokers. It is the difference between the average score on the dependent variable (attitude) for smokers and the average score for non-smokers. We are testing a mean difference. Actually, this is exactly the same as an independent-samples *t* test!

::: {.callout-important appearance="simple"}

The unstandardized regression coefficient for a dummy (0/1) variable represents the difference between the average outcome score of the group coded as '1' and the average outcome score of the group coded as '0'.

:::

Imagine that $b$ equals 1.6. This indicates that the average attitude towards smoking among smokers (coded '1') is 1.6 units above the average attitude among non-smokers (coded '0'). Is this a small or large effect? In the case of a dichotomous independent variable, we should **not** use the standardized regression coefficient to evaluate effect size. The standardized coefficient depends on the distribution of 1s and 0s, that is, which part of the respondents are smokers. But this should be irrelevant to the size of the effect.

Therefore, it is recommended to interpret only the unstandardized regression coefficient for a dichotomous independent variable. Interpret it as the difference in average scores for two groups.

### A categorical independent variable and dummy variables {#sec-categorical-predictor}

How about a categorical variable containing three or more groups, for example, the distinction between respondents who smoke (smokers), stopped smoking (former smokers), and respondents who never smoked (non-smokers)? Can we include a categorical variable as an independent variable in a regression model? Yes, we can but we need a trick.

::: {#fig-regression-categorical .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/regression-categorical/" width="100%" height="325px" style="border:none;">
</iframe>
``` 
What are the predictive effects of smoking status?
:::


In this example, smoking status is measured with three categories: (1) non-smokers, (2) former smokers, and (3) smokers. Let us use the term *categorical variable* only for variables containing three or more categories or groups. This makes it easy to distinguish them from dichotomous variables. This distinction is important because we can include a dichotomous variable straight away as a predictor in a regression model but we cannot do so for a variable with more than two categories. We can only include such a categorical independent variable if we change it into a set of dichotomies.

We can create a new dichotomous variable for each group, indicating whether (score 1) or not (score 0) the respondent belongs to this group. In the example, we could create the variables *neversmoked*, *smokesnomore*, and *smoking*. Every respondent would score 1 on one of the three variables and 0 on the other two variables (@tbl-dummytable). These variables are called *dummy variables* or *indicator variables*.


```{r}
#| label: tbl-dummytable
#| fig-cap: "Dummy variables for a categorical independent variable: One dummy variable is superfluous."
#| echo: false
#| warning: false

knitr::kable(rbind(c("1 - Non-smoker", "1", "0", "0"), 
                   c("2 - Former smoker", "0", "1", "0"),
                   c("3 - Smoker", "0", "0", "1")), 
             col.names = c("Original categorical variable:", "neversmoked", "smokesnomore", "smoking"), align = c("l", "c", "c", "c"), booktabs = TRUE) %>%
  kable_styling(font_size = 12, full_width = F, position = "float_right",
                latex_options = c("scale_down", "HOLD_position"))
```


If we want to include a categorical independent variable in a regression model, we must use all dummy variables as independent variables **except one**. In the example, we must include two out of the three dummy variables. Equation @eq-regcat includes dummy variables for former smokers ($smokesnomore$) and smokers ($smoking$).

::: {.callout-important appearance="simple"}

Include dummy variables as independent variables for *all except one* categories of a categorical variable.

The category without dummy variable is the *reference group*.

:::

$$
\small
\begin{split}
  attitude &= constant + b_1*smokesnomore + b_2*smoking
\end{split}
\normalsize
$$ {#eq-regcat}

The two dummy variables give us three different regression equations: one for each smoking status category. Just plug in the correct 0 or 1 values for respondents with a particular smoking status.

Let us first create the equation for non-smokers. To this end, we replace both $smokesnomore$ and $smoking$ by 0. As a result, both dummy variables drop from the equation [Eq. @eq-regcat1], so the constant is the predicted attitude for non-smokers. The non-smokers are our *reference group* because they are not represented by a dummy variable in the equation.

$$
\small
\begin{split}
  attitude &= constant + b_1*smokesnomore + b_2*smoking \\
  attitude_{non-smokers} &= constant + b_1*0 + b_2*0 \\
  attitude_{non-smokers} &= constant
\end{split}
\normalsize
$$ {#eq-regcat1}

For former smokers, we plug in 1 for $smokesnomore$ and 0 for $smoking$. The predicted attitude for former smokers equals the constant plus the unstandardized regression coefficient for the $smokesnomore$ dummy variable ($b_1$), see Equation @eq-regcat2. Remember that the constant represents the non-smokers (reference group), so the unstandardized regression coefficient $b_1$ for the $smokesnomore$ dummy variable shows us the difference between former smokers and non-smokers: How much more positive or more negative the average attitude towards smoking is among former smokers than among non-smokers.

$$
\small
\begin{split}
  attitude &= constant + b_1*smokesnomore + b_2*smoking \\
  attitude_{former smokers} &= constant + b_1*1 + b_2*0 \\
  attitude_{former smokers} &= constant + b_1
\end{split}
\normalsize
$$ {#eq-regcat2}

Finally, for smokers, we plug in 0 for $smokesnomore$ and 1 for $smoking$ [Eq. @eq-regcat3]. The predicted attitude for smokers equals the constant plus the unstandardized regression coefficient for the $smoking$ dummy variable ($b_2$). This regression coefficient, then, represents the difference in average attitude between smokers and non-smokers (reference group).

$$
\small
\begin{split}
  attitude &= constant + b_1*smokesnomore + b_2*smoking \\
  attitude_{smokers} &= constant + b_1*0 + b_2*1 \\
  attitude_{smokers} &= constant + b_2
\end{split}
\normalsize
$$ {#eq-regcat3}

The interpretation of the effects (regression coefficients) for the included dummies is similar to the interpretation for a single dichotomous independent variable such as smoker versus non-smoker. It is the difference between the average score of the group coded 1 on the dummy variable and the average score of the reference group on the dependent variable. The reference group is the group scoring 0 on all dummy variables that represent the categorical independent variable.

If we exclude the dummy variable for the respondents who never smoked, as in the above example, the regression weight of the dummy variable $smokesnomore$ gives the average difference between former smokers and non-smokers. If the regression weight is negative, for instance -0.8, former smokers have on average a more negative attitude towards smoking than non-smokers. If the difference is positive, former smokers have on average a more positive attitude towards smoking.

Which group should we use as reference category, that is, which group should not be represented by a dummy variable in the regression model? This is hard to say in general. If one group is of greatest interest to us, we could use this as the reference group, so all dummy variable effects express differences with this group. Alternatively, if we expect a particular ranking of the average scores, we may pick the group at the highest, lowest or middle rank as the reference group. If you can't decide, run the regression model several times with a different reference group.

Finally, note that we should not include all three dummy variables in the regression model [Eq. @eq-regcat]. We can already identify the non-smokers, because they score 0 on both the $smokesnomore$ and $smoking$ dummy variables. Adding the $neversmoked$ dummy variable to the regression model is like including the same independent variable twice. How can the estimation process decide which of the two identical independent variable is responsible for the effect? It can't decide, so the estimation process fails or it drops one of the dummy variables. If this happens, the independent variables are said to be perfectly *multicollinear*.

### Assumptions for regression analysis {#sec-regr-inference}

```{r regression-sampling, eval=FALSE, echo=FALSE, fig.pos='H', fig.align='center', fig.cap="What happens to regression lines from sample to sample?"}
# Goal: Understand that regression constant and coefficient(s) have sampling
# distributions.
# Generate a population with a weak negative effect (-0.6) of exposure on
# attitude and exposure, with a sizable error term (so a lot of variation in
# sample regression lines).
# Generate a sample (N = 10) and display it in a scatterplot with regression
# line, labelled with it's unstandardized regression coefficient value. Also
# plot the sampling distribution for the regression coefficient.
# Add a button to allow drawing a new sample; display the new sample and new
# regression line but retain the existing regression lines.
# Add button (or change sampling button) to draw 1,000 samples: don't display
# samples, just update sampling distribution with normal (or t) distribution as
# superimposed curve.

1. Which estimates can change from sample to sample: the regression constant, the regression coefficient, or both? Check your answer by drawing new samples.

2. What is the shape of the sampling distribution if you draw a lot of samples?

3. What happens if you draw samples of larger size? Think of what you learned in preceding chapters. Formulate your answer before you change sample size in @fig-regression-sampling.
```

If we are working with a random sample or we have other reasons to believe that our data could have been different due to chance (@sec-no-random-sample), we should not just interpret the results for the data set that we collected. We should apply statistical inference---confidence intervals and significance tests---to our results. The confidence interval gives us bounds for plausible population values of the unstandardized regression coefficient. The *p* value is used to test the *null hypothesis that the unstandardized regression coefficient is zero in the population*.

Each regression coefficient as well as the constant may vary from sample to sample drawn from the same population, so we should devise a sampling distribution for each of them. These sampling distributions happen to have a *t* distribution under particular assumptions.

@sec-param-estim and @sec-hypothesis have extensively discussed how confidence intervals and *p* values are constructed and how they must be interpreted. So we focus now on the assumptions under which the *t* distribution is a good approximation of the sampling distribution of a regression coefficient.

#### Independent observations

The two most important assumptions require that the observations are *independent and identically distributed*. These requirements arise from probability theory. If they are violated, the statistical results should not be trusted.

Each observation, for instance, a measurement on a respondent, must be independent of all other observations. A respondent's dependent variable score is not allowed to depend on scores of other respondents.

It is hardly possible to check that our observations are independent. We usually have to assume that this is the case. But there are situations in which we should not make this assumption. In time series data, for example, the daily amount of political news, we usually have trends, cyclic movements, or issues that affect the amount of news over a period of time. As a consequence, the amount and contents of political news on one day may depend on the amount and contents of political news on the preceding days.

Clustered data should also not be considered as independent observations. Think, for instance, of student evaluations of statistics tutorials. Students in the same tutorial group are likely to give similar evaluations because they had the same tutor and because of group processes: Both enthusiasm and dissatisfaction can be contagious.

#### Identically distributed observations

To check the assumption of identically distributed observations, we inspect the residuals. Remember, the residuals are represented by the error term ($e$) in the regression equation. They are the difference between the scores that we observe for our respondents and the scores that we predict for them with our regression model ($y - \hat{y}$).

::: {#fig-resid-normal .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/resid-normal/" width="100%" height="264px" style="border:none;">
</iframe>
``` 
What are the residuals and how are they distributed?
:::


If we sample from a population where attitude towards smoking depends on exposure, smoking status, and contact with smokers, we will be able to predict attitude from the independent variables in our sample. Our predictions will not be perfect, sometimes too high and sometimes too low. The differences between predicted and observed attitude scores are the residuals.

If our sample is truly a random sample with independent and identically distributed observations, the sizes of our errors (residuals) should be normally distributed for each value of the dependent variable, that is, attitude in our example. The residuals should result from chance for the relation between chance and a normal distribution).

So for all possible values of the dependent variable, we must collect the residuals for the observations that have this score on the dependent variable. For example, we should select all respondents who score 4.5 on the attitude towards smoking scale. Then, we select the residuals for these respondents and see whether they are approximately normally distributed.

Usually, we do not have more than one observation (if any) for a single dependent variable score, so we cannot apply this check. Instead, we use a simple and coarse approach: Are all residuals normally distributed?

A histogram with an added normal curve (like the right-hand plot in @fig-resid-normal) helps us to evaluate the distribution of the residuals. If the curve more or less follows the histogram, we conclude that the assumption of identically distributed observations is plausible. If not, we conclude that the assumption is not plausible and we warn the reader that the results can be biased.

#### Linearity and prediction errors

The other two assumptions that we use tell us about problems in our model rather than problems in our statistical inferences. Our regression model assumes a linear effect of the independent variables on the dependent variable (*linearity*) and it assumes that we can predict the dependent variable equally well or equally badly for all levels of the dependent variable (*homoscedasticity*, next section).

The regression models that we estimate assume a linear model. This means that an additional unit of the independent variable always increases or decreases the predicted value by the same amount. If our regression coefficient for the effect of exposure on attitude is -0.25, an exposure score of one predicts a 0.25 more negative attitude towards smoking than zero exposure. Exposure score five predicts the same difference in comparison to score four as exposure score ten in comparison to exposure score nine, and so on. Because of the linearity assumption, we can draw a regression model as a straight line. Residuals of the regression model help us to see whether the assumption of a linear effect is plausible.

::: {#fig-pred-linearity .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/pred-linearity/" width="100%" height="460px" style="border:none;">
</iframe>
``` 
How do residuals tell us whether the relation is linear?
:::

The relation between an independent and dependent variable, for example, exposure and attitude towards smoking, does not have to be linear. It can be curved or have some other fancy shape. Then, the linearity assumption is not met. A straight regression line does not nicely fit such data.

We can see this in a graph showing the (standardized) residuals (vertical axis) against the (standardized) predicted values of the dependent variable (on the horizontal axis), as exemplified by the lower plot in @fig-pred-linearity. Note that the residuals represent prediction errors. If our regression predictions are systematically too low at some levels of the dependent variable and too high at other levels, the residuals are not nicely distributed around zero for all predicted levels of the dependent variable. This is what you see if the association is curved or U-shaped.

This indicates that our linear model does not fit the data. If it would fit, the average prediction error is zero for all predicted levels of the dependent variable. Graphically speaking, our linear model matches the data if positive prediction errors (residuals) are more or less balanced by negative prediction errors everywhere along the regression line.

#### Homoscedasticity and prediction errors

The plot of residuals by predicted values of the dependent variable tells us more than whether a linear model fits the data.

::: {#fig-pred-homoscedasticity .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/pred-homoscedasticity/" width="100%" height="460px" style="border:none;">
</iframe>
``` 
How do residuals tell us that we predict all values equally well?
:::

The other assumption states that we can predict the dependent variable equally well at all dependent variable levels. In other words, the prediction errors (residuals) are more or less the same at all levels of the dependent variable. This is called *homoscedasticity*. If we have large prediction errors at some levels of the dependent variable, we should also have large prediction errors at other levels. As a result, the vertical width of the residuals by predictions scatter plot should be more or less the same from left to right. The dots representing residuals resemble a more or less rectangular band.

If the prediction errors are not more or less equal for all levels of the predicted scores, our model is better at predicting some values than other values. For example, low values can be predicted better than high values of the dependent variable. The dots representing residuals resemble a cone. This may signal, among other things, that we need to include moderation in the model.

```{r eval=FALSE, echo=FALSE}
#DROPPED

Why do we use the residuals and predicted values instead of a scatterplot for each dependent-independent variable pair to assess linearity and homoscedasticity? The reason is that some independent variables may predict low values and other independent variables may predict high values. This is perfectly OK if together they predict low and high values equally well.
```

#### Checking regression assumptions with SPSS


@vid-SPSSregassumpt shows how to check the assumptions of regression analysis in SPSS. The example is based on the *smokers.sav* data set, which contains data on smoking status and attitude towards smoking. The video shows how to inspect the residuals and the predicted values of the dependent variable.

::: {#vid-SPSSregassumpt}

{{< video https://www.youtube.com/embed/hx2qdaVhlaM
    width="100%"
    height="315" 
>}}

Inspecting residuals.
:::



```{r, echo=FALSE, eval=FALSE}
# Goal: Inspecting residuals.  (see Chapter 4 on hypothesis testing for video about regression basics.)
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status.
# Technique: regression analysis
# SPSS menu: linear regression, add plots
# Interpret output = check assumptions: Chart Editor of zresid * zpred plot: add reference line at 0 and perhaps at +2 and -2 to inspect shape of residual distribution.
```

### Regression Analysis in SPSS {#sec-SPSS-regression}

In SPSS, we use the *Linear* option in the *Regression* submenu for regression analysis. For a moderation model, we first use the *Compute Variable* option in the *Transform* menu to calculate an interaction variable: we multiply (using `*`) the predictor variable by the moderator variable. The interaction variable is included in the regression model as an independent variable, just like the predictor, moderator, and any other independent variables (covariates).

A categorical predictor variable such as a participant's residential area (urban, suburban, rural) is included in the regression model as dummy variables, which have the values 0 or 1, for example: dummy variables *suburban* and *rural*, each with values yes (1) and no (0). The category without dummy variable is the reference group. 

The regression coefficient of a dummy variable gives us the difference between the average score on the dependent variable of the group scoring 1 on the dummy variable and the reference group. In the example presented in @fig-regressiontable, the attitude towards smoking for participants living in a suburban environment (red box) is on average 0.33 more positive than among participants in an urban environment (the reference group).

:::{#fig-regressiontable}
```{r regressiontable, echo=FALSE, out.width="100%", fig.pos='H', fig.align='center'}
knitr::include_graphics("figures/S8_AE1.png")
```
SPSS table of regression effects for a model in which the effect of exposure is moderated by participant's smoking status (reference group: people who never smoked).
:::

The predictor variable *Exposure* is included in the interaction effect. As a consequence, the regression coefficient for this variable (green box in @fig-regressiontable) expresses the effect of exposure on attitude towards smoking for the reference group on the other variable included in the interaction effect, namely, people who never smoked. A one unit increase in exposure predicts a 0.20 more negative (-0.197) attitude towards smoking **for people who never smoked**.

If we would like to know the effect of exposure on attitude towards smoking for former smokers, we must add the regression coefficient for the interaction of exposure with former smokers (blue box) to the regression coefficient of exposure (green box): A one unit increase in exposure predicts a 0.47 more negative (-0.465 = -0.197 + -0.268) attitude towards smoking for former smokers.

An interaction effect such as -0.27 for *Former smoker \* exposure* tells us the difference between the exposure effect for former smokers and the exposure effect for people who never smoked (reference group). A plot of the regression lines shows the different exposure effects (@fig-regressionplot). The red line (effect of exposure for former smokers) has a stronger downward tendency than the blue line (exposure effect for people who never smoked). 

:::{#fig-regressionplot}
```{r regressionplot, echo=FALSE, out.width="75%", fig.pos='H', fig.align='center'}
knitr::include_graphics("figures/S8_AE2.png")
```
Simple regression lines showing the effect of exposure on attitude towards smoking for former smokers and people who never smoked (in an urban environment).
:::

#### SPSS Simple regression with a numeric predictor

@vid-SPSSregsimple shows how to run and interpret a simple regression analysis with one numeric predictor in SPSS. The video shows how to run a regression analysis with confidence intervals for the regression coefficients. The example is based on the *consumers.sav* data set, which contains data on brand awareness by advertisement exposure.

::: {#vid-SPSSregsimple}

{{< video https://www.youtube.com/embed/XrxlCOi6SgE
    width="100%"
     height="315" 
>}}

Asymmetric association as prediction
:::



```{r, echo=FALSE, eval=FALSE}
# Goal: asymmetric association as prediction
# Example: consumers.sav, brand awareness by advertisement exposure
# Technique: regression with confidence intervals
# SPSS menu: regression>linear with CI under Statistics.
# Paste & Run.
# Interpret output: R2, F test, predictive effect strength (b*) and change (b) with 95% confidence interval.
# Check assumptions: in chapter on moderation with regression analysis?
```

#### SPSS Simple regression with a categorical predictor

@vid-SPSSregdummy2 shows how to create dummy variables in SPSS. The example is based on the *smokers.sav* data set, which contains data on smoking status and attitude towards smoking. The video shows two ways of creating dummy variables: the *Transform > Create Dummy Variables* option and the *Transform > Recode into Different Variables* option. The first option is preferred because it is easier to use.

::: {#vid-SPSSregdummy2}

{{< video https://www.youtube.com/embed/c2b4dtlPS54
    width="100%"
    height="315" 
>}}

Creating dummy variables in SPSS.
:::



```{r, echo=FALSE, eval=FALSE}
# Creating dummy variables in SPSS.
# Goal: Understand creating dummy variables.
# Example: smokers.sav, respondent's smoking status ( 3 categories).
# SPSS menu: Transform > Create Dummy Variables or Trasform > Recode into Different Variables.
# Inspect results: new variables, coded 0/1.
```

@vid-SPSSregdummy shows how to use dummy variables in a regression model in SPSS. The example is based on the *smokers.sav* data set, which contains data on smoking status and attitude towards smoking. The video shows how to run a regression analysis with dummy variables.

::: {#vid-SPSSregdummy}

{{< video https://www.youtube.com/embed/Vs26zuwAZdk
    width="100%"
    height="315" 
>}}

Using dummy variables in a regression model in SPSS.
:::



```{r, echo=FALSE, eval=FALSE}
# Using dummy variables in a regression model in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure to an anti-smoking campaign and respondent's smoking status (dummies).
# SPSS menu: Transform > Create Dummy Variables
# Remember: Leave one dummy variable out.
# Interpret results: unstandardized regression coefficient as average difference with reference category. Don't interpret the standardized regression coefficient.
```

## Moderation with categorical and continuous predictors {#sec-categoricalmoderator}

What if the effect of campaign exposure on attitude towards smoking may be different in different contexts, e.g., for people who smoke themselves and people who do not smoke? Perhaps, the campaign is more effective among smokers than among non-smokers or the other way around. If so, the effect of campaign exposure is moderated by the smoking status of the participants.

In a conceptual diagram (@fig-moderator-concept2), moderation is represented by an arrow pointing at another arrow. The moderator (smoking status) changes the relation between the predictor (campaign exposure) and the dependent variable (attitude towards smoking).

```{r}
#| label: fig-moderator-concept2
#| fig-cap: "Conceptual diagram of moderation."
#| echo: false
#| warning: false

# Create coordinates for the variable names.
variables <- data.frame(x = c(0.35, 0.5, 0.65), 
                        y = c(.1, .3, .1),
                        hjust = c(1, 0.5, 0),
                        label = c("Predictor", "Moderator", "Dependent variable"))
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = x[1], y = y[1], xend = x[3], yend = y[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[2], y = y[2], xend = x[2], yend = y[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) +
  geom_label(aes(label=label, hjust = hjust)) + 
  coord_cartesian(xlim = c(0.3, 0.8), ylim = c(0, 0.4)) +
  theme_void()
# Cleanup.
rm(variables)
```


We used a similar diagram to express moderation in two-way analysis of variance (@sec-moderationanova). But now at least one of our independent variables is numeric, for example, the number of times the respondent has been exposed to the campaign.

Analysis of variance (ANOVA) investigates the effects of categorical variables on a numerical dependent variable. It cannot handle numerical independent variables. Although there are ways to include numerical independent variables in analysis of variance, for example, analysis of covariance (ANCOVA), we use regression analysis if we have a numerical dependent variable and at least one numerical independent variable.

::: {.callout-important appearance="simple"}

Use regression analysis if you have a numerical dependent variable and at least one numerical independent variable.

:::

In the current section, we discuss regression models with a numerical predictor and a categorical moderator. The next chapter (@sec-moderationcont) presents regression models in which both the predictor and the moderator are numerical.

### A dichotomous moderator

::: {#fig-dichotomous-moderator}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/dichotomous-moderator/" width="100%" height="350px" style="border:none;">
</iframe>
``` 
Is the effect of exposure on attitude moderated by smoking status?
:::

In @sec-regression-equation, we have analyzed the predictive effects of exposure to an anti-smoking campaign and smoking status on a person's attitude towards smoking. We have found a negative effect for exposure and a positive effect for smoking. More exposure predicts a more negative attitude whereas smokers have on average a more positive attitude towards smoking than non-smokers.

Our current question is: Does exposure to the campaign have the same effect for smokers and non-smokers? We want to compare an effect (exposure on attitude) for different contexts (smokers versus non-smokers), so our current question involves moderation. Is the effect of exposure on attitude moderated by smoking status?

Our moderator (smoker vs. non-smoker) is a dichotomous variable but our predictor (exposure) is numerical, so we cannot use analysis of variance. Instead, we use regression analysis, which allows numerical predictors.

In the context of a regression model, moderation means **different slopes for different groups**. The slope of the regression line is the regression coefficient, which expresses the effect of the predictor on the dependent variable. If we have different effects in different contexts (moderation), we must have different regression coefficients for different groups.

#### Interaction variable {#sec-interaction-variable}

How do we obtain different regression coefficients and lines for smokers and non-smokers? The statistical trick is quite easy: Include an additional predictor in the model that is the product (multiplication) of the predictor (exposure) and the moderator (smoking status). This new predictor is the *interaction variable*. The regression coefficient of the interaction variable is called the *interaction effect*.

::: {#fig-interaction-var .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/interaction-var/" width="100%" height="440px" style="border:none;">
</iframe>
``` 
How does an interaction variable create different regression lines for different groups?
:::

The interaction variable must be included together with the original predictor and moderator variables, see @eq-intvar. This is also visible in the statistical diagram (@fig-moderator-statistical) for moderation in a regression model.

$$
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker 
\end{split}
\normalsize
$$ {#eq-intvar}


```{r}
#| label: fig-moderator-statistical
#| fig-cap: "Statistical diagram of moderation."
#| echo: false
#| warning: false

library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(rep.int(0.3, times = 3), 0.7), 
                        y = c(.4, .25, .1, .25),
                        label = c("Exposure", "Smoker", "Exposure*Smoker", "Attitude"))
# Add coordinates for arrow endpoint.
x_diff <- 0.04
variables$xend <- variables$x[4] - x_diff #fixed translation to the left
variables$yend <- variables$y[4] + x_diff * (variables$y - variables$y[4]) / (variables$x[4] - variables$x)
rm(x_diff)
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = x[1], y = y[1], xend = xend[1], yend = yend[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[2], y = y[2], xend = xend[2], yend = yend[2]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[3], y = y[3], xend = xend[3], yend = yend[3]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  # geom_segment(aes(x = x[4], y = y[4], xend = xend[4], yend = yend[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.1, 0.5)) +
  theme_void()
```


The smoking status variable is coded 1 for smokers and 0 for non-smokers. For clarity, we name this variable *smoker* with score 1 for Yes and score 0 for No. We have two different regression equations, one for each group on the dichotomous predictor *smoker*. Just plug in the two possible values (1 and 0) for this variable. For non-smokers [Equation @eq-intvarnonsmoker], the interaction variable drops from the model because multiplying by zero yields zero. For non-smokers, our reference group, $b_1$ represents the effect of exposure on attitude. It is called the *simple slope* of exposure for non-smokers.

$$
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker \\
  attitude_{non-smokers} = &\ constant + b_1*exposure + b_2*0 + b_3*exposure*0 \\
  attitude_{non-smokers} = &\ constant + b_1*exposure
\end{split}
\normalsize
$$ {#eq-intvarnonsmoker}

In contrast, the interaction variable remains in the model for smokers [Equation @eq-intvarsmoker], who score 1 on smoking status. Note what happens with the coefficient of the exposure effect if we rearrange the terms a little: The exposure effect equals the effect for the reference group of non-smokers ($b_1$) plus the effect of the interaction variable ($b_3$). The simple slope for smokers, then, is ($b_1 + b_3$).

$$
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker \\
  attitude_{smokers} = &\ constant + b_1*exposure + b_2*1 + b_3*exposure*1 \\
  attitude_{smokers} = &\ constant + b_1*exposure + b_3*exposure + b_2 \\
  attitude_{smokers} = &\ constant + (b_1 + b_3)*exposure + b_2 
\end{split}
\normalsize
$$ {#eq-intvarsmoker}

The interaction variable changes the slope of the effect of exposure on attitude. More specifically, the regression coefficient of the interaction variable ($b_3$) shows the difference between the simple slope of the exposure effect for smokers ($b_1+b_3$) and the simple slope for non-smokers ($b_1$). 

Let us assume that the unstandardized regression coefficient of the interaction effect is -0.3. This means that the effect of exposure on attitude is more strongly negative (or less positive) for smokers than for non-smokers. One additional unit of exposure decreases the predicted attitude for smokers by 0.3 **more** than for non-smokers.

#### Conditional effects, not main effects {#sec-conditional-effects}

It is very important to note that the effects of exposure and smoking status in a model with exposure by smoking status interaction are **not** main effects as in analysis of variance. As we have seen in the preceding section [Equation @eq-intvarnonsmoker], the regression coefficient $b_1$ for exposure expresses the effect of exposure for non-smokers. It is a *conditional effect*, namely the effect for non-smokers only. Non-smokers are the *reference group* because they score zero on the moderator (*smoker*). This is quite different from a main effect in analysis of variance, which is an average effect over all groups.

In a similar way, the regression coefficient $b_2$ for smoking status expresses the effect for persons who score zero on the exposure predictor. Simply plug in the value 0 for exposure in the regression equation [Eq. @eq-simplestatus].

$$
\small
\begin{split}
  attitude = &\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker\\
  attitude_{no-expo} = &\ constant + b_1*0 + b_2*smoker + b_3*0*smoker \\
  attitude_{no-expo} = &\ constant + b_2*smoker
\end{split}
\normalsize
$$ {#eq-simplestatus}

Smoking status is a dichotomy, so its regression coefficient ($b_2$) tells us the average difference in attitude between smokers and non-smokers. Due to the inclusion of the interaction variable, it now tells us the difference in average attitude between smokers and non-smokers **who have zero exposure to the anti-smoking campaign**. Note again that this is a conditional effect, not a main effect.

#### Interpretation and statistical inference {#sec-interactioninterpretation}

```{r}
#| label: tbl-dich-moderator-output
#| fig-cap: "Predicting attitude towards smoking: regression analysis results."
#| echo: false
#| warning: false

# Table of regression coefficients for exposure moderated by status. Similar to SPSS output (with correct standardized coefficients).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
model_1 <- lm(attitude ~ exposure*status2, data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Correctly standardized coefficients.
attach(smokers)
z_exposure <- (exposure - mean(exposure))/sd(exposure)
z_status2 <- (status2 - mean(status2))/sd(status2)
z_expostatus2 <- z_exposure * z_status2
z_attitude <- (attitude - mean(attitude))/sd(attitude)
model_2 <- lm(z_attitude ~ z_exposure + z_status2 + z_expostatus2)
results_2 <- coef(summary(model_2))
results <- cbind(results[, 1:2], results_2[, 1], results[, 3:6])
results[1, 3] <- NA
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure"
attributes(results)$dimnames[[1]][3] <- "Status (smoker = 1, non-smoker = 0)"
attributes(results)$dimnames[[1]][4] <- "Exposure*Status (smoker)"
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, booktabs = TRUE,
             col.names = c("B", "Std. Error", "Beta", "t", "Sig.", "Lower Bound", "Upper Bound")) %>%
  kable_styling(font_size = 12, full_width = F,
                latex_options = c("scale_down", "HOLD_position")) %>%
  column_spec(1, color = "white",
              background = c("#FDAE61", "#2B83BA", "#ABDDA4", "#D7191C")) %>%
  column_spec(2, color = c("#FDAE61", "#2B83BA", "#ABDDA4", "#D7191C"))
# Helper function for displaying results within the text.
source("report_n.R")
#Cleanup (partial).
rm(smokers, model_1, ci, results_2, model_2, z_attitude, z_status2, z_expostatus2, z_exposure)
```


In @tbl-dich-moderator-output, the effect of exposure on attitude depends on the value of smoking status because the model includes an interaction effect of exposure with smoking status (red). Non-smokers are the reference group on the smoking status dummy variable because they are coded 0. Therefore, the regression coefficient for exposure (blue) gives us the effect of exposure on smoking attitude **for non-smokers**. 
If you want to check this, plug in 0 for the smoking status variable in the regression equation that we may construct from @tbl-dich-moderator-output. 

$$
\small
\begin{split}
  attitude = &\ \color{#FDAE61}{constant} + \color{#2B83BA}{b_1*exposure} + \color{#ABDDA4}{b_2*status} + \color{#D7191C}{b_3*exposure*status}\\
  attitude_{non-smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#ABDDA4}{1.980*0} + \color{#D7191C}{-0.327*exposure*0}\\
  attitude_{non-smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#ABDDA4}{0} + \color{#D7191C}{0}\\
  attitude_{non-smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure}
\end{split}
\normalsize
$$ {#eq-nonsmokers}

Multiplying by zero yields zero, so for non-smokers, the resulting effect of exposure on attitude is `r report_n(results[2, 1], digits = 2)`. An additional unit of exposure predicts a smoking attitude among non-smokers that is `r report_n(abs(results[2, 1]), digits = 2)` points more negative. More exposure to the campaign goes together with a more negative attitude towards smoking for non-smokers. The *p* value for this effect tests the null hypothesis that the effect is zero in the population. If the exposure effect is statistically significant, we reject this null hypothesis.

Smokers are coded 1 on the (smoking) status variable and non-smokers are coded 0, so the regression coefficient for the interaction variable tells us that the slope of the exposure effect is `r report_n(abs(results[4, 1]), digits = 2)` lower for smokers than for non-smokers. The estimated slope of the exposure effect is `r report_n(results[2, 1], digits = 2)` for non-smokers. We can add the regression coefficient of the interaction variable to obtain the estimated slope for smokers, which is `r report_n(results[2, 1] + results[4, 1], digits = 2)`.

If you want to check this, plug in 1 for the smoking status variable in the regression equation. Add the two effects of exposure in the equation to obtain the effect of exposure on attitude for smokers.

$$
\small
\begin{split}
  attitude = &\ \color{#FDAE61}{constant} + \color{#2B83BA}{b_1*exposure} + \color{#ABDDA4}{b_2*status} + \color{#D7191C}{b_3*exposure*status}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#ABDDA4}{1.980*1} + \color{#D7191C}{-0.327*exposure*1}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#ABDDA4}{1.980} + \color{#D7191C}{-0.327*exposure}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#D7191C}{-0.327*exposure} + \color{#ABDDA4}{1.980}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + (\color{#2B83BA}{-0.162} + \color{#D7191C}{-0.327})*exposure + \color{#ABDDA4}{1.980}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + -0.489*exposure + \color{#ABDDA4}{1.980}
\end{split}
\normalsize
$$ {#eq-smokers}

Now we can compare the slopes (regression coefficients) for the two groups, which gives good insight into the nature of moderation in this example. The effect of exposure on attitude is more strongly negative for smokers (`r report_n(results[2, 1] + results[4, 1], digits = 2)`) than for non-smokers (`r report_n(results[2, 1], digits = 2)`).

The interaction variable is treated as an ordinary predictor in the estimation process, so it receives a confidence interval and a *p* value. The null hypothesis for the *p* value is that the interaction effect is zero in the population. In other words, the effect of exposure on attitude is hypothesized to be the same for smokers and non-smokers in the population; no moderation is expected in the population.

We know the confidence intervals and *p* values of the exposure effect for non-smokers (the regression coefficient for exposure) and for the difference between their exposure effect and the exposure effect for smokers (the regression coefficient for the interaction effect). We do not know, however, the confidence interval and statistical significance of the exposure effect for smokers. We cannot add confidence intervals or *p* values, so we do not know if the effect of exposure for smokers is significantly different from zero in the population.

If you want to know the confidence interval or *p* value of the exposure effect for smokers, you have to rerun the regression analysis using a different dummy variable for the moderator. You should create a dichotomous variable that assigns 0 to smokers and 1 to non-smokers, and an interaction variable created with this dichotomy. The regression coefficient of the exposure effect now expresses the effect for smokers because smokers are the reference group on the new dummy variable. The associated *p* value and confidence interval apply to the exposure effect for smokers.

Interaction variables are used just like ordinary predictors, so the general assumptions of regression analysis apply. See @sec-regr-inference for a description of the assumptions and checks.

In a similar way, the effect of smoking status on attitude is conditional on exposure because smoking status and exposure are included in the interaction variable. The regression coefficient for status tells us the difference between smokers and non-smokers who have 0 exposure. So, without exposure to the campaign, smokers are on average `r report_n(results[3, 1], digits = 2)` more positive towards smoking than non-smokers. The *p* value tests the null hypothesis that the difference is zero for people without exposure (exposure = 0) to the anti-smoking campaign.

Let us conclude the interpretation with a warning. The standardized regression coefficients that SPSS reports for interaction effects or effects of predictors that are involved in interaction effects **must not be used**. They are calculated in the wrong way if the regression model includes an interaction variable. As a result, they are misleading.

```{r dich-moderator-cleanup, echo=FALSE}
#Cleanup.
rm(report_n, results)
```

#### PROCESS with 2 level moderator (Dichotomous)

As SPSS cannot apply statistical inference to indirect effects, we need to use the PROCESS macro developed for this purpose [@RefWorks:3873]. Instructions on how to add PROCESS to your SPSS installation can be found in [Appendix -@sec-installPROCESS]. To run the analysis below, PWOCESS needs to be installed in SPSS.

In @vid-PROCESS2cat, we show how to use PROCESS to estimate a moderation model with a categorical moderator with two levels. The example uses the [smokers.sav](data/smokers4.sav) data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable _status2_).

::: {.callout-important appearance="simple"}
Instructions on how to add PROCESS to your SPSS installation can be found in [Appendix -@sec-installPROCESS].
:::

In SPSS we go to `Analyze > Regression > PROCESS`. In the dialog box, we select the dependent variable (attitude) as `Y variable`, the predictor (exposure) as `X variable`, and the moderator (status2) as `Moderator variable W`. We select `Model 1` for a moderation model with one moderator. Under `Options`, we select `generate code to visualize interaction`, and we click on `OK` to run the analysis.

When PROCESS is finished, we can find the output in the SPSS output window. By double clicking on the output, we can select the generated code for visualizing the interaction. Copy the code, open a new syntax window (`File > New > Syntax`), and paste the code into the new syntax window. We can run this code to create a plot of the interaction.

The plot shows the scatter plot for the relation between exposure and attitude for the two groups of the moderator (non-smokers and smokers). To add the regression lines, we can use the `Add Fit Line to subgroups` in the `Chart Editor`. The `Chart Editor` opens when we double click on the plot.

::: {#vid-PROCESS2cat}
{{< video https://youtu.be/I6_ShDCM0vY?si=mOfkZNJ1zl1RScYd
    width="100%"
    height="315"
>}}

Estimating categorical by numerical moderation with regression in SPSS using PROCESS.
:::

@vid-PROCESS2catInterpretation shows how to interpret the output of PROCESS for a two category moderator. The example uses the [smokers.sav](data/smokers4.sav) data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable _status2_).

::: {#vid-PROCESS2catInterpretation}
{{< video https://youtu.be/dA1od_IuArI
    width="100%"
    height="315"
>}}

Interpreting the output of PROCESS for a two category moderator.
:::

The `Model Summary` of the PROCESS @out-process-model-summary, contains the $R$, $R^2$, indicating a correlation between the observed and predicted values of the dependent variable (attitude), and the predicted values of the dependent variable (attitude), which explains 23.58% of the variance. The $F$ test indicates that the model is statistically significant.

::: {#out-process-model-summary}
```{md}
Model Summary 
          R       R-sq        MSE          F        df1        df2          p 
      .4856      .2358     2.2022     8.3301     3.0000    81.0000      .0001
```

PROCESS Model Summary output.
:::

The 'Model' @out-process-model-coefficients contains the regression coefficients for the predictor (exposure), the moderator (status2), and the interaction variable `Int_1` (exposure*status2). The collumn `coeff` contains the unstandardized regression coefficients, the column `se` contains the standard errors of the coefficients, the column `t` contains the t-values, and the column `p` contains the p-values. The columns `LLCI` and `ULCI` contain the lower and upper limits of the confidence intervals for the coefficients. The `Product terms key` shows the interaction variable `Int_1` and the variables that are multiplied to create the interaction variable. As `status2` is a dichotomous variable with values 0 or 1, the interaction term is either the exposure $\times$ 1 or is cancelled out as exposure is multiplied by 0.

::: {#out-process-model-coefficients}
```{md}
Model 
              coeff         se          t          p       LLCI       ULCI 
constant      .8998      .3568     2.5215      .0136      .1898     1.6098 
exposure     -.1620      .0611    -2.6506      .0097     -.2835     -.0404 
status2      1.9798      .7379     2.6830      .0088      .5116     3.4479 
Int_1        -.3271      .1415    -2.3109      .0234     -.6087     -.0455 
 
Product terms key: 
 Int_1    :        exposure x        status2
```

PROCESS Model beta coefficients output.
:::

The benefit of using PROCESS is that it provides the conditional effects of the predictor (exposure) at different values of the moderator (status2). With a dichotomous moderator, we have two values: 0 (non-smokers) and 1 (smokers). The conditional effects are shown in the `Conditional effects of the focal predictor at values of the moderator(s)` table @out-process-conditional-effects. The column `Effect` contains the conditional effect of exposure on attitude for non-smokers and smokers. 

::: {#out-process-conditional-effects}
```{md}
Conditional effects of the focal predictor at values of the moderator(s): 
 
    status2     Effect         se          t          p       LLCI       ULCI 
      .0000     -.1620      .0611    -2.6506      .0097     -.2835     -.0404 
     1.0000     -.4890      .1277    -3.8307      .0003     -.7430     -.2350 
```

PROCESS Model conditional effects output.
:::

With conditional effects, we mean the effect of exposure on attitude for non-smokers and smokers, the regression slopes at level 0 and level 1 of the moderator. These beta coefficients in the `Effect` column are the simplified regression slopes for the two groups.

$$
\small
\begin{split}
  attitude = &\ \color{#FDAE61}{constant} + \color{#2B83BA}{b_1*exposure} + \color{#ABDDA4}{b_2*status} + \color{#D7191C}{b_3*exposure*status}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#ABDDA4}{1.980*1} + \color{#D7191C}{-0.327*exposure*1}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#ABDDA4}{1.980} + \color{#D7191C}{-0.327*exposure}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#2B83BA}{-0.162*exposure} + \color{#D7191C}{-0.327*exposure} + \color{#ABDDA4}{1.980}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + (\color{#2B83BA}{-0.162} + \color{#D7191C}{-0.327})*exposure + \color{#ABDDA4}{1.980}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#000000}{-0.489*exposure} + \color{#ABDDA4}{1.980}\\
  attitude_{smoker} = &\ \color{#FDAE61}{0.900} + \color{#ABDDA4}{1.980} \color{#000000}{-0.489*exposure}\\
  attitude_{smoker} = &\ \color{#000000}{2.880} + \color{#000000}{-0.489*exposure}\\
  attitude_{smoker} = &\ \color{#000000}{2.880} - \color{#000000}{0.489*exposure}  
\end{split}
\normalsize
$$ {#eq-process-output}

As we can see from @out-process-conditional-effects, the slope (effect) for smokers is -0.490, which is the sum of the slope for non-smokers (-0.162) and the interaction effect (-0.327). Plugging in the values in the regression @eq-process-output, and simplifying, we can see on the bottom line, that the slope for exposure is, given some rounding, indeed -0.489.

In addition, PROCESS also provides the upper and lower limits of the confidence intervals, and the $p$-value for the conditional effects. With this we can statistically test the null hypothesis that the conditional effect is zero. And we can see that the confidence intervals of the conditional effects do not include zero.

### A categorical moderator

What if we have three or more groups on our moderator? For example, smoking status measured with three categories: (1) never smoked, (2) formerly smoked, (3) currently smoking? Does the effect of exposure on attitude vary between non-smokers, participants who stopped smoking, and those who are still smoking?

::: {#fig-categorical-moderator}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/categorical-moderator/" width="100%" height="350px" style="border:none;">
</iframe>
``` 

When do we have moderation with a categorical moderator?
:::



In @sec-categorical-predictor, we learned that we must create dummy variables for all but one groups of a categorical predictor in a regression model. This is what we have to do also for a categorical moderator. If the effect of a predictor, such as exposure, is moderated by a categorical variable, we have to create an interaction variable for each dummy variable in the equation. To create the interaction variables, we multiply the predictor by each of the dummy variables.

```{r}
#| label: fig-categorical-moderator-fig
#| fig-cap: "Statistical diagram with a moderator consisting of three groups. Non-smokers are the reference group"
#| echo: false
#| warning: false

library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(rep.int(0.3, times = 5), 0.7), 
                        y = c(.5, .4, .3, .2, .1, .3),
                        label = c("Exposure", "Former smoker", "Smoker", "Exposure*Former", "Exposure*Smoker", "Attitude"))
# Add coordinates for arrow endpoint.
x_diff <- 0.04
variables$xend <- variables$x[6] - x_diff #fixed translation to the left
variables$yend <- variables$y[6] + x_diff * (variables$y - variables$y[6]) / (variables$x[6] - variables$x)
rm(x_diff)
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = x[1], y = y[1], xend = xend[1], yend = yend[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[2], y = y[2], xend = xend[2], yend = yend[2]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[3], y = y[3], xend = xend[3], yend = yend[3]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[4], y = y[4], xend = xend[4], yend = yend[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[5], y = y[5], xend = xend[5], yend = yend[5]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.1, 0.5)) +
  theme_void()
```


In the end, we have an interaction variable for all groups but one on the categorical moderator. @fig-categorical-moderator-fig shows the statistical diagram. Estimation of the model yields point estimates (regression coefficients), confidence intervals, and *p* values for all independent variables (@tbl-cat-moderator-results).

```{r}
#| label: tbl-cat-moderator-results
#| fig-cap: "Predicting attitude towards smoking for three smoking status groups: regression analysis results."
#| echo: false
#| warning: false

# Table of regression coefficients for exposure moderated by status3. Similar to SPSS output (without correct standardized coefficients).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
model_1 <- lm(attitude ~ exposure*factor(status3, levels = c(0,1,2)), data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure"
attributes(results)$dimnames[[1]][3] <- "Former smoker"
attributes(results)$dimnames[[1]][4] <- "Smoker"
attributes(results)$dimnames[[1]][5] <- "Exposure*Former smoker"
attributes(results)$dimnames[[1]][6] <- "Exposure*Smoker"
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Set column names.
attributes(results)$dimnames[[2]] <- c("B", "Std. Error", "t", "Sig.", "Lower Bound", "Upper Bound")
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, booktabs = TRUE) %>%
  kable_styling(font_size = 12, full_width = F,
                latex_options = c("scale_down", "HOLD_position"))
# Cleanup.
rm(smokers, model_1, ci, results)
```


Remember that the effects of predictors that are included in interactions are conditional effects: effects for the reference group or reference value on the other variable involved in the interaction. Non-smokers are the reference group for participant's smoking status. The *p* value for the *exposure* predictor tests the hypothesis that the exposure effect for non-smokers is zero in the population.

For the two dummy variables *Former smoker* and *Smoker*, the null hypothesis is tested that they have the same average attitude in the population as the non-smokers (reference group) if they are not exposed to the anti-smoking campaign. Participants who are not exposed to the campaign (zero exposure) are the reference group here.

Interaction predictors show effect differences. In @tbl-cat-moderator-results, the interaction predictors test the null hypotheses that the effect of exposure on attitude is equal for former smokers and non-smokers (*Exposure\*Former smoker*) or for smokers and non-smokers (*Exposure\*Smoker*) in the population.

If we would like to know whether the exposure effect for former smokers is significantly different from zero, we have to rerun the regression model using former smokers as reference group. This new model would also tell us whether the exposure effect for former smokers is significantly different from the exposure effect for people who are still smoking.

#### SPSS dummy variables for interaction {#sec-catmodSPSS}

As described above, regression analysis with categorical moderators requires dummy and interaction variables. It is always possible to manually create dummy variables for categorical moderators as shown in @vid-SPSSregpred. However, the PROCESS macro for SPSS is a very useful tool for estimating moderation models with categorical moderators. It is easy to use and it automatically creates interaction terms and dummy variables for you. The add on is free and can be downloaded from [Andrew Hayes' website](http://www.processmacro.org/download.html).

::: {#vid-SPSSregpred}
{{< video https://www.youtube.com/embed/5ORuOV4obUU
    width="100%"
    height="315" 
>}}

Creating dummy variables for categorical by numerical interaction predictors (2 methods).
:::

```{r, echo=FALSE, eval=FALSE}
# Goal: Creating categorical by numerical interaction predictors (2 methods).
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable status2 and status3), use contact with smokers as a covariate.
# SPSS menu: Transform > Compute Variable for status2 (already 0/1 variable) ; Transform > Create Dummy Variables for status3 (3 categories)
# Interpret output: none.

# Create interaction predictors (also for cont*cont interaction variables in addition to interactions with dummies) and dummies for main effects in one go: 
#   - ensure that categorical variables are marked as Nominal or Ordinal in Variable View
#   - command Transform > Create Dummy Variables ; select (numerical) predictor and (categorical) moderator under 'Create Dummy Variables for:'
#   - under Create main-effect dummies (option checked by default) specify a short name for both variables, separated with a comma ; the name of the numerical variable is irrelevant but must be specified
#   - ensure that the option _Do not create dummies for scale variables values_ is selected under Measurement Level Usage
#   - select the _Create dummies for all two-way interactions_ option under Two-Way Interactions and give a short name, e.g., interact
#   - Note: this procedure can also be used to create a numerical by numerical interaction variable
```

#### PROCESS with more than 2 levels in moderator (Categorical)

If we have more than two levels in our moderator, we can use the same procedure as with the dichotomous analysis. @vid-SPSSrunModmorecat shows how to use PROCESS to estimate a moderation model with a categorical moderator with more than two levels. The example uses the [smokers.sav](data/smokers4.sav) data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable _status3_).

In the dialog box of PROCESS, we select the dependent variable (attitude) as `Y variable`, the predictor (exposure) as `X variable`, and the moderator (status3) as `Moderator variable W`. We select `Model 1` for a moderation model with one moderator. Under `Options`, we select `generate code to visualize interaction`.

::: {.callout-important appearance="simple"}
Instructions on how to add PROCESS to your SPSS installation can be found in [Appendix -@sec-installPROCESS].
:::

In addition we have to let PROCESS know that we have a categorical moderator with more than two levels. We do this by clicking the `Multicategorical` button and indicating that the `Moderator variable W` is multicatigorical. Click `continue` and click on `OK` to run the analysis. @vid-SPSSrunModmorecat shows how to use PROCESS to estimate a moderation model with a categorical moderator with more than two levels.

::: {#vid-SPSSrunModmorecat}
{{< video https://youtu.be/WA7hSVy2bpA
    width="100%"
    height="315" 
>}}

Running a moderation analysis in SPSS using PROCESS with a categorical moderator with more than 2 levels and a continuous predictor.
:::

When the analysis is finished, we can find the output in the SPSS output window. By double clicking on the output, we can select the generated code for visualizing the interaction. Copy the code, open a new syntax window (`File > New > Syntax`), and paste the code into the new syntax window. We can run this code to create a plot of the interaction.

@vid-SPSSinterpretModmorecat shows how to interpret the output of PROCESS for a categorical moderator with more than two levels. The example uses the [smokers.sav](data/smokers4.sav) data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable _status3_).

::: {#vid-SPSSinterpretModmorecat}
{{< video https://youtu.be/1yLUdqFyXkM
    width="100%"
    height="315" 
>}}

Interpretation of SPSS PROCESS output with a categorical moderator with more than 2 levels and a continuous predictor.
:::

#### Common support {#sec-commonsupportdichotomous}

In a regression model with moderation, we have to interpret the effect of a predictor involved in an interaction at a particular value of the moderator (@sec-conditional-effects). The estimated effect at a particular value of the moderator can only be trusted if there are quite some observations at or near this value of the moderator. In addition, these observations should cover the full range of values on the predictor. After all, the effect that we estimate must tell us whether high values on the predictor go together with higher (or lower) values on the dependent variable than low values on the predictor.

For example, we need quite some observations for smokers to estimate the conditional effect of exposure on attitude for smokers. If there are hardly any smokers in our sample, we cannot estimate the effect of exposure on attitude for them in a reliable way. Even if we have quite some observations for smokers but all smokers have low exposure, we cannot say much about the effect of exposure on attitude for them. If we cannot say much about the effect within this group, we cannot say much about the difference between this effect and effects for other groups. In short, the moderation model is problematic in this situation.

::: {#fig-common-support .column-page-inset-right}

```{=html}
<iframe src="https://sharon-klinkenberg.shinyapps.io/common-support/" width="100%" height="305px" style="border:none;">
</iframe>
``` 
How well do the observations cover the predictor within each category of smoking status?
:::


The variation of predictor scores for a particular value of the moderator is called *common support* [@RefWorks:3838]. If common support for predictors involved in moderation is poor, we should hesitate to draw conclusions from the estimated effects. Guidelines for good common support are hard to give. Common support is usually acceptable if there are observations over the entire range of the predictor.

It is recommended to check the number of observations per value of the moderator. For a categorical moderator, such as smoking status, a scatter plot of the dependent variable (vertical axis) by predictor (horizontal axis) with dots coloured according to the moderator category may do the job. The left panel of @fig-common-support shows an example. Check that there are observations for more or less all values of the predictor in each color. If the scatter plot is hard to read, create a histogram of predictor values grouped by moderator categories, as in the right panel of @fig-common-support.

##### SPSS check common support

@vid-SPSSregSupport1 shows how to check common support for a predictor at different moderator values in SPSS. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable _status2_), using contact with smokers as a covariate.

::: {#vid-SPSSregSupport1}

{{< video https://www.youtube.com/embed/kxX4rqsyWQQ
    width="100%"
    height="315" 
>}}

Check common support for a predictor at different moderator values in SPSS.
:::

```{r, echo=FALSE, eval=FALSE}
# Goal: Check common support for a predictor at different moderator values in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# Technique: 
# SPSS menu: 
# Interpret output: 
# Check assumptions: 
#
# * Check common support: 
#   - make histograms of the predictor panelled by the categorical grouping variable ; check that there are observations for more or less all values of the predictor (on the X axis) 
```


#### Visualizing moderation and covariates

A plot with different regression lines for different categories of the moderator is a very useful way of presenting your results. We can, however, only plot a regression line if we have a single independent variable. After all, we only have one horizontal (X) axis in a plot to display a predictor.

In a moderation model, we have at least two independent variables: the predictor and the moderator. For example, exposure is our predictor because our interest focuses on the effect of campaign exposure on attitude. Participant's smoking status is the moderator because we expect different exposure effects for participants with different smoking statuses. We may even have additional independent variables for which we want to control (more on this in @sec-confounder), for example, a participant's contacts with smokers. Let us call these additional independent variables *covariates*.

::: {.callout-important appearance="simple"}

A _predictor_ is the independent variable that is currently central to our analysis.

A _moderator_ is an independent variable for which we expect different effects of the predictor.

A _covariate_ is an independent variable that is currently not central to our analysis.

:::

Note that the distinction between predictor, moderator, and covariate is temporary. As soon as we focus on another variable, that variable becomes the predictor and the other predictors become moderators or covariates. The distinction between predictor, moderator, and covariate is just terminology to show on which variable we focus.

```{r}
#| label: fig-moderator-covar-statistical
#| fig-cap: "Statistical diagram of moderation with contact as covariate."
#| echo: false
#| warning: false

library(ggplot2)
# Create coordinates for the variable names.
variables <- data.frame(x = c(rep.int(0.3, times = 4), 0.7), 
                        y = c(.4, .3, .2, .1, .25),
                        label = c("Exposure", "Smoker", "Exposure*Smoker", "Contact", "Attitude"))
# Add coordinates for arrow endpoint.
x_diff <- 0.04
variables$xend <- variables$x[5] - x_diff #fixed translation to the left
variables$yend <- variables$y[5] + x_diff * (variables$y - variables$y[5]) / (variables$x[5] - variables$x)
rm(x_diff)
ggplot(variables, aes(x, y)) + 
  geom_segment(aes(x = x[1], y = y[1], xend = xend[1], yend = yend[1]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[2], y = y[2], xend = xend[2], yend = yend[2]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[3], y = y[3], xend = xend[3], yend = yend[3]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_segment(aes(x = x[4], y = y[4], xend = xend[4], yend = yend[4]), arrow = arrow(length = unit(0.04, "npc"), type = "closed")) + 
  geom_label(aes(label=label)) + 
  coord_cartesian(xlim = c(0.2, 0.8), ylim = c(0.1, 0.5)) +
  theme_void()
```


@fig-moderator-covar-statistical shows the statistical diagram of a moderation model with contact as covariate and @tbl-dich-moderator-cov-output summarizes the estimated effects. How can we get rid of the moderator and covariate(s), so exposure is left as the only independent variable and we can plot regression lines for exposure effects?

```{r}
#| label: tbl-dich-moderator-cov-output
#| fig-cap: "Predicting attitude towards smoking: regression analysis results with contact as covariate."
#| echo: false
#| warning: false

# Table of regression coefficients for exposure moderated by status. Similar to SPSS output (with correct standardized coefficients).
# Create effect sizes.
smokers <- haven::read_spss("data/smokers.sav")
model_1 <- lm(attitude ~ exposure*status2 + contact, data = smokers)
# Table with results in SPSS style.
results <- coef(summary(model_1))
# Confidence intervals
ci <- confint.lm(model_1)
results <- cbind(results, ci)
# Correctly standardized coefficients.
attach(smokers)
z_exposure <- (exposure - mean(exposure))/sd(exposure)
z_status2 <- (status2 - mean(status2))/sd(status2)
z_expostatus2 <- z_exposure * z_status2
z_attitude <- (attitude - mean(attitude))/sd(attitude)
z_contact <- (contact - mean(contact))/sd(contact)
model_2 <- lm(z_attitude ~ z_exposure + z_status2 + z_contact + z_expostatus2)
results_2 <- coef(summary(model_2))
results <- cbind(results[, 1:2], results_2[, 1], results[, 3:6])
results[1, 3] <- NA
# Move effect of contact to the last row of the results table.
results <- rbind(results[1:3,], results[5,], results[4,])
# Adjust parameter names
attributes(results)$dimnames[[1]][1] <- "(Constant)"
attributes(results)$dimnames[[1]][2] <- "Exposure"
attributes(results)$dimnames[[1]][3] <- "Smoker (0 = no, 1 = yes))"
attributes(results)$dimnames[[1]][4] <- "Exposure*Smoker"
attributes(results)$dimnames[[1]][5] <- "Contact"
# Table.
options(knitr.kable.NA = '')
knitr::kable(results, digits = 3, booktabs = TRUE,
             col.names = c("B", "Std. Error", "Beta", "t", "Sig.", "Lower Bound", "Upper Bound")) %>%
  kable_styling(font_size = 12, full_width = F,
                latex_options = c("scale_down", "HOLD_position")) %>%
  column_spec(1, color = "white",
              background = c("#FDAE61", "#2B83BA", "#ABDDA4", "#D7191C", "brown")) %>%
  column_spec(2, color = c("#FDAE61", "#2B83BA", "#ABDDA4", "#D7191C", "brown"))
# Helper function for displaying results within the text.
source("report_n.R")
# Store average covariate value for a later plot.
contact_avg = mean(contact)
#Cleanup (partial, smokers and results are saved for inline use).
rm(model_1, ci, results_2, model_2, z_attitude, z_status2, z_expostatus2, z_exposure, z_contact)
```

As a first step, use the estimated values of the regression coefficients in the SPSS output (@tbl-dich-moderator-cov-output) to create a regression equation (@eq-intvarcov1). Just start at the top of the table and write down the regression coefficients (*B*) and the independent variable names.

$$
\small
\begin{split}
  attitude = &\ \color{#FDAE61}{`r report_n(results[1,1], 3)`} + \color{#2B83BA}{`r report_n(results[2,1], 3)`*exposure} + \color{#ABDDA4}{`r report_n(results[3,1], 3)`*smoker} + \color{#D7191C}{`r report_n(results[4,1], 3)`*exposure*smoker}\\
  &+ \color{brown}{`r report_n(results[5,1], 3)`*contact}
\end{split}
\normalsize
$$ {#eq-intvarcov1}

As a second step, choose an interesting value for every independent variable in the equation except the predictor. If we want to have the regression line for smokers, choose 1 for the *smoker* variable. For a numerical covariate such as *contact*, it is recommended to choose the mean. Average contact with smokers happens to be <span style="color:Brown;">`r report_n(mean(smokers$contact),3)`</span> in our example. Now replace the independent variables in the equation by the selected values and simplify the equation (@eq-intvarcov2).

$$
\small
\begin{split}
  attitude = &\ \color{#FDAE61}{`r report_n(results[1,1], 3)`} + \color{#2B83BA}{`r report_n(results[2,1], 3)`*exposure} + \color{#ABDDA4}{`r report_n(results[3,1], 3)`*smoker} + \color{#D7191C}{`r report_n(results[4,1], 3)`*exposure*smoker}\\
  &+ \color{brown}{`r report_n(results[5,1], 3)`*contact}\\
  attitude = &\ \color{#FDAE61}{`r report_n(results[1,1], 3)`} + \color{#2B83BA}{`r report_n(results[2,1], 3)`*exposure} + \color{#ABDDA4}{`r report_n(results[3,1], 3)`*1} + \color{#D7191C}{`r report_n(results[4,1], 3)`*exposure*1} + \color{brown}{`r report_n(results[5,1], 3)`*`r report_n(round(mean(smokers$contact),3),3)`}\\
  attitude = &\ \color{#FDAE61}{`r report_n(results[1,1], 3)`} + \color{#2B83BA}{`r report_n(results[2,1], 3)`*exposure} + \color{#ABDDA4}{`r report_n(results[3,1], 3)`} + \color{#D7191C}{`r report_n(results[4,1], 3)`*exposure} + \color{brown}{`r report_n(results[5,1] * round(mean(smokers$contact),3),3)`}\\
  attitude = &\ (\color{#FDAE61}{`r report_n(results[1,1], 3)`} + \color{#ABDDA4}{`r report_n(results[3,1], 3)`}+ \color{brown}{`r report_n(results[5,1] * round(mean(smokers$contact),3),3)`}) + (\color{#2B83BA}{`r report_n(results[2,1], 3)`*exposure} + \color{#D7191C}{`r report_n(results[4,1], 3)`*exposure})\\
  attitude = &\ `r report_n(round(results[1,1],3) + round(results[3,1],3) + (round(results[5,1],3) * round(mean(smokers$contact),3)),3)` + `r report_n(round(results[2,1],3) + round(results[4,1],3), 3)`*exposure
\end{split}
\normalsize
$$ {#eq-intvarcov2}

The terms with *smoker* and *contact* disappear from the equation, so *exposure* is the only independent variable that remains in the equation. Now, we can draw the simple regression line predicting attitude from exposure for smokers using this equation. Note that this is the regression line for people with average contact with smokers.

Repeat these steps but plug in the score 0 for the *smoker* predictor to obtain the simple regression line for non-smokers. @fig-dich-moderator-cov-plot shows the two regression lines and their equations. The effect of exposure on attitude is more strongly negative for smokers than for non-smokers.

```{r}
#| label: fig-dich-moderator-cov-plot
#| fig-cap: "The effects of exposure on attitude for non-smokers and smokers. Both smokers and non-smokers are assumed to have average contact with smokers."
#| echo: false
#| warning: false

#Create and show the plot.
ggplot(smokers, aes(x = exposure, y = attitude, 
                    colour = factor(status2,
                                    labels = c("Non-smoker", "Smoker")))) +
  geom_point() +
  geom_vline(xintercept = 0) +
  geom_abline(aes(
      intercept = results[1,1] + results[5,1]*mean(contact), 
      slope = results[2,1],
                colour = "Non-smoker"
      ),
      show.legend = F
    ) +
  geom_text(aes(x = 0.1, y = -4.9,
                label = paste0(
                  "Non-smokers: attitude = ",
                  report_n(round(results[1,1], 3) + round(results[5,1], 3)*round(mean(contact), 3),3),
                  " + ",
                  report_n(results[2,1],3),
                  " * exposure"
                ),
                hjust = 0,
                vjust = 0,
                colour = "Non-smoker"
                ),
      show.legend = F
            ) +
  geom_abline(aes(
      intercept = results[1,1] + results[3,1] + results[5,1]*mean(contact), 
      slope = results[2,1] + results[4,1],
      colour = "Smoker"),
    show.legend = F
    ) +
  geom_text(aes(x = 9.9, y = 4.9,
                label = paste0(
                  "Smokers: attitude = ",
                  report_n(round(results[1,1], 3) + round(results[3,1], 3) + round(results[5,1], 3)*round(mean(contact), 3),3),
                  " + ",
                  report_n(round(results[2,1], 3) + round(results[4,1], 3),3),
                  " * exposure"
                ),
                hjust = 1,
                vjust = 1,
                colour = "Smoker"
                ),
      show.legend = F
            ) +
  scale_colour_manual(
    name = "Smoking status",
    values = c("Non-smoker" = unname(brewercolors["Blue"]),
               "Smoker" = unname(brewercolors["Red"]))
    ) +
  scale_x_continuous(
    name = "Exposure",
    limits = c(0,10)
  ) +
  scale_y_continuous(
    name = "Attitude",
    limits = c(-5, 5),
    breaks = c(
      0,
      round(round(results[1,1], 3) + round(results[5,1], 3)*round(mean(smokers$contact), 3),3),
      round(round(results[1,1], 3) + round(results[3,1], 3) + round(results[5,1], 3)*round(mean(smokers$contact), 3),3)
    )
  ) +
  theme_general() +
  theme(legend.position = "bottom")

```

##### SPSS visualizing interaction

Though PROCESS can visualize the interaction between a continuous predictor and a categorical moderator, you sometimes want to have some more control over the plot. In this case, you can use the `Graphs > Legacy Dialogs > Scatter/Dot` menu in SPSS to create a scatterplot with regression lines for different groups of the moderator.

@vid-SPSSregmodlines shows how to visualize the interaction between a continuous predictor and a categorical moderator in SPSS. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable _status3_), using contact with smokers as a covariate.

::: {#vid-SPSSregmodlines}

{{< video https://www.youtube.com/embed/5KgpEjFzHiQ
    width="100%"
    height="315" 
>}}

Visualizing moderation by regression lines in a scatterplot in SPSS.
:::

```{r, echo=FALSE, eval=FALSE}
# Goal: Representing moderation by regression lines in a scatterplot in SPSS.
# Example: smokers.sav, predict the attitude towards smoking from exposure moderated by smoking status (variable _status2_), use contact with smokers as a covariate.
# Technique: linear regression
# SPSS menu: 
# Interpret output: 

# * Visualize categorical moderator with reference lines in scatterplot:
#   - {skip} No covariates: Graphs > Regression Variable Plots: dependent variable on Y, predictor on X, categorical moderator in Color by, Options>Scatterplot Fit Lines > Linear and Grouping > Fit line for each categorical colour group
#   - With covariates: 
#     - create scatterplot with Graphs > Legacy Dialogs > Scatter/Dot > Simple Scatter
#     - select dependent variable under Y Axis: and (numerical) predictor under X Axis: 
#     - select (original) categorical moderator under Set Markers by: (colours the observations according to moderator value, useful for inspecting common support)
#     - Paste & Run 
#     - display the mean values of covariates with Analyze > Descriptive Statistics > Fequencies ; select Statistics > Mean
#     - in SPSS Output, double-click the scatterplot to open it in the Chart Editor 
#     - add reference line (Options > Reference Line from Equation) ; in the Properties window under the Reference Line tab, add the regression equation for the first group in the moderator variable: use x for the predictor displayed on the X axis and use the category value (0/1) for dummies and the average values for numerical covariates
#     - repeat for other categories of the moderator variable
#     - change type (or colour) of the line in the Properties window under the Lines tab
#     - if you like, add label to lines describing the moderator group: Options > Text Box
#     - close the Chart Editor
```











## Take-Home Points

-   We use regression anaclysis if our dependent variable is numeric and we have at least one numeric independent (predictor) variable.

-   We use dummy variables to include a categorical variable as a predictor in a regression model. We need a dummy (1/0) variable for each category on the categorical variable except for one category, which represents the reference group.

-   We use an *F* test to test the null hypothesis that the regression model does not help to predict the dependent variable in the population. We use a *t* test to test the null hypothesis that a regression coefficient is zero in the population.

-   In a regression model, moderation means that there are different slopes (effects of the predictor) for different groups or contexts (moderator).

-   Interaction variables represent moderation in a regression model.

-   An interaction variable is the product of the predictor and moderator. If a categorical moderator is represented by one or more dummy variables, we need an interaction variable for each of the moderator's dummy variables.

-   Statistical inference for an interaction variable is exactly the same as for "ordinary" regression predictors.

-   The effect of the predictor in a model with an interaction variable does *not* represent a main or average effect. It is a conditional effect: The effect for cases that score zero on the moderator.

-   To interpret moderation, describe the effects (slopes, unstandardized regression coefficients) and visualize the regression lines for different groups.

-   Warn the reader if the predictor scores are not nicely distributed for all groups or levels (no common support).

-   Don't use the standardized regression coefficients (Beta) for interaction variables, variables included in interactions, or for dummy variables in SPSS.

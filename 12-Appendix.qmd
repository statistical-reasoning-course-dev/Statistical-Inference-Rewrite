```{r setup, include=FALSE}
source("R_setup.R")
```

# Appendix {-}

## Flow chart statistical test selection {-}

The flow chart in @fig-choice-diagram-copy helps you select the appropriate statistical test based on the type of dependent and independent variables. 

It helps in the learning proces to think of different research designs for each test. Try to think of one for the independent samples _t_-test. Work your way from right to left to determine the type of variables and think of some research that conforms to this.

```{r choice-diagram-copy, fig.width=9, echo=FALSE, fig.cap="Flow chart for statistical test selection."}
source("flowchart.R")
p
rm(p)
```

An interactive tool for test selection made by [Matt Jackson-Wood](https://scholar.google.co.uk/citations?user=MUanZDgAAAAJ&hl=en) can be found [here](https://www.statsflowchart.co.uk).

## All SPSS Tutorial Videos List {-}

Chapter @sec-probmodels

* @vid-SPSSbootstrap1: Bootstrapping in SPSS.
* @vid-SPSSbootstrap2: Interpreting bootstrap results in SPSS.
* @vid-SPSSExact1: Performing an exact test in SPSS.
* @vid-SPSSExact2: Interpreting exact test results in SPSS.

Chapter @sec-param-estim

* @vid-SPSSconflevel: Setting the confidence level in SPSS.

Chapter @sec-hypothesis

* @vid-SPSSbinomial: A binomial test on a single proportion in SPSS.
* @vid-SPSSchisq1: A chi-squared test on a frequency distribution in SPSS.
* @vid-SPSS1mean: A one-sample _t_ test in SPSS.

Chapter @sec-anova

* @vid-SPSS1way: One-way analysis of variance (ANOVA) in SPSS.
* @vid-SPSS2way: Two-way analysis of variance with moderation in SPSS.
* @vid-SPSSeta2: Calculating eta^2^ from SPSS output.

Chapter @sec-moderationcat

* @vid-SPSSregsimple: Executing and interpreting regression analysis in SPSS.
* @vid-SPSSregdummy2: Creating dummy variables in SPSS.
* @vid-SPSSregdummy: Using dummy variables in a regression model in SPSS.
* @vid-SPSSregassumpt: Checking assumptions for regression models in SPSS.
* @vid-SPSSregpred: Creating categorical by numerical interaction predictors for regression in SPSS.
* @vid-SPSSregcatmod: Estimating categorical by numerical moderation with regression in SPSS.
* @vid-SPSSregmodlines: Representing moderation by regression lines in a scatterplot in SPSS.
* @vid-SPSSregSupport1: Checking common support for a predictor at different moderator values in SPSS.

Chapter @sec-moderationcont

* @vid-SPSSregcenter: Mean-centering variables for regression analysis in SPSS.
* @vid-SPSSreglines2: Regression lines for a numerical moderator in a scatterplot in SPSS.
* @vid-SPSSregSupport2: Checking common support with a numerical moderator in SPSS.

Chapter @sec-confounder

* @vid-SPSSregconfound: Identifying confounders with regression in SPSS.

Chapter @sec-mediation

* @vid-SPSSmediatpar: Estimating a single or parallel mediation model with PROCESS (Model 4).
* @vid-SPSSmediatserial: Estimating a serial mediation model with PROCESS (Model 6).
* @vid-SPSSmediatcov: Estimating a mediation model including covariates with PROCESS.
* @vid-SPSSpath: Estimating a path model in SPSS.

Appendix: Variance / Association

* @vid-SPSSLevene: Levene's _F_ test on equal variances in SPSS.
* @vid-SPSSTindep: Independent samples _t_ test on two means.
* @vid-SPSSTpaired: Paired samples _t_ test on two means.
* @vid-SPSScorrelation: Test on a correlation.
* @vid-SPSSchisqcross: Chi-squared test on a contingency table (crosstab).
* @vid-cohend: Obtaining Cohen's _d_ with SPSS.

## Formulating Statistical Hypotheses {-}

A _research hypothesis_ is a statement about the empirical world that can be tested against data. Communication scientists, for instance, may hypothesize that:  

* a television station reaches half of all households in a country,
* media literacy is below a particular standard (for instance, 5.5 on a 10-point scale) among children,  
* opinions about immigrants are not equally polarized among young and old voters,  
* the celebrity endorsing a fundraising campaign makes a difference to people's willingness to donate,  
* more exposure to brand advertisements increases brand awareness,
* and so on.  

As these examples illustrate, research hypotheses seldom refer to statistics such as means, proportions, variances, or correlations. Still, we need such statistics to test a hypothesis. The researcher must translate the hypothesis into a new hypothesis specifying a statistic in the population, for example, the population mean. The new hypothesis is called a _statistical hypothesis_.  

Translating the research hypothesis into a statistical hypothesis is perhaps the most creative part of statistical analysis, which is just a fancy way of saying that it is difficult to give general guidelines stating which statistic fits which research hypothesis. All we can do is give some hints.  

Research questions usually address shares, score levels, associations, or score variation. If a research question talks about how frequent some characteristic occurs (How many candies are yellow?) or which part has a particular characteristic (Which percentage of all candies are yellow?), we are dealing with one or two categorical variables. Here, we need a binomial, chi-squared, or exact test (see @fig-choice-diagram-copy).

If a research question asks how high a group scores or whether one group scores higher than another group, we are dealing with score levels. The variable of central interest usually is numerical (interval or ratio measurement level) and we are concerned with mean or median scores. There is a range of tests that we can apply, depending on the number of groups that we want to compare (one, two, three or more): _t_ tests or analysis of variance. 

Instead of comparing mean scores of groups, a research question about score levels can address associations between numerical variables, for example, Are heavier candies more sticky? Here, the score level on one variable (candy weight) is linked to the score level on another variable (candy stickiness). This is where we use correlations or regression analysis.

Finally, a research question may address the variation of numeric scores, for example, Does the weight of yellow candies vary more strongly than the weight of red candies? Variance is the statistic that we use to measure variation in numeric scores.

## Proportions: shares {-}

A proportion is the statistic best suited to test research hypotheses addressing the share of a category or entity in the population. The hypothesis that a television station reaches half of all households in a country provides an example. All households in the country constitute the population. The share of the television station is the proportion or percentage of all households watching this television station.  

If we want to use a statistic, we need to know the variable and cases (units of analysis) for which the statistic must be calculated. In this example, a household does or does not watch the television station, so our variable is a dichotomy with the two categories ("No, does not watch this station", "Yes, watches this station") usually coded as 0 versus 1 or 1 versus 2.  

Each household provides an observation, namely either the score 0 or the score 1 on this variable or no score if there are missing values. To test the research hypothesis that a television station reaches half of all households in a country, we have to formulate a statistical hypothesis about the proportion---of households viewing this television station---in the population---all households in this country. For example, the researcher's statistical hypothesis could be that the proportion in the population is 0.5.  

We can also be interested in more than two categories, for instance, does the television station reach the same share of all households in the north, east, south, and west of the country? This translates into a statistical hypothesis containing three or more proportions in the population. If 30% of households in the population are situated in the west, 25 % in the south and east, and 20% in the north, we would expect these proportions in the sample if all regions are equally represented. Our statistical hypothesis is actually a relative frequency distribution, such as, for instance, in @tbl-hypo-freq2.

```{r hypo-freq2, echo=FALSE}
knitr::kable(data.frame(Region = c("North", "East", "South", "West"), HP = c(0.20, 0.25, 0.25, 0.30)), digits = 2, caption = "Statistical hypothesis about four proportions as a frequency table.", col.names = c("Region", "Hypothesized Proportion"), booktabs = TRUE) %>%
  kable_styling(font_size = 12, full_width = F, position = "float_right")
```

A test for this type of statistical hypothesis is called a one-sample chi-squared test. It is up to the researcher to specify the hypothesized proportions for all categories. This is not a simple task: What reasons do we have to expect particular values, say a region's share of thirty per cent of all households instead of twenty-five per cent?

The test is mainly used if researchers know the true proportions of the categories in the population from which they aimed to draw their sample. If we try to draw a sample from all citizens of a country, we usually know the frequency distribution of sex, age, educational level, and so on for all citizens from the national bureau of statistics. With the bureau's information, we can test if the respondents in our sample have the same distribution with respect to sex, age, or educational level as the population from which we tried to draw the sample; just use the official population proportions in the null hypothesis. 

If the proportions in the sample do not differ more from the known proportions in the population than we expect based on chance, the sample is _representative_ of the population _in the statistical sense_ (see @sec-representative). As always, we use the _p_ value of the test as the probability of obtaining our sample or a sample that is even more different from the null hypothesis, if the null hypothesis is true. Note that the null hypothesis now represents the (distribution in) the population from which we tried to draw our sample. We conclude that the sample is representative of this population in the statistical sense if we can _not_ reject the null hypothesis, that is, if the _p_ value is _larger_ than .05. Not rejecting the null hypothesis means that we have sufficient probability that our sample was drawn from the population that we wanted to investigate. We can now be more confident that our sample results generalize to the population that we meant to investigate.

### Testing proportions in SPSS {-}



::: {#vid-SPSSbinomial}

{{< video https://www.youtube.com/embed/tmdZpOSObco
    width="100%"
    height="315" 
>}}

A binomial test on a single proportion.
:::



```{r, echo=FALSE, eval=FALSE}

# A binomial test on a single proportion can be executed in SPSS with the command _Analyze > Nonparametric Tests > Binomial_. In the dialog , you have to enter a variable. 
# 
# If this variable is a dichotomy (it has only two values), you can leave the _Define Dichotomy_ option at "Get from data". SPSS will use the first category score that it encounters in the data set as the test category. This is tricky. It will test the sample proportion of this value against the test proportion that you specify elsewhere in this dialog.
# 
# If the variable has more than two categories or you want to be sure about the category that you use for the test, use the "Cut point" option under _Define Dichotomy_ to divide all scores into two groups. The lowest score up to and including the cut point are used as the test category.
# 
# The statistics under Options are not interesting if you just want to test a proportion.
#
# Figure shows the output. The sample proportion does not differ significantly from 0.5. In this example, we would report: "We cannot reject the hypothesis that the TV station reaches half of all households, _p_ = .784."
#
# The one-sided versus two-sided output is not discussed in the video.
```


::: {#vid-SPSS1mean}

{{< video https://www.youtube.com/embed/Gupx40D5bLY
    width="100%"
    height="315" 
>}}

Execute a one sample t test in SPSS,
:::



```{r, echo=FALSE, eval=FALSE}
# To execute a one sample t test in SPSS, use the command _ANALYZE > COMPARE MEANS > ONE SAMPLE T TEST_. Select a numeric variable in the dialog and enter the hypothesized population mean under _Test Value_. 
# 
# Media literacy is measured on a ten point scale. Is average media literacy (in the population) equal to 5.5? A one sample t test tells us that average media literacy in our sample (_M_ = 4.47, _SD_ = 1.64) is statistically significantly different from 5.5, _t_ (86) = -5.87, _p_ < .001, 95% CI [-1.38, -0.68]. We are confident that the population average media literacy score is 0.68 to 1.38 below 5.5, so somewhere between 4.12 and 4.82.

```


For a binomial test, see the video in @fig-SPSSbinomial. A one-sample chi-squared test is explained in the video of @fig-SPSSchisq1.



::: {#vid-SPSSchisq1}

{{< video https://www.youtube.com/embed/8DAau9jFUhA
    width="100%"
    height="315" 
>}}

One-sample chi-squared test.
:::



```{r, echo=FALSE, eval=FALSE}
# If we want to test a frequency distribution against a known or hypothesized population distribution, we must use a one-sample chi-squared test. This test is available in SPSS with the command _ANALYZE > NONPARAMETRIC TESTS > LEGACY DIALOGS > CHI SQUARE_. Select the categorical variable for which you want to test the distribution under _Test variable List_.
# 
# Select the option _All categories equal_ under _Expected Values_ if you hypothesize that all categories have the same proportions in the population. In the example, we hypothesize that households are equally distributed over the four regions. This is a plausible hypothesis if the four regions are known to contain a quarter of all households in the country or if the sample was stratified by region, that is, every region was meant to deliver the same number of households to the sample.
# 
# If the hypothesized distribution is not equal over all categories, specify the expected proportions, percentages, or sample frequencies under _Values_. You must specify an expected value for each category in the exact order in which the categories are coded. Be careful not to make mistakes. 
# 
# Although the frequencies of the four regions are not exactly the same in the sample, the hypothesis of equal population frequencies cannot be rejected, Chi-square (3) = 3.27, _p_ = .352.

```


## Mean and median: level {-}
Research hypotheses that focus on the level of scores are usually best tested with the mean or another measure of central tendency such as the median value. For example, the hypothesis that media literacy is below a particular standard (e.g., 5.5 on a 10-point scale) among children refers to a level: the level of media literacy scores. 

The hypothesis probably does not argue that all children have a media literacy score below 5.5. Instead, it means to say that the overall level is below this standard. The center of the distribution offers a good indication of the general score level.

For a numeric (interval or ratio measurement level) variable such as the 10-point scale in the example, the mean is a good measure of the distribution's center. In this example, our statistical hypothesis would be that average media literacy score of all children in the population is (below) 5.5.  

### Testing one mean or median in SPSS {-}

The one-sample _t_ test in SPSS is explained in @fig-SPSS1mean.

## Variance: (dis)agreement {-}
Although rare, research hypotheses may focus on the variation in scores rather than on score level. The hypothesis about polarization provides an example. Polarization means that we have scores well above the center and well below the center rather than all scores concentrated in the middle. If voters' opinions about immigrants are strongly polarized, we have a lot of voters strongly in favour of admitting immigrants as well as many voters strongly opposed to admitting immigrants.  

For a numeric variable, the variance or standard deviation---the latter is just the square root of the former---is the appropriate statistic to test a hypothesis about polarization. The research hypothesis concerns the variation of scores in two groups, for instance, young versus old voters. The statistical hypothesis would be that the variance in opinions in the population of young voters is different from the variance in the population of old voters.  

### Testing two variances in SPSS {-}

#### Instructions {-}

::: {#vid-SPSSLevene}

{{< video https://www.youtube.com/embed/8k-OIXiRdCc
    width="100%"
    height="315" 
>}}

Levene's F test in SPSS
:::



```{r, echo=FALSE, eval=FALSE}
# Levene's F test for the null hypothesis that two groups have equal population variances is part of the independent-samples t test. 

# The output in @fig-Levene-output shows that the polarization (variation) in attitudes towards immigrants is not the same among young and old voters, _F_ = 4.99, _p_ = .029. Polarization is stronger among old voters (_SD_ = 2.06) than among young voters (_SD_ = 1.26). 
# 
# In a similar way, we can test if two, three or more groups have equal population variances with Levene's F test using the option _Homogeneity of variance test_ in a one-way analysis of variance (command _Analyze >  COMPARE MEANS > ONE-WAY ANOVA_).

```

#### Exercises {-}











## Association: relations between characteristics {-}

Finally, research hypotheses may address the relation between two or more variables. Relations between variables are at stake if the research hypothesis states or implies that one (type of) characteristic is related to another (type of) characteristic. The statistical name for a relation between variables is _association_.  

Take, for example, an analysis of the effect of a celebrity endorser on the willingness to donate. Here, the endorser to whom a person is exposed (one characteristic) is related to this person's willingness to donate (another characteristic). Another example: If exposure to the campaign increases willingness to donate, a person's willingness to donate is positively related to this person's exposure to the campaign.  

### Score level differences {-}

Association comes in two related flavors: a difference in score level between groups or the predominance of particular combinations of scores on different variables.

The relation between the endorser's identity and willingness to donate is an example of the first flavor. All people are confronted with one of the celebrities as endorser of the fund-raising campaign. This is captured by a categorical variable: the endorsing celebrity. 

The categorical variable clusters people into groups: One group is confronted with Celebrity A, another group with Celebrity B, and so on. If the celebrity matters to the willingness to donate, the general level of donation willingness should be higher in the group exposed to one celebrity than in the group exposed to another celebrity.

Thus, we return to statistics needed to test research hypotheses about score levels, namely measures of central tendency. If willingness to donate is a numeric variable, we can use group means to test the association between endorsing celebrity (grouping variable) and willingness to donate (score variable). The statistical hypothesis would then be that group means are not equal in the population of all people. 

If you closely inspect the choice diagram in @fig-flowchart, you will see that we prefer to use a _t_ distribution if we compare two different groups (independent-samples _t_ test) or two repeated observations for the same group (paired-samples _t_ test). By contrast, if we have three or more groups, we use analysis of variance with an _F_ distribution. 

### Comparing means in SPSS {-}

#### Instructions {-}

::: {#vid-SPSSTindep}

{{< video https://www.youtube.com/embed/g4O0oTVm-Tk
    width="100%"
    height="315" 
>}}

Association as level differences between groups.
:::



```{r, echo=FALSE, eval=FALSE}
# Goal: association as level differences between groups: are females more wiling to donate to a fund raiser than males?
# Example: donors.sav, outcome is willingness to donate (post), predictor (grouping variable) is sex (0, 1).
# Technique: independent-samples t test.
# SPSS menu: Compare Means ; options: check level of confidence interval.
# Paste & Run.
# Interpret output: choose right row in test table depending on Levene's F test ; test result and significance, confidence interval.
# Check assumptions: each group more than 30 observations or normally distributed (histogram panelled by sex).
```

----

::: {#vid-SPSSTpaired}

{{< video https://www.youtube.com/embed/7mGKG3mLqN0
    width="100%"
    height="315" 
>}}

Association as score level change.
:::



```{r, echo=FALSE, eval=FALSE}
# Goal: Association as score level change (due to intermediary treatment).
# Example: donors.sav, outcome is willingness to donate post and prior exposure to a fund raiser campaign.
# Technique: paired-samples t test ; repeated measurements.
# SPSS menu: Compare Means ; second selected variable will be subtracted form the first, so post-pre is better combination; options: check level of confidence interval.
# Paste & Run.
# Interpret output: test result and significance, confidence interval.
# Check assumptions: each measurement more than 30 observations or normally distributed (histogram per variable).
```

----

For an instruction and exercises on one-way analysis of variance, see @fig-SPSS1way and @sec-onewaySPSS. For two-way analysis of variance, see @sec-twowaySPSS (instructions and exercises).

#### Exercises {-}











### Combinations of scores {-}

The other flavor of association represents situations in which some combinations of scores on different variables are much more common than other combinations of scores. 

Think of the hypothesis that brand awareness is related to exposure to advertisements for that brand. If the hypothesis is true, people with high exposure and high brand awareness should occur much more often than people with high exposure and low brand awareness or low exposure and high brand awareness.

The two variables here are exposure and brand awareness. One combination of scores on the two variables is high exposure combined with high brand awareness. This combination should be more common than high exposure combined with low brand awareness.  

Measures of association are statistics that put a number to the pattern in combinations of scores. The exact statistic that we use depends on the measurement level of the variables. For numerical variables, measured at the interval or ratio level, we use Pearson's correlation coefficient or the regression coefficient. For ordinal variables with quite a lot of different scores, we use Spearman's rank correlation. 

For categorical variables, measured at the nominal or ordinal level, chi-squared indicates whether variables are statistically associated. The larger chi-squared, the more likely we are to conclude that the variables are associated in the population. If variables are not associated, they are said to be _statistically independent_.

Several measures exist that express the strength of the association between two categorical variables. We use Phi and Cramer's _V_ (two nominal variables, symmetric association), Goodman & Kruskals tau (two nominal variables, asymmetric association), Kendalls tau-b (two categorical ordinal variables, symmetric association), and Somers' _d_ (two categorical ordinal variables, asymmetric association).

### Testing associations in SPSS {-}

#### Instructions {-}

::: {#vid-SPSScorrelation}

{{< video https://www.youtube.com/embed/2g3Oyfe76h0
    width="100%"
    height="315" 
>}}

Association as combinations of scores.
:::



```{r, echo=FALSE, eval=FALSE}
# Goal: association as combinations of scores
# Example: consumers.sav, brand awareness by advertisement exposure
# Technique: Pearson and Spearman correlations for two numeric variables
# Check data: check linearity in scatterplot, add lines for means to explain combinations of scores that determine the correlation, add regression line to point out the influence of a bivariate outlier ; Spearman's rank correlation is less sensative to bivariate outliers
# SPSS menu: Correlate > Bivariate
# Paste & Run.
# Interpret output: size and statistical significance, no confidence interval  ; use bootstrapping for a confidence interval (see other video)
# Check assumptions: for using t distribution, Pearson: histograms with normal distribution (mention don't show), Spearman: samples size over 30 
```

----

::: {#vid-SPSSchisqcross}

{{< video https://www.youtube.com/embed/Nqtg8q-S0k8
    width="100%"
    height="315" 
>}}

Association as relatively frequent/infrequent category combinations.
:::



```{r, echo=FALSE, eval=FALSE}
# Goal: association as relatively frequent/infrequent category combinations
# Example: consumers.sav, word of mouth and sex
# Technique: cross-tabulation
# SPSS menu: Descriptive Statistics > Crosstabs with option Display clustered bar charts ; Statistics: chi square ; Phi and Cramer's V (two nominal variables, symmetric association), Goodman & Kruskals tau (two nominal variables, asymmetric association), Kendalls tau-b (with lambda: two categorical ordinal variables, symmetric association), and Somers' d (two categorical ordinal variables, symmetric association) ; Cells: column percentages and expected frequencies
# Paste & Run.
# Check assumptions: 2x2 table so Fisher's exact test ; larger tables: sufficient number of expected observations in each cell, a minimum of one and at least 80% above five
# Interpret output: statistical test ; association size ; association contents
```

----

For regression analysis (instructions and exercises), see @sec-SPSS-regression.

#### Exercises {-}















## Cohen's _d_ calculatons {#sec-CohenCalculations}

------------------------------------------------------------------------

These are the formulas for Cohen's *d* for a one-sample *t* test, a paired-samples *t* test, and an independent-samples *t* test (they will be provided if needed):

::: {style="column-count: 3; -moz-column-count: 3"}
```{=tex}
\begin{equation}
  d_{one_-sample} = \frac{M - \mu_0}{SD}
\end{equation}
```
```{=tex}
\begin{equation}
  d_{paired_-samples} = \frac{M_{diff} - \mu_{0_-diff}}{SD_{diff}}
\end{equation}
```
```{=tex}
\begin{equation}
  d_{independent_-samples} = \frac{2*t}{\sqrt(df)}
\end{equation}
```
:::

Where:

-   $M$ is the sample mean, $\mu_0$ is the hypothesized population mean, and $SD$ is the standard deviation in the sample,

-   $M_{diff}$ is the difference between the two means in the sample, $\mu_{0_-diff}$ is the hypothesized difference between the two means in the population mean, which is zero in case of a nil hypothesis, and $SD_{diff}$ is the standard deviation of the difference in the sample,

-   $t$ is the test statistic value and $df$ is the number of degrees of freedom of the *t* test.

------------------------------------------------------------------------

The sample outcome can be a single mean, for instance the average weight of candies, but it can also be the difference between two means, for example, the difference in colourfulness of yellow candies at the beginning and end of a time period. In the latter case, the standard deviation that we need is the standard deviation of colourfulness difference across all candies (@sec-dependentsamples). In the case of independent samples, such as average weight of red versus yellow candies, we need a special combined (*pooled*) standard deviation for yellow and red candy weight that is not reported by SPSS. Here, we use the *t* value and degrees of freedom to calculate Cohen's *d*.

### Obtaining Cohen's *d* with SPSS

::: {#vid-cohend}

{{< video https://www.youtube.com/embed/HmyW7HRM64Q
    width="100%"
    height="315" 
>}}

Obtaining Cohen's *d* with SPSS
:::

It is, relatively easy to calculate Cohen's _d_ by hand from SPSS output. Remember that we must divide the unstandardized effect by the standard deviation, though the latest versions of SPSS can also produce this in the output.

For a t test on one mean, the unstandardized effect is the difference between the sample mean and the hypothesized mean. SPSS reports this value in the column __Mean Difference__ of the table with test results. Drop any negative signs! Divide it by the standard deviation of the variable as given in Table __One-Sample Statistics__.

In the example, Cohen's _d_ is 0.036 / 0.169 = 0.21. This is a weak effect.

For a paired-samples t test, the unstandardized effect size is reported in the column __Mean__ in the Table __Paired Samples Test__. The standard deviation of the difference can be found in column __Std. Deviation__ in the same table. Divide the first by the second, for instance, 1.880 / 1.033 = 1.82. This is a strong effect.

For an independent-samples t test, the situation is less fortuitous because SPSS does not report the pooled sample standard deviation that we need. The pooled sample standard deviation takes a sort of  average of the outcome variable's standard deviations in the two groups. As an approximation, we can calculate Cohen's _d_ as follows: Double the t value and divide it by the square root of the degrees of freedom.

In the example, Cohen's _d_ equals $(2 * 0.651) / \surd(18) = 0.31$. This is a moderate effect size.
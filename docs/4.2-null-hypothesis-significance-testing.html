<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.2 Null Hypothesis Significance Testing | Statitstical Inference</title>
  <meta name="description" content="4.2 Null Hypothesis Significance Testing | Statitstical Inference" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="4.2 Null Hypothesis Significance Testing | Statitstical Inference" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.2 Null Hypothesis Significance Testing | Statitstical Inference" />
  
  
  

<meta name="author" content="Wouter de Nooy" />
<meta name="author" content="et al." />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4.1-binarydecision.html"/>
<link rel="next" href="4.3-confidence-intervals-to-test-hypotheses.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<link href="libs/vis-9.1.0/vis-network.min.css" rel="stylesheet" />
<script src="libs/vis-9.1.0/vis-network.min.js"></script>
<script src="libs/visNetwork-binding-2.1.2/visNetwork.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="styleX.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Introduction and Readerâ€™s Guide</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#intended-audience-and-setting"><i class="fa fa-check"></i>Intended Audience and Setting</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#interactive-content"><i class="fa fa-check"></i>Interactive Content</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#disclaimer"><i class="fa fa-check"></i>Disclaimer</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-samp-dist.html"><a href="1-samp-dist.html"><i class="fa fa-check"></i><b>1</b> Sampling Distribution: How Different Could My Sample Have Been?</a>
<ul>
<li class="chapter" data-level="" data-path="1-samp-dist.html"><a href="1-samp-dist.html#summary"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1.1" data-path="1.1-statistical-inference-making-the-most-of-your-data.html"><a href="1.1-statistical-inference-making-the-most-of-your-data.html"><i class="fa fa-check"></i><b>1.1</b> Statistical Inference: Making the Most of Your Data</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-discreterandomvariable.html"><a href="1.2-discreterandomvariable.html"><i class="fa fa-check"></i><b>1.2</b> A Discrete Random Variable: How Many Yellow Candies in My Bag?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="1.2-discreterandomvariable.html"><a href="1.2-discreterandomvariable.html#samplestatistic"><i class="fa fa-check"></i><b>1.2.1</b> Sample statistic</a></li>
<li class="chapter" data-level="1.2.2" data-path="1.2-discreterandomvariable.html"><a href="1.2-discreterandomvariable.html#sampling-distribution"><i class="fa fa-check"></i><b>1.2.2</b> Sampling distribution</a></li>
<li class="chapter" data-level="1.2.3" data-path="1.2-discreterandomvariable.html"><a href="1.2-discreterandomvariable.html#probdistribution"><i class="fa fa-check"></i><b>1.2.3</b> Probability and probability distribution</a></li>
<li class="chapter" data-level="1.2.4" data-path="1.2-discreterandomvariable.html"><a href="1.2-discreterandomvariable.html#expectedvalue"><i class="fa fa-check"></i><b>1.2.4</b> Expected value or expectation</a></li>
<li class="chapter" data-level="1.2.5" data-path="1.2-discreterandomvariable.html"><a href="1.2-discreterandomvariable.html#unbiased-est"><i class="fa fa-check"></i><b>1.2.5</b> Unbiased estimator</a></li>
<li class="chapter" data-level="1.2.6" data-path="1.2-discreterandomvariable.html"><a href="1.2-discreterandomvariable.html#representative"><i class="fa fa-check"></i><b>1.2.6</b> Representative sample</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="1.3-cont-random-var.html"><a href="1.3-cont-random-var.html"><i class="fa fa-check"></i><b>1.3</b> A Continuous Random Variable: Overweight And Underweight.</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="1.3-cont-random-var.html"><a href="1.3-cont-random-var.html#continuous-variable"><i class="fa fa-check"></i><b>1.3.1</b> Continuous variable</a></li>
<li class="chapter" data-level="1.3.2" data-path="1.3-cont-random-var.html"><a href="1.3-cont-random-var.html#cont_sample_stat"><i class="fa fa-check"></i><b>1.3.2</b> Continuous sample statistic</a></li>
<li class="chapter" data-level="1.3.3" data-path="1.3-cont-random-var.html"><a href="1.3-cont-random-var.html#probability-density"><i class="fa fa-check"></i><b>1.3.3</b> Probability density</a></li>
<li class="chapter" data-level="1.3.4" data-path="1.3-cont-random-var.html"><a href="1.3-cont-random-var.html#probabilities-always-sum-to-1"><i class="fa fa-check"></i><b>1.3.4</b> Probabilities always sum to 1</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="1.4-concluding-remarks.html"><a href="1.4-concluding-remarks.html"><i class="fa fa-check"></i><b>1.4</b> Concluding Remarks</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="1.4-concluding-remarks.html"><a href="1.4-concluding-remarks.html#sample-characteristics-as-observations"><i class="fa fa-check"></i><b>1.4.1</b> Sample characteristics as observations</a></li>
<li class="chapter" data-level="1.4.2" data-path="1.4-concluding-remarks.html"><a href="1.4-concluding-remarks.html#means-at-three-levels"><i class="fa fa-check"></i><b>1.4.2</b> Means at three levels</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="1.5-take-home-points.html"><a href="1.5-take-home-points.html"><i class="fa fa-check"></i><b>1.5</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-probmodels.html"><a href="2-probmodels.html"><i class="fa fa-check"></i><b>2</b> Probability Models: How Do I Get a Sampling Distribution?</a>
<ul>
<li class="chapter" data-level="" data-path="2-probmodels.html"><a href="2-probmodels.html#summary-1"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="2.1" data-path="2.1-exact-approaches-to-the-sampling-distribution.html"><a href="2.1-exact-approaches-to-the-sampling-distribution.html"><i class="fa fa-check"></i><b>2.1</b> Exact Approaches to the Sampling Distribution</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="2.1-exact-approaches-to-the-sampling-distribution.html"><a href="2.1-exact-approaches-to-the-sampling-distribution.html#exact-approaches-for-categorical-data"><i class="fa fa-check"></i><b>2.1.1</b> Exact approaches for categorical data</a></li>
<li class="chapter" data-level="2.1.2" data-path="2.1-exact-approaches-to-the-sampling-distribution.html"><a href="2.1-exact-approaches-to-the-sampling-distribution.html#computer-intensive"><i class="fa fa-check"></i><b>2.1.2</b> Computer-intensive</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="2.2-SPSS-exact.html"><a href="2.2-SPSS-exact.html"><i class="fa fa-check"></i><b>2.2</b> Exact Approaches in SPSS</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="2.2-SPSS-exact.html"><a href="2.2-SPSS-exact.html#instructions"><i class="fa fa-check"></i><b>2.2.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="2.3-theoretical-approx.html"><a href="2.3-theoretical-approx.html"><i class="fa fa-check"></i><b>2.3</b> Theoretical Approximations of the Sampling Distribution</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-theoretical-approx.html"><a href="2.3-theoretical-approx.html#reasons-for-a-bell-shaped-probability-distribution"><i class="fa fa-check"></i><b>2.3.1</b> Reasons for a bell-shaped probability distribution</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-theoretical-approx.html"><a href="2.3-theoretical-approx.html#cond-probdistr"><i class="fa fa-check"></i><b>2.3.2</b> Conditions for the use of theoretical probability distributions</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-theoretical-approx.html"><a href="2.3-theoretical-approx.html#cond-check"><i class="fa fa-check"></i><b>2.3.3</b> Checking conditions</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-theoretical-approx.html"><a href="2.3-theoretical-approx.html#complicatedsampling"><i class="fa fa-check"></i><b>2.3.4</b> More complicated sample statistics: differences</a></li>
<li class="chapter" data-level="2.3.5" data-path="2.3-theoretical-approx.html"><a href="2.3-theoretical-approx.html#independent-samples"><i class="fa fa-check"></i><b>2.3.5</b> Independent samples</a></li>
<li class="chapter" data-level="2.3.6" data-path="2.3-theoretical-approx.html"><a href="2.3-theoretical-approx.html#dependentsamples"><i class="fa fa-check"></i><b>2.3.6</b> Dependent samples</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-spss-and-theoretical-approximation-of-the-sampling-distribution.html"><a href="2.4-spss-and-theoretical-approximation-of-the-sampling-distribution.html"><i class="fa fa-check"></i><b>2.4</b> SPSS and Theoretical Approximation of the Sampling Distribution</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-boot-approx.html"><a href="2.5-boot-approx.html"><i class="fa fa-check"></i><b>2.5</b> The Bootstrap Approximation of the Sampling Distribution</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="2.5-boot-approx.html"><a href="2.5-boot-approx.html#sampling-with-and-without-replacement"><i class="fa fa-check"></i><b>2.5.1</b> Sampling with and without replacement</a></li>
<li class="chapter" data-level="2.5.2" data-path="2.5-boot-approx.html"><a href="2.5-boot-approx.html#limitations-to-bootstrapping"><i class="fa fa-check"></i><b>2.5.2</b> Limitations to bootstrapping</a></li>
<li class="chapter" data-level="2.5.3" data-path="2.5-boot-approx.html"><a href="2.5-boot-approx.html#any-sample-statistic-can-be-bootstrapped"><i class="fa fa-check"></i><b>2.5.3</b> Any sample statistic can be bootstrapped</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="2.6-boot-spss.html"><a href="2.6-boot-spss.html"><i class="fa fa-check"></i><b>2.6</b> Bootstrapping in SPSS</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="2.6-boot-spss.html"><a href="2.6-boot-spss.html#instructions-1"><i class="fa fa-check"></i><b>2.6.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="2.7-when-do-we-use-which-approach-to-the-sampling-distribution.html"><a href="2.7-when-do-we-use-which-approach-to-the-sampling-distribution.html"><i class="fa fa-check"></i><b>2.7</b> When Do We Use Which Approach to the Sampling Distribution?</a></li>
<li class="chapter" data-level="2.8" data-path="2.8-take-home-points-1.html"><a href="2.8-take-home-points-1.html"><i class="fa fa-check"></i><b>2.8</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-param-estim.html"><a href="3-param-estim.html"><i class="fa fa-check"></i><b>3</b> Estimating a Parameter: Which Population Values Are Plausible?</a>
<ul>
<li class="chapter" data-level="" data-path="3-param-estim.html"><a href="3-param-estim.html#summary-2"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="3.1" data-path="3.1-point-estimate.html"><a href="3.1-point-estimate.html"><i class="fa fa-check"></i><b>3.1</b> Point Estimate</a></li>
<li class="chapter" data-level="3.2" data-path="3.2-interval-estimate-for-the-sample-statistic.html"><a href="3.2-interval-estimate-for-the-sample-statistic.html"><i class="fa fa-check"></i><b>3.2</b> Interval Estimate for the Sample Statistic</a></li>
<li class="chapter" data-level="3.3" data-path="3.3-precisionsesamplesize.html"><a href="3.3-precisionsesamplesize.html"><i class="fa fa-check"></i><b>3.3</b> Precision, Standard Error, and Sample Size</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-precisionsesamplesize.html"><a href="3.3-precisionsesamplesize.html#sample-size"><i class="fa fa-check"></i><b>3.3.1</b> Sample size</a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-precisionsesamplesize.html"><a href="3.3-precisionsesamplesize.html#standard-error"><i class="fa fa-check"></i><b>3.3.2</b> Standard error</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-crit-values.html"><a href="3.4-crit-values.html"><i class="fa fa-check"></i><b>3.4</b> Critical Values</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-crit-values.html"><a href="3.4-crit-values.html#standardization-and-z-scores"><i class="fa fa-check"></i><b>3.4.1</b> Standardization and <em>z</em> scores</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-crit-values.html"><a href="3.4-crit-values.html#int-est-sample-mean"><i class="fa fa-check"></i><b>3.4.2</b> Interval estimates from critical values and standard errors</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-ci-parameter.html"><a href="3.5-ci-parameter.html"><i class="fa fa-check"></i><b>3.5</b> Confidence Interval for a Parameter</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-ci-parameter.html"><a href="3.5-ci-parameter.html#fixed-pop-values"><i class="fa fa-check"></i><b>3.5.1</b> Reverse reasoning from one sample mean</a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-ci-parameter.html"><a href="3.5-ci-parameter.html#conf-interval"><i class="fa fa-check"></i><b>3.5.2</b> One confidence interval does not say anything</a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-ci-parameter.html"><a href="3.5-ci-parameter.html#bootstrap-confidenceinterval"><i class="fa fa-check"></i><b>3.5.3</b> Confidence intervals with bootstrapping</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-SPSS-CI.html"><a href="3.6-SPSS-CI.html"><i class="fa fa-check"></i><b>3.6</b> Confidence Intervals in SPSS</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="3.6-SPSS-CI.html"><a href="3.6-SPSS-CI.html#instruction"><i class="fa fa-check"></i><b>3.6.1</b> Instruction</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="3.7-take-home-points-2.html"><a href="3.7-take-home-points-2.html"><i class="fa fa-check"></i><b>3.7</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-hypothesis.html"><a href="4-hypothesis.html"><i class="fa fa-check"></i><b>4</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="" data-path="4-hypothesis.html"><a href="4-hypothesis.html#summary-3"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="4.1" data-path="4.1-binarydecision.html"><a href="4.1-binarydecision.html"><i class="fa fa-check"></i><b>4.1</b> A Binary Decision</a></li>
<li class="chapter" data-level="4.2" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html"><i class="fa fa-check"></i><b>4.2</b> Null Hypothesis Significance Testing</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#null-hypothesis"><i class="fa fa-check"></i><b>4.2.1</b> Null hypothesis</a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#alternative-hypothesis"><i class="fa fa-check"></i><b>4.2.2</b> Alternative hypothesis</a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#true-effect-size"><i class="fa fa-check"></i><b>4.2.3</b> True effect size</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#alpha"><i class="fa fa-check"></i><b>4.2.4</b> Alpha</a></li>
<li class="chapter" data-level="4.2.5" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#alpha-1"><i class="fa fa-check"></i><b>4.2.5</b> 1 - Alpha</a></li>
<li class="chapter" data-level="4.2.6" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#power"><i class="fa fa-check"></i><b>4.2.6</b> Power</a></li>
<li class="chapter" data-level="4.2.7" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#beta"><i class="fa fa-check"></i><b>4.2.7</b> Beta</a></li>
<li class="chapter" data-level="4.2.8" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#test-statistic"><i class="fa fa-check"></i><b>4.2.8</b> Test statistic</a></li>
<li class="chapter" data-level="4.2.9" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#p-value"><i class="fa fa-check"></i><b>4.2.9</b> P-value</a></li>
<li class="chapter" data-level="4.2.10" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#observed-effect-size"><i class="fa fa-check"></i><b>4.2.10</b> Observed effect size</a></li>
<li class="chapter" data-level="4.2.11" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#post-hoc-power"><i class="fa fa-check"></i><b>4.2.11</b> Post hoc power</a></li>
<li class="chapter" data-level="4.2.12" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#meta-analysis"><i class="fa fa-check"></i><b>4.2.12</b> Meta analysis</a></li>
<li class="chapter" data-level="4.2.13" data-path="3.3-precisionsesamplesize.html"><a href="3.3-precisionsesamplesize.html#sample-size"><i class="fa fa-check"></i><b>4.2.13</b> Sample size</a></li>
<li class="chapter" data-level="4.2.14" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#one-twosidedtests"><i class="fa fa-check"></i><b>4.2.14</b> One-Sided and Two-Sided Tests</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-confidence-intervals-to-test-hypotheses.html"><a href="4.3-confidence-intervals-to-test-hypotheses.html"><i class="fa fa-check"></i><b>4.3</b> Confidence Intervals to test hypotheses</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-confidence-intervals-to-test-hypotheses.html"><a href="4.3-confidence-intervals-to-test-hypotheses.html#estimation-instead-of-hypothesis-testing"><i class="fa fa-check"></i><b>4.3.1</b> Estimation instead of hypothesis testing</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-confidence-intervals-to-test-hypotheses.html"><a href="4.3-confidence-intervals-to-test-hypotheses.html#bootstrapped-confidence-intervals"><i class="fa fa-check"></i><b>4.3.2</b> Bootstrapped confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-bayesian-hypothesis-testing.html"><a href="4.4-bayesian-hypothesis-testing.html"><i class="fa fa-check"></i><b>4.4</b> Bayesian hypothesis testing</a></li>
<li class="chapter" data-level="4.5" data-path="4.5-statistical-test-selection.html"><a href="4.5-statistical-test-selection.html"><i class="fa fa-check"></i><b>4.5</b> Statistical test selection</a></li>
<li class="chapter" data-level="4.6" data-path="4.6-reporting-test-results.html"><a href="4.6-reporting-test-results.html"><i class="fa fa-check"></i><b>4.6</b> Reporting test results</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-reporting-test-results.html"><a href="4.6-reporting-test-results.html#reporting-to-fellow-scientists"><i class="fa fa-check"></i><b>4.6.1</b> Reporting to fellow scientists</a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-reporting-test-results.html"><a href="4.6-reporting-test-results.html#reporting-to-the-general-reader"><i class="fa fa-check"></i><b>4.6.2</b> Reporting to the general reader</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-critical-reflection.html"><a href="4.7-critical-reflection.html"><i class="fa fa-check"></i><b>4.7</b> Critical reflection</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-critical-reflection.html"><a href="4.7-critical-reflection.html#criticismsNHST"><i class="fa fa-check"></i><b>4.7.1</b> Criticisms of Null Hypothesis Significance Testing</a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-critical-reflection.html"><a href="4.7-critical-reflection.html#statistical-significance-is-not-a-measure-of-effect-size"><i class="fa fa-check"></i><b>4.7.2</b> Statistical significance is not a measure of effect size</a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-critical-reflection.html"><a href="4.7-critical-reflection.html#cap-chance"><i class="fa fa-check"></i><b>4.7.3</b> Capitalization on Chance</a></li>
<li class="chapter" data-level="4.7.4" data-path="4.7-critical-reflection.html"><a href="4.7-critical-reflection.html#specifying-hypotheses-afterwards"><i class="fa fa-check"></i><b>4.7.4</b> Specifying hypotheses afterwards</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-take-home-points-3.html"><a href="4.8-take-home-points-3.html"><i class="fa fa-check"></i><b>4.8</b> Take home points</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-anova.html"><a href="5-anova.html"><i class="fa fa-check"></i><b>5</b> Moderation with Analysis of Variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="" data-path="5-anova.html"><a href="5-anova.html#summary-4"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="5.1" data-path="5.1-different-means-for-three-or-more-groups.html"><a href="5.1-different-means-for-three-or-more-groups.html"><i class="fa fa-check"></i><b>5.1</b> Different Means for Three or More Groups</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="5.1-different-means-for-three-or-more-groups.html"><a href="5.1-different-means-for-three-or-more-groups.html#anova-meandiffs"><i class="fa fa-check"></i><b>5.1.1</b> Mean differences as effects</a></li>
<li class="chapter" data-level="5.1.2" data-path="5.1-different-means-for-three-or-more-groups.html"><a href="5.1-different-means-for-three-or-more-groups.html#between-variance"><i class="fa fa-check"></i><b>5.1.2</b> Between-groups variance and within-groups variance</a></li>
<li class="chapter" data-level="5.1.3" data-path="5.1-different-means-for-three-or-more-groups.html"><a href="5.1-different-means-for-three-or-more-groups.html#anova-model"><i class="fa fa-check"></i><b>5.1.3</b> <em>F</em> test on the model</a></li>
<li class="chapter" data-level="5.1.4" data-path="5.1-different-means-for-three-or-more-groups.html"><a href="5.1-different-means-for-three-or-more-groups.html#anova-assumpt"><i class="fa fa-check"></i><b>5.1.4</b> Assumptions for the <em>F</em> test in analysis of variance</a></li>
<li class="chapter" data-level="5.1.5" data-path="5.1-different-means-for-three-or-more-groups.html"><a href="5.1-different-means-for-three-or-more-groups.html#which-groups-have-different-average-scores"><i class="fa fa-check"></i><b>5.1.5</b> Which groups have different average scores?</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="5.2-onewaySPSS.html"><a href="5.2-onewaySPSS.html"><i class="fa fa-check"></i><b>5.2</b> One-Way Analysis of Variance in SPSS</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="5.2-onewaySPSS.html"><a href="5.2-onewaySPSS.html#instructions-2"><i class="fa fa-check"></i><b>5.2.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="5.3-different-means-for-two-factors.html"><a href="5.3-different-means-for-two-factors.html"><i class="fa fa-check"></i><b>5.3</b> Different Means for Two Factors</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="5.3-different-means-for-two-factors.html"><a href="5.3-different-means-for-two-factors.html#anova2way"><i class="fa fa-check"></i><b>5.3.1</b> Two-way analysis of variance</a></li>
<li class="chapter" data-level="5.3.2" data-path="5.3-different-means-for-two-factors.html"><a href="5.3-different-means-for-two-factors.html#balanced"><i class="fa fa-check"></i><b>5.3.2</b> Balanced design</a></li>
<li class="chapter" data-level="5.3.3" data-path="5.3-different-means-for-two-factors.html"><a href="5.3-different-means-for-two-factors.html#maineffects"><i class="fa fa-check"></i><b>5.3.3</b> Main effects in two-way analysis of variance</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="5.4-moderationanova.html"><a href="5.4-moderationanova.html"><i class="fa fa-check"></i><b>5.4</b> Moderation: Group-Level Differences that Depend on Context</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="5.4-moderationanova.html"><a href="5.4-moderationanova.html#types-of-moderation"><i class="fa fa-check"></i><b>5.4.1</b> Types of moderation</a></li>
<li class="chapter" data-level="5.4.2" data-path="5.4-moderationanova.html"><a href="5.4-moderationanova.html#testing-main-and-interaction-effects"><i class="fa fa-check"></i><b>5.4.2</b> Testing main and interaction effects</a></li>
<li class="chapter" data-level="5.4.3" data-path="5.4-moderationanova.html"><a href="5.4-moderationanova.html#assumptions-for-two-way-analysis-of-variance"><i class="fa fa-check"></i><b>5.4.3</b> Assumptions for two-way analysis of variance</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="5.5-reporting-two-way-analysis-of-variance.html"><a href="5.5-reporting-two-way-analysis-of-variance.html"><i class="fa fa-check"></i><b>5.5</b> Reporting Two-Way Analysis of Variance</a></li>
<li class="chapter" data-level="5.6" data-path="5.6-twowaySPSS.html"><a href="5.6-twowaySPSS.html"><i class="fa fa-check"></i><b>5.6</b> Two-Way Analysis of Variance in SPSS</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="5.6-twowaySPSS.html"><a href="5.6-twowaySPSS.html#instructions-3"><i class="fa fa-check"></i><b>5.6.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="5.7-take-home-points-4.html"><a href="5.7-take-home-points-4.html"><i class="fa fa-check"></i><b>5.7</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-moderationcat.html"><a href="6-moderationcat.html"><i class="fa fa-check"></i><b>6</b> Regression Analysis And A Categorical Moderator</a>
<ul>
<li class="chapter" data-level="" data-path="6-moderationcat.html"><a href="6-moderationcat.html#summary-5"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="6.1" data-path="6.1-regression-equation.html"><a href="6.1-regression-equation.html"><i class="fa fa-check"></i><b>6.1</b> The Regression Equation</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-regression-equation.html"><a href="6.1-regression-equation.html#a-numerical-predictor"><i class="fa fa-check"></i><b>6.1.1</b> A numerical predictor</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-regression-equation.html"><a href="6.1-regression-equation.html#dichpredictor"><i class="fa fa-check"></i><b>6.1.2</b> Dichotomous predictors</a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-regression-equation.html"><a href="6.1-regression-equation.html#categorical-predictor"><i class="fa fa-check"></i><b>6.1.3</b> A categorical independent variable and dummy variables</a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-regression-equation.html"><a href="6.1-regression-equation.html#regr-inference"><i class="fa fa-check"></i><b>6.1.4</b> Sampling distributions and assumptions</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-SPSS-regression.html"><a href="6.2-SPSS-regression.html"><i class="fa fa-check"></i><b>6.2</b> Regression Analysis in SPSS</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="6.2-SPSS-regression.html"><a href="6.2-SPSS-regression.html#instructions-4"><i class="fa fa-check"></i><b>6.2.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html"><i class="fa fa-check"></i><b>6.3</b> Different Lines for Different Groups</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html#a-dichotomous-moderator-and-numerical-predictor"><i class="fa fa-check"></i><b>6.3.1</b> A dichotomous moderator and numerical predictor</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html#interaction-variable"><i class="fa fa-check"></i><b>6.3.2</b> Interaction variable</a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html#conditional-effects"><i class="fa fa-check"></i><b>6.3.3</b> Conditional effects, not main effects</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html#interactioninterpretation"><i class="fa fa-check"></i><b>6.3.4</b> Interpretation and statistical inference</a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html#a-categorical-moderator"><i class="fa fa-check"></i><b>6.3.5</b> A categorical moderator</a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html#commonsupportdichotomous"><i class="fa fa-check"></i><b>6.3.6</b> Common support</a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-categoricalmoderator.html"><a href="6.3-categoricalmoderator.html#visualizing-moderation-and-covariates"><i class="fa fa-check"></i><b>6.3.7</b> Visualizing moderation and covariates</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-catmodSPSS.html"><a href="6.4-catmodSPSS.html"><i class="fa fa-check"></i><b>6.4</b> A Dichotomous or Categorical Moderator in SPSS</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-catmodSPSS.html"><a href="6.4-catmodSPSS.html#instructions-5"><i class="fa fa-check"></i><b>6.4.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-take-home-points-5.html"><a href="6.5-take-home-points-5.html"><i class="fa fa-check"></i><b>6.5</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-moderationcont.html"><a href="7-moderationcont.html"><i class="fa fa-check"></i><b>7</b> Regression Analysis With A Numerical Moderator</a>
<ul>
<li class="chapter" data-level="" data-path="7-moderationcont.html"><a href="7-moderationcont.html#summary-6"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="7.1" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html"><i class="fa fa-check"></i><b>7.1</b> A Numerical Moderator</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#interpret-cont-interaction"><i class="fa fa-check"></i><b>7.1.1</b> Interaction variable</a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#conditional-effect-cont"><i class="fa fa-check"></i><b>7.1.2</b> Conditional effect</a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#mean-centering"><i class="fa fa-check"></i><b>7.1.3</b> Mean-centering</a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#symmetry-of-predictor-and-moderator"><i class="fa fa-check"></i><b>7.1.4</b> Symmetry of predictor and moderator</a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#visualization-of-the-interaction-effect"><i class="fa fa-check"></i><b>7.1.5</b> Visualization of the interaction effect</a></li>
<li class="chapter" data-level="7.1.6" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#statistical-inference-on-conditional-effects"><i class="fa fa-check"></i><b>7.1.6</b> Statistical inference on conditional effects</a></li>
<li class="chapter" data-level="7.1.7" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#common-support"><i class="fa fa-check"></i><b>7.1.7</b> Common support</a></li>
<li class="chapter" data-level="7.1.8" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#assumptions"><i class="fa fa-check"></i><b>7.1.8</b> Assumptions</a></li>
<li class="chapter" data-level="7.1.9" data-path="7.1-cont-moderator-regression.html"><a href="7.1-cont-moderator-regression.html#higher-order-interaction-effects"><i class="fa fa-check"></i><b>7.1.9</b> Higher-order interaction effects</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-reportmoderation.html"><a href="7.2-reportmoderation.html"><i class="fa fa-check"></i><b>7.2</b> Reporting Regression Results</a></li>
<li class="chapter" data-level="7.3" data-path="7.3-RegressionContModSPSS.html"><a href="7.3-RegressionContModSPSS.html"><i class="fa fa-check"></i><b>7.3</b> A Numerical Moderator in SPSS</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-RegressionContModSPSS.html"><a href="7.3-RegressionContModSPSS.html#instructions-6"><i class="fa fa-check"></i><b>7.3.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-take-home-points-6.html"><a href="7.4-take-home-points-6.html"><i class="fa fa-check"></i><b>7.4</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-confounder.html"><a href="8-confounder.html"><i class="fa fa-check"></i><b>8</b> Regression Analysis And Confounders</a>
<ul>
<li class="chapter" data-level="" data-path="8-confounder.html"><a href="8-confounder.html#summary-7"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="8.1" data-path="8.1-controlling.html"><a href="8.1-controlling.html"><i class="fa fa-check"></i><b>8.1</b> Controlling for Effects of Other Predictors</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-controlling.html"><a href="8.1-controlling.html#partialeffect"><i class="fa fa-check"></i><b>8.1.1</b> Partial effect</a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-controlling.html"><a href="8.1-controlling.html#confounding-variables"><i class="fa fa-check"></i><b>8.1.2</b> Confounding variables</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-indirectcorrelation.html"><a href="8.2-indirectcorrelation.html"><i class="fa fa-check"></i><b>8.2</b> Indirect Correlation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-indirectcorrelation.html"><a href="8.2-indirectcorrelation.html#indirect-correlation-and-size-of-confounding"><i class="fa fa-check"></i><b>8.2.1</b> Indirect correlation and size of confounding</a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-indirectcorrelation.html"><a href="8.2-indirectcorrelation.html#confounders-are-not-included-in-the-regression-model"><i class="fa fa-check"></i><b>8.2.2</b> Confounders are not included in the regression model</a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-indirectcorrelation.html"><a href="8.2-indirectcorrelation.html#randomization"><i class="fa fa-check"></i><b>8.2.3</b> Randomization for avoiding confounders</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-confounders.html"><a href="8.3-confounders.html"><i class="fa fa-check"></i><b>8.3</b> Two Types of Confounders</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-confounders.html"><a href="8.3-confounders.html#suppression"><i class="fa fa-check"></i><b>8.3.1</b> Suppression</a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-confounders.html"><a href="8.3-confounders.html#spuriousness"><i class="fa fa-check"></i><b>8.3.2</b> Reinforcement and spuriousness</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-compmodelSSPSS.html"><a href="8.4-compmodelSSPSS.html"><i class="fa fa-check"></i><b>8.4</b> Comparing Regression Models in SPSS</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="8.4-compmodelSSPSS.html"><a href="8.4-compmodelSSPSS.html#instructions-7"><i class="fa fa-check"></i><b>8.4.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="8.5-take-home-points-7.html"><a href="8.5-take-home-points-7.html"><i class="fa fa-check"></i><b>8.5</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-mediation.html"><a href="9-mediation.html"><i class="fa fa-check"></i><b>9</b> Mediation with Regression Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="9-mediation.html"><a href="9-mediation.html#summary-8"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="9.1" data-path="9.1-mediation-as-causal-process.html"><a href="9.1-mediation-as-causal-process.html"><i class="fa fa-check"></i><b>9.1</b> Mediation as Causal Process</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-mediation-as-causal-process.html"><a href="9.1-mediation-as-causal-process.html#causalcriteria"><i class="fa fa-check"></i><b>9.1.1</b> Criteria for a causal relation</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-mediation-as-causal-process.html"><a href="9.1-mediation-as-causal-process.html#indirecteffect"><i class="fa fa-check"></i><b>9.1.2</b> Mediation as indirect effect</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-mediation-as-causal-process.html"><a href="9.1-mediation-as-causal-process.html#causal-process"><i class="fa fa-check"></i><b>9.1.3</b> Causal process</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-path-model-with-regression-analysis.html"><a href="9.2-path-model-with-regression-analysis.html"><i class="fa fa-check"></i><b>9.2</b> Path Model with Regression Analysis</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-path-model-with-regression-analysis.html"><a href="9.2-path-model-with-regression-analysis.html#requirements"><i class="fa fa-check"></i><b>9.2.1</b> Requirements</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-path-model-with-regression-analysis.html"><a href="9.2-path-model-with-regression-analysis.html#size-indirect-effects"><i class="fa fa-check"></i><b>9.2.2</b> Size of indirect effects</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-path-model-with-regression-analysis.html"><a href="9.2-path-model-with-regression-analysis.html#direction-of-indirect-effects"><i class="fa fa-check"></i><b>9.2.3</b> Direction of indirect effects</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-path-model-with-regression-analysis.html"><a href="9.2-path-model-with-regression-analysis.html#partialserialmediation"><i class="fa fa-check"></i><b>9.2.4</b> Parallel and serial mediation</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-path-model-with-regression-analysis.html"><a href="9.2-path-model-with-regression-analysis.html#partialfullmediation"><i class="fa fa-check"></i><b>9.2.5</b> Partial and full mediation</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-path-model-with-regression-analysis.html"><a href="9.2-path-model-with-regression-analysis.html#significance-of-indirect-effects"><i class="fa fa-check"></i><b>9.2.6</b> Significance of indirect effects</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-mediationcovariate.html"><a href="9.3-mediationcovariate.html"><i class="fa fa-check"></i><b>9.3</b> Controlling for Covariates</a></li>
<li class="chapter" data-level="9.4" data-path="9.4-reporting-mediation-results.html"><a href="9.4-reporting-mediation-results.html"><i class="fa fa-check"></i><b>9.4</b> Reporting Mediation Results</a></li>
<li class="chapter" data-level="9.5" data-path="9.5-SPSSPROCESS.html"><a href="9.5-SPSSPROCESS.html"><i class="fa fa-check"></i><b>9.5</b> Mediation with SPSS and PROCESS</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-SPSSPROCESS.html"><a href="9.5-SPSSPROCESS.html#instructions-8"><i class="fa fa-check"></i><b>9.5.1</b> Instructions</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-criticisms-of-mediation.html"><a href="9.6-criticisms-of-mediation.html"><i class="fa fa-check"></i><b>9.6</b> Criticisms of Mediation</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-criticisms-of-mediation.html"><a href="9.6-criticisms-of-mediation.html#causal-order-assumed"><i class="fa fa-check"></i><b>9.6.1</b> Causal order assumed</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-criticisms-of-mediation.html"><a href="9.6-criticisms-of-mediation.html#time-order"><i class="fa fa-check"></i><b>9.6.2</b> Time order</a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-criticisms-of-mediation.html"><a href="9.6-criticisms-of-mediation.html#causality-or-underlying-construct"><i class="fa fa-check"></i><b>9.6.3</b> Causality or underlying construct?</a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-criticisms-of-mediation.html"><a href="9.6-criticisms-of-mediation.html#every-effect-in-a-path-model-can-be-confounded"><i class="fa fa-check"></i><b>9.6.4</b> Every effect in a path model can be confounded</a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-criticisms-of-mediation.html"><a href="9.6-criticisms-of-mediation.html#recommendations"><i class="fa fa-check"></i><b>9.6.5</b> Recommendations</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-combining-mediation-and-moderation.html"><a href="9.7-combining-mediation-and-moderation.html"><i class="fa fa-check"></i><b>9.7</b> Combining Mediation and Moderation</a></li>
<li class="chapter" data-level="9.8" data-path="9.8-take-home-points-8.html"><a href="9.8-take-home-points-8.html"><i class="fa fa-check"></i><b>9.8</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a>
<ul>
<li class="chapter" data-level="" data-path="flow-chart-statistical-test-selection.html"><a href="flow-chart-statistical-test-selection.html"><i class="fa fa-check"></i>Flow chart statistical test selection</a></li>
<li class="chapter" data-level="" data-path="all-spss-tutorial-videos-list.html"><a href="all-spss-tutorial-videos-list.html"><i class="fa fa-check"></i>All SPSS Tutorial Videos List</a></li>
<li class="chapter" data-level="" data-path="formulating-statistical-hypotheses.html"><a href="formulating-statistical-hypotheses.html"><i class="fa fa-check"></i>Formulating Statistical Hypotheses</a></li>
<li class="chapter" data-level="" data-path="proportions-shares.html"><a href="proportions-shares.html"><i class="fa fa-check"></i>Proportions: shares</a>
<ul>
<li class="chapter" data-level="" data-path="proportions-shares.html"><a href="proportions-shares.html#testing-proportions-in-spss"><i class="fa fa-check"></i>Testing proportions in SPSS</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mean-and-median-level.html"><a href="mean-and-median-level.html"><i class="fa fa-check"></i>Mean and median: level</a>
<ul>
<li class="chapter" data-level="" data-path="mean-and-median-level.html"><a href="mean-and-median-level.html#testing-one-mean-or-median-in-spss"><i class="fa fa-check"></i>Testing one mean or median in SPSS</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="variance-disagreement.html"><a href="variance-disagreement.html"><i class="fa fa-check"></i>Variance: (dis)agreement</a>
<ul>
<li class="chapter" data-level="" data-path="variance-disagreement.html"><a href="variance-disagreement.html#testing-two-variances-in-spss"><i class="fa fa-check"></i>Testing two variances in SPSS</a></li>
<li class="chapter" data-level="" data-path="variance-disagreement.html"><a href="variance-disagreement.html#answers"><i class="fa fa-check"></i>Answers</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="association-relations-between-characteristics.html"><a href="association-relations-between-characteristics.html"><i class="fa fa-check"></i>Association: relations between characteristics</a>
<ul>
<li class="chapter" data-level="" data-path="association-relations-between-characteristics.html"><a href="association-relations-between-characteristics.html#score-level-differences"><i class="fa fa-check"></i>Score level differences</a></li>
<li class="chapter" data-level="" data-path="association-relations-between-characteristics.html"><a href="association-relations-between-characteristics.html#comparing-means-in-spss"><i class="fa fa-check"></i>Comparing means in SPSS</a></li>
<li class="chapter" data-level="" data-path="association-relations-between-characteristics.html"><a href="association-relations-between-characteristics.html#answers-1"><i class="fa fa-check"></i>Answers</a></li>
<li class="chapter" data-level="" data-path="association-relations-between-characteristics.html"><a href="association-relations-between-characteristics.html#combinations-of-scores"><i class="fa fa-check"></i>Combinations of scores</a></li>
<li class="chapter" data-level="" data-path="association-relations-between-characteristics.html"><a href="association-relations-between-characteristics.html#testing-associations-in-spss"><i class="fa fa-check"></i>Testing associations in SPSS</a></li>
<li class="chapter" data-level="" data-path="association-relations-between-characteristics.html"><a href="association-relations-between-characteristics.html#answers-2"><i class="fa fa-check"></i>Answers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="chapter" data-level="10" data-path="10-chapter-4-leftovers.html"><a href="10-chapter-4-leftovers.html"><i class="fa fa-check"></i><b>10</b> Chapter 4 leftovers</a>
<ul>
<li class="chapter" data-level="10.1" data-path="10.1-testing-a-null-hypothesis-with-a-theoretical-probability-distribution.html"><a href="10.1-testing-a-null-hypothesis-with-a-theoretical-probability-distribution.html"><i class="fa fa-check"></i><b>10.1</b> Testing a Null Hypothesis with a Theoretical Probability Distribution</a></li>
<li class="chapter" data-level="10.2" data-path="10.2-nullSPSS.html"><a href="10.2-nullSPSS.html"><i class="fa fa-check"></i><b>10.2</b> Specifying Null Hypotheses in SPSS</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-nullSPSS.html"><a href="10.2-nullSPSS.html#specify-null-for-binomial-test"><i class="fa fa-check"></i><b>10.2.1</b> Specify null for binomial test</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-take-home-points-9.html"><a href="10.3-take-home-points-9.html"><i class="fa fa-check"></i><b>10.3</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="4.2-null-hypothesis-significance-testing.html"><a href="4.2-null-hypothesis-significance-testing.html#power"><i class="fa fa-check"></i><b>11</b> Which Sample Size Do I Need? Power!</a>
<ul>
<li class="chapter" data-level="" data-path="11-power.html"><a href="11-power.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="11.1" data-path="11.1-effectsize.html"><a href="11.1-effectsize.html"><i class="fa fa-check"></i><b>11.1</b> Effect Size</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-effectsize.html"><a href="11.1-effectsize.html#practical-relevance"><i class="fa fa-check"></i><b>11.1.1</b> Practical relevance</a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-effectsize.html"><a href="11.1-effectsize.html#unstandardized-effect-size"><i class="fa fa-check"></i><b>11.1.2</b> Unstandardized effect size</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-hypothetical-world-versus-imaginary-true-world.html"><a href="11.2-hypothetical-world-versus-imaginary-true-world.html"><i class="fa fa-check"></i><b>11.2</b> Hypothetical World Versus Imaginary True World</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="11.2-hypothetical-world-versus-imaginary-true-world.html"><a href="11.2-hypothetical-world-versus-imaginary-true-world.html#imagining-a-population-with-a-small-effect"><i class="fa fa-check"></i><b>11.2.1</b> Imagining a population with a small effect</a></li>
<li class="chapter" data-level="11.2.2" data-path="11.2-hypothetical-world-versus-imaginary-true-world.html"><a href="11.2-hypothetical-world-versus-imaginary-true-world.html#the-world-of-the-researcher"><i class="fa fa-check"></i><b>11.2.2</b> The world of the researcher</a></li>
<li class="chapter" data-level="11.2.3" data-path="11.2-hypothetical-world-versus-imaginary-true-world.html"><a href="11.2-hypothetical-world-versus-imaginary-true-world.html#the-alternative-world-of-a-small-effect"><i class="fa fa-check"></i><b>11.2.3</b> The alternative world of a small effect</a></li>
<li class="chapter" data-level="11.2.4" data-path="11.2-hypothetical-world-versus-imaginary-true-world.html"><a href="11.2-hypothetical-world-versus-imaginary-true-world.html#typeIIerror"><i class="fa fa-check"></i><b>11.2.4</b> Type II error</a></li>
<li class="chapter" data-level="11.2.5" data-path="11.2-hypothetical-world-versus-imaginary-true-world.html"><a href="11.2-hypothetical-world-versus-imaginary-true-world.html#power-of-the-test"><i class="fa fa-check"></i><b>11.2.5</b> Power of the test</a></li>
<li class="chapter" data-level="11.2.6" data-path="11.2-hypothetical-world-versus-imaginary-true-world.html"><a href="11.2-hypothetical-world-versus-imaginary-true-world.html#post-hoc-power-1"><i class="fa fa-check"></i><b>11.2.6</b> Post hoc power</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="11.3-sizeeffectpower.html"><a href="11.3-sizeeffectpower.html"><i class="fa fa-check"></i><b>11.3</b> Sample Size, Effect Size, and Power</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-sizeeffectpower.html"><a href="11.3-sizeeffectpower.html#so-how-do-we-determine-sample-size"><i class="fa fa-check"></i><b>11.3.1</b> So how do we determine sample size?</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-research-hypothesis-as-null-hypothesis.html"><a href="11.4-research-hypothesis-as-null-hypothesis.html"><i class="fa fa-check"></i><b>11.4</b> Research Hypothesis as Null Hypothesis</a></li>
<li class="chapter" data-level="11.5" data-path="11.5-take-home-points-10.html"><a href="11.5-take-home-points-10.html"><i class="fa fa-check"></i><b>11.5</b> Take-Home Points</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-crit-discus.html"><a href="12-crit-discus.html"><i class="fa fa-check"></i><b>12</b> Critical Discussion of Null Hypothesis Significance Testing</a>
<ul>
<li class="chapter" data-level="" data-path="12-crit-discus.html"><a href="12-crit-discus.html#summary-10"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="12.0.1" data-path="12-crit-discus.html"><a href="12-crit-discus.html#strawmen"><i class="fa fa-check"></i><b>12.0.1</b> Knocking down straw men (over and over again)</a></li>
<li class="chapter" data-level="12.1" data-path="12.1-alternatives-for-null-hypothesis-significance-testing.html"><a href="12.1-alternatives-for-null-hypothesis-significance-testing.html"><i class="fa fa-check"></i><b>12.1</b> Alternatives for Null Hypothesis Significance Testing</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="12.1-alternatives-for-null-hypothesis-significance-testing.html"><a href="12.1-alternatives-for-null-hypothesis-significance-testing.html#replication"><i class="fa fa-check"></i><b>12.1.1</b> Replication</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="12.2-no-random-sample.html"><a href="12.2-no-random-sample.html"><i class="fa fa-check"></i><b>12.2</b> What If I Do Not Have a Random Sample?</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="12.2-no-random-sample.html"><a href="12.2-no-random-sample.html#theoretical-population"><i class="fa fa-check"></i><b>12.2.1</b> Theoretical population</a></li>
<li class="chapter" data-level="12.2.2" data-path="12.2-no-random-sample.html"><a href="12.2-no-random-sample.html#datageneratingprocess"><i class="fa fa-check"></i><b>12.2.2</b> Data generating process</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="12.3-take-home-points-11.html"><a href="12.3-take-home-points-11.html"><i class="fa fa-check"></i><b>12.3</b> Take-Home Points</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statitstical Inference</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="null-hypothesis-significance-testing" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Null Hypothesis Significance Testing<a href="4.2-null-hypothesis-significance-testing.html#null-hypothesis-significance-testing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Null Hypothesis Significance Testing (NHST) is the most widely used method for statistical inference in the social sciences and beyond. The logic underlying NHST is called the Neyman Pearson approach <span class="citation">(<a href="12.3-take-home-points-11.html#ref-RefWorks:3930">Lehmann, 1993</a>)</span>. Though these names are not widely known, the work of Jerzy Neyman (1894â€“1981) and Egon Pearson (1895â€“1980) still has a profound impact on the way current research is conducted, reviews are considered, and papers are published.</p>

<div class="rmdpearson">
<p>Our habit of formulating a null hypothesis and an alternative hypothesis for all situations not covered by the null hypothesis is generally attributed to the statistician R.A. Fisher. This, however, is not entirely correct <span class="citation">(see, e.g., <a href="12.3-take-home-points-11.html#ref-RefWorks:3931">Halpin &amp; Stam, 2006</a>)</span>. Fisher introduced the concept of a null hypothesis <span class="citation">(<a href="12.3-take-home-points-11.html#ref-RefWorks:3932">Ronald Aylmer Fisher, 1935</a>: 18)</span> but not the concept of an alternative hypothesis.
The statisticians Jerzy Neyman and Egon Pearson introduced the idea of working with two or more hypotheses. But the two hypotheses do not cover all possible population values and they were usually not called a null and alternative hypothesis. They specify two or more different population values. A statistical test is used to determine which of the hypotheses fits the sample best. <span class="citation">(<a href="12.3-take-home-points-11.html#ref-RefWorks:3906">J. Neyman &amp; Pearson, 1933</a>)</span></p>
Egon Pearson. Photo by Grasso Luigi, Wikimedia Commons, CC BY-SA 4.0
</div>
<p>The Neyman Pearson approach ensures tight control on the probability of making correct and incorrect decisions. It is a decision framework that gives you a clear criteria and also an indication of what the probability is that your decision is wrong. The decision in this regard, is either the acceptance or rejection of the <span class="math inline">\(H_0\)</span> hypothesis.</p>
<p>The Neyman Pearson approach is about choosing your desired probability of making correct and incorrect decisions, setting up the right conditions for this, and making a decision. It considers the following:</p>
<ol style="list-style-type: decimal">
<li>Alpha - Determine your desired risk of drawing the wrong conclusion.</li>
<li>Power - Determine your desired probability of drawing the correct conclusion.</li>
<li>The true effect size</li>
<li>The sample size needed to achieve desired power.</li>
<li>Conduct your research with this sample size.</li>
<li>Determine the test statistic.</li>
<li>Determine if <span class="math inline">\(p\)</span>-value <span class="math inline">\(\leq \alpha\)</span>. If so, reject <span class="math inline">\(H_0\)</span>.</li>
</ol>
<p>The two decisions can be visualized in a <span class="math inline">\(2 \times 2\)</span> table where in reality <span class="math inline">\(H_0\)</span> can be true or false (<span class="math inline">\(H_A\)</span> is true), and the decision can either be to reject <span class="math inline">\(H_0\)</span> or not. Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:decisiontable">4.1</a> illustrates the correct and incorrect decisions that can be made. The green squares obviously indicate that it is a good decision to reject <span class="math inline">\(H_0\)</span> when it is infact false, and not to reject <span class="math inline">\(H_0\)</span> if it is in reality true. And the red squares indicate that it is a wrong decision to reject <span class="math inline">\(H_0\)</span> when it is actually true (Type I error), or not reject <span class="math inline">\(H_0\)</span> if it is in reality false (Type II error).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decisiontable"></span>
<img src="figures/decision_table.svg" alt="NHST decision table." width="300px" />
<p class="caption">
Figure 4.1: NHST decision table.
</p>
</div>
<p>Intuitively it is easy to understand that you would want the probability of an incorrect decision to be low, and the probability of a correct decision to be high. But how do we actually set these probabilities? Lets consider the amount of yellow candies from the candy factory again. In chapter <a href="1.2-discreterandomvariable.html#discreterandomvariable">1.2</a> we learned that the factory produces candy bags where one fifth of the candies are supposed to be yellow. Now suppose we donâ€™t know this and our null hypothesis would be that half or the candies would be yellow. In figure <a href="1.2-discreterandomvariable.html#fig:expected-value">1.4</a> you can set the parameter values to .5 and .2 and see what the discrete probability distributions look like.</p>
<p>As the candy factory produces bags with ten candies, we can look at both probability distributions. Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:twobinom">4.2</a> shows both distributions.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><span class="math inline">\(H_0\)</span> Distribution
<ul>
<li>Half of the candies in the bag are yellow</li>
<li>The parameter of the candy machine is .5</li>
<li>With expected value 5 out of 10</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><span class="math inline">\(H_A\)</span> Distribution
<ul>
<li>One fifth of the candies in the bag are yellow</li>
<li>The parameter of the candy machine is .2</li>
<li>With expected value 2 out of 10</li>
</ul></li>
</ul>
</div>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twobinom"></span>
<img src="GentleIntro_files/figure-html/twobinom-1.png" alt="Discrete binomial distributions" width="420px" />
<p class="caption">
Figure 4.2: Discrete binomial distributions
</p>
</div>
<p>We will use both distributions in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:twobinom">4.2</a> to clarify the different components within the Neyman Pearson approach later in this chapter. For now, take a good look at both probability distributions, and consider a bag of candy containing 4 yellow candies. Are you able to determine if this bag is the result of a manufacturing process that produces bags with 20% or 50% yellow candies.</p>
<p>Doing research is essentially the same. You collect one sample, and have to determine if the effect of your study is non existent (<span class="math inline">\(H_0 = \text{true}\)</span>) or that there is something going on (<span class="math inline">\(H_0 \neq \text{true}\)</span>).</p>
<p>Statistical hypotheses come in pairs: a null hypothesis (<em>H</em><sub>0</sub>) and an <em>alternative hypothesis</em> (<em>H</em><sub>1</sub> / <em>H</em><sub>A</sub>). We met the null hypothesis in the preceding sections. We use it to create a (hypothetical) sampling distribution. To this end, a null hypothesis must specify one value for the population statistic that we are interested in, for example, .5 or .2 as the proportion of yellow candies.</p>
<div id="null-hypothesis" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Null hypothesis<a href="4.2-null-hypothesis-significance-testing.html#null-hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The null hypothesis reflects the skeptical stance in research. It assumes that there is nothing going on. There is no difference between experimental condition, there is no correlation between variables, there is no predictive value to your regression model, a coin is fair, and so forth. Though a null hypothesis can be expressed as a single value, that does not mean that we always get that specific value when we take a random sample.</p>
<p>If our null assumption of our candy factory machine is that it produces bags with 5 out of 10 yellow candies, then there is still a chance that some bags will contain just one yellow candy of even 0 yellow candies. As can be seen in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nulldistribution">4.3</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nulldistribution"></span>
<img src="GentleIntro_files/figure-html/nulldistribution-1.png" alt="Discrete binomial distributions" width="420px" />
<p class="caption">
Figure 4.3: Discrete binomial distributions
</p>
</div>
</div>
<div id="alternative-hypothesis" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Alternative hypothesis<a href="4.2-null-hypothesis-significance-testing.html#alternative-hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The assumption that a researcher wants to test is called a <em>research hypothesis</em>. It is a statement about the empirical world that can be tested against data. Communication scientists, for instance, may hypothesize that:</p>
<ul>
<li>a television station reaches half of all households in a country,</li>
<li>media literacy is below a particular standard (for instance, 5.5 on a 10-point scale) among children,</li>
<li>opinions about immigrants are not equally polarized among young and old voters,<br />
</li>
<li>the celebrity endorsing a fundraising campaign makes a difference to adultâ€™s willingness to donate,<br />
</li>
<li>more exposure to brand advertisements increases brand awareness among consumers,</li>
<li>and so on.</li>
</ul>
<p>These are statements about populations: all households in a country, children, voters, adults, and consumers. As these examples illustrate, research hypotheses seldom refer to statistics such as means, proportions, variances, or correlations. Still, we need a statistic to test a hypothesis. The researcher must translate the research hypothesis into a new hypothesis that refers to a statistic in the population, for example, the population mean. <strong>The alternative hypothesis</strong> therefore indicates what the researcher expects in terms of effects, differences, deviation from null. It is the operationalisation of what you expect to find if your theory would be accurate.</p>
<p>In case of our candy factory example, the alternative hypothesis would be that the machine produces bags with 2 out of 10 yellow candies and that the machines parameter is .2, one in five yellow candies per bag. Assuming this parameter, does not ensure that every bag will contain exactly 2 yellow candies. Some bags will contain 0, 1, 3, 4, 5, 6, 7, 8, 9, or even 10 yellow candies. The probabilities for each can again be visualized using the exact discrete binomial probability distribution (figure <a href="4.2-null-hypothesis-significance-testing.html#fig:altdistribution">4.4</a>) as we did for the null hypothesis.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:altdistribution"></span>
<img src="GentleIntro_files/figure-html/altdistribution-1.png" alt="Discrete binomial distributions" width="420px" />
<p class="caption">
Figure 4.4: Discrete binomial distributions
</p>
</div>
<p>Note that both the probability distribution for <span class="math inline">\(H_0\)</span> and for <span class="math inline">\(H_A\)</span> contain the results of assumptions about reality, and that at this stage only the sample size, the amount of candies in a bag (10) has been used to determine the distribution. No data has been gathered yet in determining these distributions. It is all based on assumptions.</p>
</div>
<div id="true-effect-size" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> True effect size<a href="4.2-null-hypothesis-significance-testing.html#true-effect-size" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The true effect size is the difference between the null hypothesis and the true alternative hypothesis. In the candy factory example, the true effect size is .5 - .2 = .3. This is the difference in the proportion of yellow candies in the bags. In figure <a href="4.2-null-hypothesis-significance-testing.html#fig:twobinomeffect">4.5</a> you can see the difference in the two distributions. The true effect size is the difference in the expected value of the two distributions. In absolute terms, it is the 5 - 3, though, in terms of the parameter it is the proportion .5 - .2.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twobinomeffect"></span>
<img src="GentleIntro_files/figure-html/twobinomeffect-1.png" alt="Discrete binomial distributions" width="420px" />
<p class="caption">
Figure 4.5: Discrete binomial distributions
</p>
</div>
<p><strong>True</strong> refers to the actual difference in the population. The problem is that we do not know this difference. In our candy factory example, we can only observe the specific candy bag. Have an assumption about the null hypothesis and the alternative hypothesis. Though the <strong>true</strong> effect size refers only to the actual effect in the population, the actual difference, the actual correlation, the actual parameter value.</p>
<p>Understanding the null and alternative hypothesis and their associated probability distributions is crucial for grasping the logic of the Neyman Pearson approach. In the following chapters, we will use these distributions to explain the different components of the Neyman Pearson approach.</p>
</div>
<div id="alpha" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Alpha<a href="4.2-null-hypothesis-significance-testing.html#alpha" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Alpha = <span class="math inline">\(\alpha\)</span> = Significance level = significance = rejection region = Type I error</p>
</blockquote>
<p>The first step in the Neyman Pearson approach is to set the desired Type I error rate, also known as the significance level, <span class="math inline">\(\alpha\)</span>. This is the probability of rejecting the null hypothesis when it is in reality true. In the <span class="math inline">\(2 \times 2\)</span> descision table in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:alphatable">4.6</a>, this corresponds to the top left quadrant.</p>
<p>As a researcher, you decide how much risk you are willing to take to make a Type I error. As the Neyman Pearson approach is a decision framework, you have to set this probability before you start collecting data. The most common value for <span class="math inline">\(\alpha\)</span> is .05, which means that you accept a 5% chance of making a Type I error.</p>
<style type="text/css">
.alpha { stroke: black;}
#alpha > svg { transform: scale(.8);}
</style>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:alphatable"></span>
<img src="figures/decision_table.svg" alt="NHST decision table." width="300px" />
<p class="caption">
Figure 4.6: NHST decision table.
</p>
</div>
<p>In our yellow candy example, assuming the null hypothesis to be true, relates to the parameter value of .5 and the associated probability distribution shown in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nulldistribution">4.3</a>. We have already determined that if <span class="math inline">\(H_0\)</span> is true, it is still possible we could get a bag with 0 or 10 yellow candies. Deciding to reject the null hypothesis in any of these cases, would be wrong, because the null hypothesis is assumed to be true. The exact probabilities can be found on the y-axis of figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nulldistribution">4.3</a>, and are also shown in the table <a href="4.2-null-hypothesis-significance-testing.html#tab:nullprobtable">4.1</a> below. Looking at the probability of getting 0 or 10 candies in table <a href="4.2-null-hypothesis-significance-testing.html#tab:nullprobtable">4.1</a>, we see that together this amounts to .002 or 0.2%. If we would decide to only reject the null hypothesis if we would get 0 or 10 candies, this would be a wrong decision, but we would also know that the chance of such a decision is pretty low. Our Type I error, alpha, significance level, would be .002.</p>
<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important;">
<span id="tab:nullprobtable">Table 4.1: </span>Probabilities of drawing a certain amount of yellow candies from a bag of 10 candies, assuming the null hypothesis to be true.
</caption>
<tbody>
<tr>
<td style="text-align:left;">
Yellow
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
2
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
4
</td>
<td style="text-align:left;">
5
</td>
<td style="text-align:left;">
6
</td>
<td style="text-align:left;">
7
</td>
<td style="text-align:left;">
8
</td>
<td style="text-align:left;">
9
</td>
<td style="text-align:left;">
10
</td>
</tr>
<tr>
<td style="text-align:left;">
Prob
</td>
<td style="text-align:left;">
0.001
</td>
<td style="text-align:left;">
0.010
</td>
<td style="text-align:left;">
0.044
</td>
<td style="text-align:left;">
0.117
</td>
<td style="text-align:left;">
0.205
</td>
<td style="text-align:left;">
0.246
</td>
<td style="text-align:left;">
0.205
</td>
<td style="text-align:left;">
0.117
</td>
<td style="text-align:left;">
0.044
</td>
<td style="text-align:left;">
0.010
</td>
<td style="text-align:left;">
0.001
</td>
</tr>
</tbody>
</table>
<p>Choosing such an alpha level would result in a threshold between 0 and 2 and 9 and 10. We call this the <strong>critical value</strong> associated with the chosen alpha level. Where on the outside of the threshold we would reject the null hypothesis, and inside the threshold we would not reject the null hypothesis. So, if that is our decision criteria, we would reject the null hypothesis if we would draw a bag with 0 or 10 yellow candies, and not reject the null hypothesis if we would draw a bag with 1, 2, 3, 4, 5, 6, 7, 8, or 9 yellow candies. Amounting to a Type I error rate of .002 or 0.2%. Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nulldistributionalpha">4.7</a> shows the critical values for the null hypothesis distribution, and indicate what de decision would be for values on the outside and inside of the decision boundary.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nulldistributionalpha"></span>
<img src="GentleIntro_files/figure-html/nulldistributionalpha-1.png" alt="H0 binomial distributions with critical values" width="420px" />
<p class="caption">
Figure 4.7: H0 binomial distributions with critical values
</p>
</div>
<p>In the social sciences, we allow ourselves to make a wrong descision more often. We usually set the alpha level to .05. For our discrete example setting the alpha level to .05 is not really possible. Looking at table <a href="4.2-null-hypothesis-significance-testing.html#tab:nullprobtable">4.1</a>, we could raise the significance level to .044 if we would reject the null hypothesis if we would draw 0, 1, 2,, 8, 9 or 10 yellow candies. This would result in a Type I error rate of 4.4%. Though if we would also reject the null hypothesis with 3 or 7 yellow candies, we would have a Type I error rate of 5.7%. For a discrete probability distribution with a limited number of outcomes, it is not always possible to set the alpha level exactly to .05.</p>
<p>For continuous probability distributions, such as the normal distribution, it is possible to set the alpha level exactly to .05. For example the null hypothesis that average media literacy in the population of children equals 5.5 on a scale from one to ten.</p>
<p>We can estimate a sampling distribution around the hypothesized population value using a theoretical approach (Chapter <a href="2.3-theoretical-approx.html#theoretical-approx">2.3</a>). Remember (Section <a href="1.2-discreterandomvariable.html#expectedvalue">1.2.4</a>) that the population value is the expected value of the sampling distribution, that is, its mean (if the estimator is unbiased). The sampling distribution, then, is centered around the population value specified in the null hypothesis. This sampling distribution tells us the probabilities of all possible sample outcomes <em>if the null hypothesis is true</em>. It allows us to identify the most unlikely samples. In step two in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nullsampling">4.8</a>, we set the alpha level to .05. This means that we cut off 2.5% of the area in each tail of the sampling distribution. The <strong>critical values</strong> are the values that separate the 2.5% of the area in each tail from the 95% of the area in the middle. If we assume the population parameter to be 5.5, rejecting the null hypothesis would again be a wrong decision. Thus setting the boundary by using an alpha level of .05, would yield a wrong decision in 5% of the samples we take. Just like the discrete candy color case, we decide to reject <span class="math inline">\(H_0\)</span> on the outside of the critical value and not reject <span class="math inline">\(H_0\)</span> on the inside of the critical value.</p>
<p>Note that the reasoning for the discrete case and the continuous case is the same. The only difference is that for the continuous case we can set the alpha level exactly to .05.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nullsampling"></span>
<iframe src="https://sharon-klinkenberg.shinyapps.io/nullsampling/?showcase=0" width="775px" height="410px" data-external="1">
</iframe>
<p class="caption">
Figure 4.8: Sampling distribution of average media literacy according to the null hypothesis.
</p>
</div>
<p>Probability distributions other than the standard-normal distribution, however, do not have fixed critical values. Their critical values depend on the <em>degrees of freedom</em> of the test, usually abbreviated to <em>df</em>. The degrees of freedom of a test may depend on sample size, the number of groups that we compare, or the number of rows and columns in a contingency table. We do not have to worry about this.</p>
<p>The <em>t</em> distribution is an example of a probability distribution for which the critical values depend on the degrees of freedom of the test. In this case, the degrees of freedom are determined by sample size. Larger samples have more degrees of freedom and, as a consequence, they have slightly lower critical values. For samples that are not too small the critical values of <em>t</em> are near 2. You may have noticed this in Figure <a href="10.1-testing-a-null-hypothesis-with-a-theoretical-probability-distribution.html#fig:crit-df">10.1</a>.</p>
</div>
<div id="alpha-1" class="section level3 hasAnchor" number="4.2.5">
<h3><span class="header-section-number">4.2.5</span> 1 - Alpha<a href="4.2-null-hypothesis-significance-testing.html#alpha-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The decision to not reject the null hypothesis when it is in reality true is indicated by <span class="math inline">\(1 - \alpha\)</span>. It does not go by any other name, but in terms of probability, it is directly dependent on your desired type I error rate, your chosen alpha level. It therefore corresponds to probability of 1, 2, 3, 4, 5, 6, 7, 8, or 9 yellow candies in the candy factory example, a 99.8% (1 - .002) chance of making the correct decision. The inside of the critical value in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nulldistributionalpha">4.7</a> is the area where we do not reject the null hypothesis. In the <span class="math inline">\(2 \times 2\)</span> decision table in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:decisiontable">4.1</a>, this corresponds to the bottom left green quadrant.</p>
<p>Now that we have determined our critical value based on our desired alpha, significance level, we can use this to look at the power.</p>
</div>
<div id="power" class="section level3 hasAnchor" number="4.2.6">
<h3><span class="header-section-number">4.2.6</span> Power<a href="4.2-null-hypothesis-significance-testing.html#power" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The power refers to the probability of making the correct decision when the null hypothesis is false. In the <span class="math inline">\(2 \times 2\)</span> decision table in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:decisiontable">4.1</a>, this corresponds to the top right quadrant. As we have already set our decision criteria by choosing our alpha level in the previous step, we already know when we decide to reject the null. In figure (fig:nulldistributionalpha) we determined our type I error could be 0.2%, if we would reject the null hypothesis if we would draw 0 or 10 yellow candies. The critical value would in that case be between 0 and 1 and 9 and 10. This critical value carries over when determining the power.</p>
<p>Though, as we assume the null hypothesis to be false, we need to specify what alternative distribution to use. We already established that this would be the distribution with a parameter value of .2. In figure <a href="4.2-null-hypothesis-significance-testing.html#fig:altdistributionpower">4.9</a>, we see that our descision criteria is still the same. That we decide to reject the null when we sample 0 or 10 yellow candies. But the distribution has now changed.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:altdistributionpower"></span>
<img src="GentleIntro_files/figure-html/altdistributionpower-1.png" alt="H0 binomial distributions with critical values" width="420px" />
<p class="caption">
Figure 4.9: H0 binomial distributions with critical values
</p>
</div>
<p>If this alternative distribution would actually be true, deciding to reject the null would be a good decision. Though, we can also see that if this alternative is true, if the parameter turly is .2, getting a bag with 0 or 10 yellow candies does not happen that often. The probabilities for 10 yellow candies is almost none, and the probability for getting 0 yellow candies is about 11%. This means that if the alternative hypothesis is true, if our sample originates from the alternative hypothesis, we would only make the decision to reject the null hypothesis in 11% of the samples we get out of it. So, the power of the test, correctly rejecting the null when this specific alternative is true is only 11%.</p>
<blockquote>
<p>The only way to increase the power is to increase the sample size of the study.</p>
</blockquote>
<p>As stated earlier we would rather have a high probability of making the correct decision. In the social sciences we are striving for a power of .80. This means that we want to make the correct decision in 80% of the cases when the null hypothesis is false. In our candy factory example, this would mean that we would want to reject the null hypothesis in 80% of the replications. With our machine producing bags with 10 candies, this is just not possible. The only way to increase the power is to increase the sample size of the study. In the candy factory example, this would mean that we would have to increase the number of candies in the candy bags. We will come back to this in the chapter <a href="3.3-precisionsesamplesize.html#sample-size">3.3.1</a> on sample size.</p>
<p>One more thing to note, is that the true power of the test is only known when the alternative hypothesis is true. In practice, we do not know if the null or the alternative hypothesis is true. We can only calculate the power of the test when we assume some alternative hypothesis. It is good practice to base your assumptions about the alternative hypothesis on previous research, theory, or other empirical evidence. This is mostly expressed as the expected effect size, the expected difference between the null and the alternative hypothesis.</p>
<p>In all statistical software, the power of the test is not calculated based on the true effect size, but on the found effect size in your sample. This is called the observed power and will be covered in chapter <a href="4.2-null-hypothesis-significance-testing.html#observed-effect-size">4.2.10</a>.</p>
</div>
<div id="beta" class="section level3 hasAnchor" number="4.2.7">
<h3><span class="header-section-number">4.2.7</span> Beta<a href="4.2-null-hypothesis-significance-testing.html#beta" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The probability of making a Type II error is indicated by <span class="math inline">\(\beta\)</span>. It is the probability of not rejecting the null hypothesis when it is in reality false. In the <span class="math inline">\(2 \times 2\)</span> decision table in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:decisiontable">4.1</a>, this corresponds to the bottom right quadrant. The power of the test is <span class="math inline">\(1 - \beta\)</span>. In our candy factory example, the power of the test is .11, so the probability of making a Type II error is .89. It is the probability of getting a bag with 1, 2, 3, 4, 5, 6, 7, 8, or 9 yellow candies, when the machine actually produces bags with 2 yellow candies, the addition of the height of the bars in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:altdistributionpower">4.9</a> for these values.</p>
</div>
<div id="test-statistic" class="section level3 hasAnchor" number="4.2.8">
<h3><span class="header-section-number">4.2.8</span> Test statistic<a href="4.2-null-hypothesis-significance-testing.html#test-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In chapter <a href="1.2-discreterandomvariable.html#samplestatistic">1.2.1</a> we discussed the <strong>sample statistic</strong>, and defined it as any value describing a characteristic of the sample. This could be the mean, or the proportion, or the correlation, or the regression coefficient. It is a value that is calculated from the sample. Note that also conversions of the sample statistic, such as the difference between two sample means, or the ratio of two sample variances, <span class="math inline">\(t\)</span>-values, <span class="math inline">\(F\)</span>-values, and <span class="math inline">\(\chi^2\)</span>-values are also sample statistics. A test statistic more or less standardizes the difference between the sample statistic and the population value that we expect under the null hypothesis.</p>
<p>The <strong>test statistic</strong> is a sample statistic that is used to test the null hypothesis. In our candy factory example, the test statistic would be the number of yellow candies in the bag we sample. If we would draw a bag with 4 yellow candies, the test statistic would be 4.</p>
<p>In the previous sections, we have determined our decision criteria, the critical value, based on our desired alpha level. We have also determined the power of the test, based on the alternative hypothesis. The test statistic is used to determine if we reject the null hypothesis or not. If the test statistic is outside the critical value, we reject the null hypothesis. If the test statistic is inside the critical value, we do not reject the null hypothesis.</p>
<p>Looking at figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nulldistributionalpha">4.7</a>, we see that the critical value is between 0 and 1 and 9 and 10. If we would draw a bag with 4 yellow candies, we can check if the value 4 is inside or outside the critical value. As 4 is inside the critical value, we would not reject the null hypothesis.</p>
<blockquote>
<p>The test statistic is the value that is used to decide if we reject the null hypothesis or not.</p>
</blockquote>
<p>In the continuous case, as described in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nullsampling">4.8</a>, the test statistic is the sample mean. If the sample mean is outside the critical value, we reject the null hypothesis. If the sample mean is inside the critical value, we do not reject the null hypothesis. If you select Step 4 in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:nullsampling">4.8</a>, and draw a few samples, you can see if the test statistic, the sample mean, is inside or outside the critical value. Again, the reasoning for the continuous case is the same as for the discrete case.</p>
</div>
<div id="p-value" class="section level3 hasAnchor" number="4.2.9">
<h3><span class="header-section-number">4.2.9</span> P-value<a href="4.2-null-hypothesis-significance-testing.html#p-value" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How do we know that the test statistic that we have drawn is among the five percent most unlikely samples if the null hypothesis is true? In other words, how do we know that our sample statistic outcome is in the rejection region?</p>
<blockquote>
<p>The <strong>p-value</strong> is the probability of obtaining a test statistic at least as extreme as the result actually observed, under the assumption that the null hypothesis is true.</p>
<p>Rejecting the null hypothesis does not mean that this hypothesis is false or that the alternative hypothesis is true. Please, never forget this.</p>
</blockquote>
<p>We have learned that a test is statistically significant if the test statistic is in the rejection region. Statistical software, however, usually does not report the rejection region for the sample statistic. Instead, it reports the <em>p value</em> of the test, which is sometimes referred to as <em>significance</em> or <em>Sig.</em> in SPSS.</p>
<p>In the previous section we considered a sample with 4 yellow candies. The p-value considers the probability of such a sample, but also ads the probability of getting a sample with less yellow candies. This is what is meant with â€œat least as extremeâ€. this is not really intuitive, but it refers to the less likely test statistics, in our case 0, 1, 2 and 3, are even less probable than 4. The assumption that the null hypothesis is true indicates that we need to look at the probabilities from the null distribution. Looking at table <a href="4.2-null-hypothesis-significance-testing.html#tab:nullprobtable">4.1</a>, we see that the probability of getting 0, 1, 2, 3 or 4 yellow candies under the null distribution is 0.001 + 0.010 + 0.044 + 0.117 + 0.205 = 0.377. This is the p-value. The conditional probability of getting a sample that is as or less likely than the test statistic that we have. The conditional probability is referring to our assumption that the null hypothesis is true.</p>
<p>The reasoning applied when comparing our test statistic to the critical value is the same as when comparing the p-value to the alpha level. If the p-value is smaller or equal to than the alpha level, we reject the null hypothesis. If the p-value is larger than the alpha level, we do not reject the null hypothesis.</p>
<p>If the test statistic is within the critical values, the p-value is always larger than the alpha level. If the test statistic lies outside the critical value, the p-value is always smaller than the alpha level. In the case that the test statistic is exactly the same as the critical value, the p-value is exactly equal to the alpha level, we still decide to reject the null hypothesis.</p>

<div class="rmdimportant">
Reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(p\)</span>-value <span class="math inline">\(\leq \alpha\)</span>
</div>
<p>As both the p-value and the alpha level assume the null to be true, you can find both probabilities under the null distribution. In the continuous case, the p-value is the area under the curve of the probability distribution that is more extreme than the sample mean. The significance level is chosen by you as a researcher and is fixed.</p>
<blockquote>
<p>It is important to remember that a <em>p</em> value is a probability <em>under the assumption that the null hypothesis is true</em>. Therefore, it is a <em>conditional probability</em>.</p>
</blockquote>
<p>Compare it to the probability that we throw sixes with a dice. This probability is one out of six under the assumption that the dice is fair. Probabilities rest on assumptions. If the assumptions are violated, we cannot calculate probabilities.</p>
<p>If the dice is not fair, we donâ€™t know the probability of throwing sixes. In the same way, we have no clue whatsoever of the probability of drawing a sample like the one we have if the null hypothesis is not true in the population.</p>
<p>Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:t-alpha-p">4.10</a> shows a t-distribution for a null assumption with an alpha level of 5% (red area) and the p-value (blue area) for a random sample with a t-value of 2. For a two sided test (left) and a one sided test (right). We will cover one and two sided testing in chapter <a href="4.2-null-hypothesis-significance-testing.html#one-twosidedtests">4.2.14</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:t-alpha-p"></span>
<img src="GentleIntro_files/figure-html/t-alpha-p-1.png" alt="T-distributions with alpha level and p-value" width="820px" height="350px" />
<p class="caption">
Figure 4.10: T-distributions with alpha level and p-value
</p>
</div>
<p>In figure <a href="4.2-null-hypothesis-significance-testing.html#fig:t-alpha-p">4.10</a>, the red vertical boundaries represent the critical value associated with a chosen alpha level of 5%, the red area under the curve. The blue vertical line represents the t-value from the sample, which in this example was 2. The blue area under the curve represents the p-value, the probability of getting this t-value or more extreme.</p>
<p>Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:twosided">4.11</a> representing the sampling distribution of average media literacy. You can take a sample and play around with the population mean according to some null hypothesis. If the mean in the sample is outside the critical value, it falls in the alpha rejection region.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twosided"></span>
<iframe src="https://sharon-klinkenberg.shinyapps.io/twosided/?showcase=0" width="775px" height="268px" data-external="1">
</iframe>
<p class="caption">
Figure 4.11: Sampling distribution of average media literacy according to the null hypothesis.
</p>
</div>
<p>The reasoning is again the same as in the discrete case. If the p-value is smaller or equal to the alpha level, we reject the null hypothesis. If the p-value is larger than the alpha level, we do not reject the null hypothesis.</p>
</div>
<div id="observed-effect-size" class="section level3 hasAnchor" number="4.2.10">
<h3><span class="header-section-number">4.2.10</span> Observed effect size<a href="4.2-null-hypothesis-significance-testing.html#observed-effect-size" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In chapter <a href="4.2-null-hypothesis-significance-testing.html#true-effect-size">4.2.3</a> we discussed the true effect, the difference between the null hypothesis and the true alternative hypothesis. The problem is that we do not know the true effect, we do not know wich of the two hypothesis is actually true. In some cases we donâ€™t even know if our expected alternative hypothesis is correct.</p>
<p>We can only estimate the true effect using the sample statistic. The difference between the sample statistic and the null hypothesis is called the <strong>observed effect size</strong>. In the candy factory example, the observed effect size is the difference between the number of yellow candies in the sample and the number of yellow candies in the null hypothesis. If the null hypothesis is that the machine produces bags with 5 yellow candies, and the sample contains 4 yellow candies, the observed effect size is 1.</p>
<p>The same defenition holds for the continuous case. If the null hypothesis is that the average media literacy in the population is 5.5, and the sample mean is 3.9, the observed effect size is 1.6. Or if we hypothesize that average candy weight in the population is 2.8 grams and we find an average candy weight in our sample bag of 2.75 grams, the effect size is -0.05 grams. If a difference of 0.05 grams is a great deal to us, the effect is practically relevant.</p>
<p>Note that the effect sizes depend on the scale on which we measure the sample outcome. The unstandardized effect size of average candy weight changes if we measure candy weight in grams, micro grams, kilograms, or ounces. Of course, changing the scale does not affect the meaning of the effect size but the number that we are looking at is very different: 0.05 grams, 50 milligrams, 0.00005 kilos, or 0.00176 ounces. For this reason, we do not have rules of thumb for interpreting these <strong>unstandardized effect sizes</strong> in terms of small, medium, or large effects. But we do have rules of thumb for <strong>standardized effect sizes</strong>.</p>
<p>You can imagine that estimating the true effect size on just one sample is not very reliable. The observed effect size could be the result of our sample being the result of the null being true, or the alternative being true. The way researchers try to get a notion of the true effect size is by replicating the study. If the observed effect size is consistent over multiple replications, we can be more confident that the average observed effect size is the true effect size. This is what we will cover in chapter <a href="4.2-null-hypothesis-significance-testing.html#meta-analysis">4.2.12</a> about meta analysis.</p>
<div id="standardized-effect-size-cohens-d-for-one-or-two-means" class="section level4 hasAnchor" number="4.2.10.1">
<h4><span class="header-section-number">4.2.10.1</span> Standardized effect size: Cohenâ€™s <em>d</em> for one or two means<a href="4.2-null-hypothesis-significance-testing.html#standardized-effect-size-cohens-d-for-one-or-two-means" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In scientific research, we rarely have precise norms for raw differences (unstandardized effects) that are practically relevant or substantial. For example, what would be a practically relevant attitude change among people exposed to a health campaign?</p>
<p>To avoid answering this difficult question, we can take the variation in scores (standard deviation) into account. In the context of the candies example, we will not be impressed by a small difference between observed and expected (hypothesized) average candy weight if candy weights vary a lot. In contrast, if candy weight is quite constant, a small average difference can be important.</p>
<p>For this reason, standardized effect sizes for sample means divide the difference between the sample mean and the hypothesized population mean by the standard deviation in the sample. Thus, we take into account the variation in scores. This standardized observed effect size for tests on one or two means is known as <em>Cohenâ€™s</em> d.</p>
<hr />
<p>These are the formulas for Cohenâ€™s <em>d</em> for a one-sample <em>t</em> test, a paired-samples <em>t</em> test, and an independent-samples <em>t</em> test (they will be provided if needed):</p>
<div style="column-count: 3; -moz-column-count: 3">
<span class="math display">\[\begin{equation}
  d_{one_-sample} = \frac{M - \mu_0}{SD}
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
  d_{paired_-samples} = \frac{M_{diff} - \mu_{0_-diff}}{SD_{diff}}
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
  d_{independent_-samples} = \frac{2*t}{\sqrt(df)}
\end{equation}\]</span>
</div>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(M\)</span> is the sample mean, <span class="math inline">\(\mu_0\)</span> is the hypothesized population mean, and <span class="math inline">\(SD\)</span> is the standard deviation in the sample,</p></li>
<li><p><span class="math inline">\(M_{diff}\)</span> is the difference between the two means in the sample, <span class="math inline">\(\mu_{0_-diff}\)</span> is the hypothesized difference between the two means in the population mean, which is zero in case of a nil hypothesis, and <span class="math inline">\(SD_{diff}\)</span> is the standard deviation of the difference in the sample,</p></li>
<li><p><span class="math inline">\(t\)</span> is the test statistic value and <span class="math inline">\(df\)</span> is the number of degrees of freedom of the <em>t</em> test.</p></li>
</ul>
<hr />
<p>The sample outcome can be a single mean, for instance the average weight of candies, but it can also be the difference between two means, for example, the difference in colourfulness of yellow candies at the beginning and end of a time period. In the latter case, the standard deviation that we need is the standard deviation of colourfulness difference across all candies (Section <a href="2.3-theoretical-approx.html#dependentsamples">2.3.6</a>). In the case of independent samples, such as average weight of red versus yellow candies, we need a special combined (<em>pooled</em>) standard deviation for yellow and red candy weight that is not reported by SPSS. Here, we use the <em>t</em> value and degrees of freedom to calculate Cohenâ€™s <em>d</em>.</p>
<p>Using an inventory of published results of tests on one or two means, <span class="citation">Cohen (<a href="12.3-take-home-points-11.html#ref-RefWorks:3933">1969</a>)</span> proposed rules of thumb for standardized effect sizes (ignore a negative sign if it occurs):</p>
<ul>
<li>0.2: weak (small) effect,</li>
<li>0.5: moderate (medium) effect,</li>
<li>0.8: strong (large) effect.</li>
</ul>
<p>Note that Cohenâ€™s <em>d</em> can take values above one. These are not errors, they reflect very strong or huge effects <span class="citation">(<a href="12.3-take-home-points-11.html#ref-sawilowskyNewEffectSize2009">Sawilowsky, 2009</a>)</span>.</p>
<div id="obtaining-cohens-d-with-spss" class="section level5 hasAnchor" number="4.2.10.1.1">
<h5><span class="header-section-number">4.2.10.1.1</span> Obtaining Cohenâ€™s <em>d</em> with SPSS<a href="4.2-null-hypothesis-significance-testing.html#obtaining-cohens-d-with-spss" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cohend"></span>
<iframe src="https://www.youtube.com/embed/HmyW7HRM64Q" width="640px" height="360px" data-external="1">
</iframe>
<p class="caption">
Figure 4.12: Obtaining Cohenâ€™s <em>d</em> with SPSS.
</p>
</div>
<p>Unfortunately, the t test commands in SPSS have no option to calculate Cohenâ€™s d.Â It is, however, relatively easy to calculate Cohenâ€™s <em>d</em> by hand from SPSS output. Remember that we must divide the unstandardized effect by the standard deviation.</p>
<p>For a t test on one mean, the unstandardized effect is the difference between the sample mean and the hypothesized mean. SPSS reports this value in the column <strong>Mean Difference</strong> of the table with test results. Drop any negative signs! Divide it by the standard deviation of the variable as given in Table <strong>One-Sample Statistics</strong>.</p>
<p>In the example, Cohenâ€™s <em>d</em> is 0.036 / 0.169 = 0.21. This is a weak effect.</p>
<p>For a paired-samples t test, the unstandardized effect size is reported in the column <strong>Mean</strong> in the Table <strong>Paired Samples Test</strong>. The standard deviation of the difference can be found in column <strong>Std. Deviation</strong> in the same table. Divide the first by the second, for instance, 1.880 / 1.033 = 1.82. This is a strong effect.</p>
<p>For an independent-samples t test, the situation is less fortuitous because SPSS does not report the pooled sample standard deviation that we need. The pooled sample standard deviation takes a sort of average of the outcome variableâ€™s standard deviations in the two groups. As an approximation, we can calculate Cohenâ€™s <em>d</em> as follows: Double the t value and divide it by the square root of the degrees of freedom.</p>
<p>In the example, Cohenâ€™s <em>d</em> equals <span class="math inline">\((2 * 0.651) / \surd(18) = 0.31\)</span>. This is a moderate effect size.</p>
</div>
</div>
<div id="assoc-size" class="section level4 hasAnchor" number="4.2.10.2">
<h4><span class="header-section-number">4.2.10.2</span> Association as effect size<a href="4.2-null-hypothesis-significance-testing.html#assoc-size" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Measures of association such as Pearsonâ€™s product-moment correlation coefficient or Spearmanâ€™s rank correlation coefficient express effect size if the null hypothesis expects no correlation in the population. If zero correlation is expected, a correlation coefficient calculated for the sample expresses the difference between what is observed (sample correlation) and what is expected (zero correlation in the population).</p>
<p>Effect size is also zero according to the standard null hypotheses used for tests on the regression coefficient (<em>b</em>), <em>R</em><sup>2</sup> for the regression model, and eta<sup>2</sup> for analysis of variance. As a result, we can use the standardized regression coefficient (Beta in SPSS and <em>b</em>* according to APA), <em>R</em><sup>2</sup>, and eta<sup>2</sup> as standardized effect sizes.</p>
<p>Because they are standardized, we can interpret their effect sizes using rules of thumb. The rule of thumb for interpreting a standardized regression coefficient (<em>b</em>*) or a correlation coefficient, for example, could be that a value between 0 and .10 is interpreted as no or a very weak association, between .10 and .30 as weak, between .30 and .50 as moderate, .50 to .80 as strong, and .80 to 1.00 as very strong, while exactly 1.00 is a perfect association. Note that we ignore the sign (plus or minus) of the effect when we interpret its size.</p>
</div>
</div>
<div id="post-hoc-power" class="section level3 hasAnchor" number="4.2.11">
<h3><span class="header-section-number">4.2.11</span> Post hoc power<a href="4.2-null-hypothesis-significance-testing.html#post-hoc-power" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just as the observed effect size is based on the test statistic acquired from your sample, so is the post hoc power. It is also known as: observed, retrospective, achieved, prospective and a priori power <span class="citation">(<a href="12.3-take-home-points-11.html#ref-doi:10.1080/19312450701641375">Oâ€™Keefe, 2007</a>)</span>.</p>
<blockquote>
<p>The power of a test assuming a population effect size equal to the observed effect size in the current sample.</p>
</blockquote>
<p>The post hoc power refers to the probability of rejecting the null hypothesis assuming the alternative hypothesis has a population mean equal to the observed sample mean or more accurately the observed test statistic. Though SPSS produces this when you ask it, it is obvious that multiple replications of a research study will yield different results. As the true population mean is not a random variable, the actual power is fixed and should not vary.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:posthocpower"></span>
<img src="GentleIntro_files/figure-html/posthocpower-1.png" alt="Discrete binomial distributions showing post hoc power" width="420px" />
<p class="caption">
Figure 4.13: Discrete binomial distributions showing post hoc power
</p>
</div>
<p>Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:posthocpower">4.13</a> shows the post hoc power for a sample of 10 candies. The null hypothesis is that the machine produces bags with 5 yellow candies. The alternative hypothesis is that the machine produces bags with 2 yellow candies. But the post hoc power assumes the found test statistic of 4 candies to be the alternative population parameter of .4. Following the same decision criteria as defined in the previous sections, the post hoc power is almost zero. This is the probability of 0 or 1 yellow candies under the alternative distribution.</p>
<p>You can imagine that if we look at a different candy bag and we would find 7 yellow candies, the post hoc power would be not be the same. The post hoc power does not have much practical use, though you should be warned that this is what SPSS produces when you ask for power.</p>
<!-- create a new shiny app with null distribution, and random sample with mean and alternative distribtuion and post hoc power area under the HA distribution -->
</div>
<div id="meta-analysis" class="section level3 hasAnchor" number="4.2.12">
<h3><span class="header-section-number">4.2.12</span> Meta analysis<a href="4.2-null-hypothesis-significance-testing.html#meta-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned in chapter <a href="4.2-null-hypothesis-significance-testing.html#observed-effect-size">4.2.10</a>, the observed effect size is based on the sample statistic, and will differ with every sample you take. If our research hypothesis is actually true, than a random sample from the alternative distribution would more often result in sampled number of yellow candies close to 2. But as we have seen in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:altdistribution">4.4</a>, getting 4 yellow candies is reasonably probable.</p>
<p>Now imagine that we would take multiple samples from the alternative distribution, and calculate the observed effect size for each sample. If we would plot these observed effect sizes, we would get a sampling distribution of observed effect sizes.</p>
<p>In research we conduct replication studies to see if the observed effect size is consistent over multiple replications. If this is the case, we can be more confident that the average observed effect size is the true effect size and we can determine the parameter of the alternative hypothesis.</p>
<p>Again imagine that we get a hundred bags of candy and we consistently find 7 to 9 yellow candies, this would give us an indication of the true effect size being 8. It would also indicate that our initial alternative hypothesis is highly unlikely.
This is essentially what meta analysis is about. Collecting effect sizes from multiple studies and combining them to get an indication of the true effect size.</p>
<p>[to-do] add communication meta analysis example.</p>
<p>Meta-analysis is a good example of combining research efforts to increase our understanding. It favours estimation over hypothesis testing because the goal is to obtain more precise estimates of population values or effects. Meta-analysis is strongly recommended as a research strategy by Geoff Cumming, who coined the concept <em>New Statistics</em>. See Cummingâ€™s book <span class="citation">(<a href="12.3-take-home-points-11.html#ref-RefWorks:3883">2012</a>)</span>, <a href="http://www.latrobe.edu.au/psychology/research/research-areas/cognitive-and-developmental-psychology/esci">website</a>, or <a href="https://www.youtube.com/user/geoffdcumming">YouTube channel</a> if you are curious to learn more.</p>
</div>
<div id="sample-size" class="section level3 hasAnchor" number="4.2.13">
<h3><span class="header-section-number">4.2.13</span> Sample size<a href="3.3-precisionsesamplesize.html#sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As stated in chapter <a href="4.2-null-hypothesis-significance-testing.html#power">4.2.6</a>, the only way to increase the power of a test is to increase the sample size. In the candy factory example, the sample size is the total number of candies in the bag. With only 10 candies in the bag, the power of the test is only 0.11. To reach our desired power of 80%, we clearly need to increase the sample size. In figure <a href="4.2-null-hypothesis-significance-testing.html#fig:twobinomN20">4.14</a>, we increased the number of candies in the bag to 20. We can see on the x-axis that the posible outcome space for the number of yellow candies in the bag is now 0 to 20. This of still assumes our <span class="math inline">\(H_0\)</span> to be true, and the parameter of the machine is still <span class="math inline">\(\theta = .5\)</span>, half of the candies in the bag should be yellow. Though the parameter is still the same, the expected value when we have bags of 20 cancies is now <span class="math inline">\(.5 \times 20 = 10\)</span>, right in the middle of our distribution.</p>
<p>Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:twobinomN20">4.14</a> still follows the reasoning scheme we have setup earlier. We decide to reject <span class="math inline">\(H_0\)</span> on the outside of our critical values (Red vertical line). We determined the position of the critical value based on our chosen alpha level. Because our outcome space is larger we can be more accurate in striving for an <span class="math inline">\(\alpha = .5\)</span>. Our alpha is now 4.1%, we get this by adding the yellow bars 0, 1, 2, 3, 4, 5 and 15 up unitil 20, under the null distribution. This is not exactly 5 percent, but shifting the critical value inwards, would make the alpha level to high. So, this is close enough.</p>
<p>With this sample size, we can acquire our desired power of 80%. If we would assume our alternative hypothesis to be true, our decision to reject the null when you get 5 or less yellow candies, would be correct 80% of the time. The power of 80% is the sum of the light yellow bars under the assumption that <span class="math inline">\(H_A\)</span> is true on the outside of our critical value. So, the power is the probability of getting 0, 1, 2, 3 ,4 ,5 or 15, 16,17, 18, 19 ,20 yellow candies under the alternative distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twobinomN20"></span>
<img src="GentleIntro_files/figure-html/twobinomN20-1.png" alt="Discrete binomial distributions" width="820px" />
<p class="caption">
Figure 4.14: Discrete binomial distributions
</p>
</div>
<p>The same reasoning is applied to the continuous case. Letâ€™s revisit the candy weight example. We could have a null hypothesis that the average yellow candy weight is the same as the weight of all other candy colors. But if in reality the yellow candies would be heavier, letâ€™s say with an effect size of .3, we would need to determine what sample size we would need to get a power of 80% and a alpha of 5%.</p>
<p>Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:sample-size-power">4.15</a> shows the relation between sample size, power, alpha and effect size. You can play around with the sliders de determine what sample size you would need to obtain a power of 80% for an effect size of .3.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sample-size-power"></span>
<iframe src="https://sharon-klinkenberg.shinyapps.io/sample-size-power2/?showcase=0" width="775px" height="385px" data-external="1">
</iframe>
<p class="caption">
Figure 4.15: How does test power depend on effect size, type of test, significance level, and sample size? Sampling distributions of the sample mean under the null hypothesis (H<sub>0</sub>, left-hand curve) and under the assumed true value of the population mean (H<sub>1</sub>, right-hand curve) for a one-sample <em>t</em> test.
</p>
</div>
<p>Just like the discrete case, we choose an alpha level, and we can see the critical value in the null distribution. The alpha level of 5% is the area under the curve of the null distributon on the outside of the critical values. The power is the area under the alternative distribution that is outside the critical values.</p>
<p>The reasoning is again the same as in the discrete case. We first determine our desired alpha and power, make sure our sample size is large enough to get the desired power, for our effect size of interest. Then, when we collect our data, we can calculate our test statistic and determine if we can reject the null hypothesis or not, being confident that we will be wrong in our conclusion in 5% of the cases, and that we will be right in 80% of the cases when the alternative hypothesis is actually true.</p>
<div id="how-to-determine-sample-size" class="section level4 hasAnchor" number="4.2.13.1">
<h4><span class="header-section-number">4.2.13.1</span> How to determine sample size<a href="4.2-null-hypothesis-significance-testing.html#how-to-determine-sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As stated in chapter @ref(#power) about the power of a test, we already considered that we do not know the parameter for the alternative distribution and that we therefore also donâ€™t know the true effect size. We stated that you can make an educated guess about the true effect size based on previous research, theory, or other empirical evidence.</p>
<p>In research you can take these assumptions into account by conducting a power analysis. A power analysis is a statistical method to determine the sample size you need to get a desired power for a given effect size.</p>
<p>It can be difficult to specify the effect size that we should expect or that is practically relevant. If there is little prior research comparable to our new project, we cannot reasonably specify an effect size and calculate sample size. Though, if there are meta analysis available for you research topic of interest or you have the effect sizes from a few previous studies, you can use G<em>Power to calculate the sample size you need to get a desired power for a given effect size. G</em>Power is a stand alone program that can be downloaded from the internet, and is specificly designed to calculate sample size for a wide range of statistical tests.</p>
<blockquote>
<p>Download <a href="http://www.gpower.hhu.de/">G*Power here</a></p>
</blockquote>
<p>In G<em>Power you can specify the test you want to conduct, the effect size you expect, the alpha level you want to use, and the power you want to achieve. G</em>Power will then calculate the sample size you need to get the desired power for the given effect size.</p>
<p>For our candy color example, we can use G*Power to calculate the sample size we need to get a power of 80% for a given effect size of .3.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:g-power"></span>
<img src="figures/g-power.png" alt="Power analysis in G*Power for a binomial distribution" width="500px" />
<p class="caption">
Figure 4.16: Power analysis in G*Power for a binomial distribution
</p>
</div>
<p>In figure <a href="4.2-null-hypothesis-significance-testing.html#fig:g-power">4.16</a> you can see that for the binomial test we have set the proportion p1 to .5 (<span class="math inline">\(H_0\)</span>) and the proportion p2 (<span class="math inline">\(H_A\)</span>) to .2, indirectly setting the effect size to .3. We have set the alpha level to 5% and the power to 80%. By hitting the calculate button, G*Power will calculate the sample size we need. In this case we need 20 candies in the bag to get a power of 80%. The plot shows exactly the same information as in figure <a href="4.2-null-hypothesis-significance-testing.html#fig:twobinomN20">4.14</a>, though with lines instead of bars.</p>
</div>
</div>
<div id="one-twosidedtests" class="section level3 hasAnchor" number="4.2.14">
<h3><span class="header-section-number">4.2.14</span> One-Sided and Two-Sided Tests<a href="4.2-null-hypothesis-significance-testing.html#one-twosidedtests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the preceding section, you may have had some trouble when you were determining whether a research hypothesis is a null hypothesis or an alternative hypothesis. The research hypothesis stating that average media literacy is below 5.5 in the population, for example, represents the alternative hypothesis because it does not fix the hypothesized population value to one number. The accompanying null hypothesis must cover all other options, so it must state that the population mean is 5.5 or higher. But this null hypothesis does not specify one value as it should, right?</p>
<p>This null hypothesis is slightly different from the ones we have encountered so far, which equated the population value to a single value. If the null hypothesis equates a parameter to a single value, the null hypothesis can be rejected if the sample statistic is either too high or too low. There are two ways of rejecting the null hypothesis, so this type of hypothesis and test are called <em>two-sided</em> or <em>two-tailed</em>.</p>
<p>By contrast, the null hypothesis stating that the population mean is 5.5 or higher is a <em>one-sided</em> or <em>one-tailed</em> hypothesis. It can only be rejected if the sample statistic is at one side of the spectrum: only below (left-sided) or only above (right-sided) the hypothesized population value. In the media literacy example, the null hypothesis is only rejected if the sample mean is well below the hypothesized population value. A test of a one-sided null hypothesis is called a <em>one-sided test</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nonsig-1sided"></span>
<iframe src="https://sharon-klinkenberg.shinyapps.io/nonsig-1sided/?showcase=0" width="420px" height="310px" data-external="1">
</iframe>
<p class="caption">
Figure 4.17: One-sided and two-sided tests of a null hypothesis.
</p>
</div>
<p>In a left-sided test of the media literacy hypothesis, the researcher is not interested in demonstrating that average media literacy among children can be larger than 5.5. She only wants to test if it is below 5.5, perhaps because an average score below 5.5 is alarming and requires an intervention, or because prior knowledge about the world has convinced her that average media literacy among children can only be lower than 5.5 on average in the population.</p>
<p>If it is deemed important to note values well over 5.5 as well as values well below 5.5, the research and null hypotheses should be two-sided. Then, a sample average well above 5.5 would also have resulted in a rejection of the null hypothesis. In a left-sided test, however, a high sample outcome cannot reject the null hypothesis.</p>
<div id="boundary-value-as-hypothesized-population-value" class="section level4 hasAnchor" number="4.2.14.1">
<h4><span class="header-section-number">4.2.14.1</span> Boundary value as hypothesized population value<a href="4.2-null-hypothesis-significance-testing.html#boundary-value-as-hypothesized-population-value" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sign-left"></span>
<iframe src="https://sharon-klinkenberg.shinyapps.io/sign-left/?showcase=0" width="775px" height="268px" data-external="1">
</iframe>
<p class="caption">
Figure 4.18: Sampling distribution of average media literacy.
</p>
</div>
<p>You may wonder how a one-sided null hypothesis equates the parameter of interest with one value as it should. The special value here is 5.5. If we can reject the null hypothesis stating that the population mean is 5.5 because our sample mean is sufficiently lower than 5.5, we can also reject any hypothesis involving population means higher than 5.5.</p>
<p>In other words, if you want to know if the value is not 5.5 or more, it is enough to find that it is less than 5.5. If itâ€™s less than 5.5, then you know itâ€™s also less than any number above 5.5. Therefore, we use the boundary value of a one-sided null hypothesis as the hypothesized value for the population in a one-sided test.</p>
</div>
<div id="one-sided-two-sided-distinction-is-not-always-relevant" class="section level4 hasAnchor" number="4.2.14.2">
<h4><span class="header-section-number">4.2.14.2</span> One-sided â€“ two-sided distinction is not always relevant<a href="4.2-null-hypothesis-significance-testing.html#one-sided-two-sided-distinction-is-not-always-relevant" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Note that the difference between one-sided and two-sided tests is only useful if we test a statistic against one particular value or if we test the difference between two groups.</p>
<p>In the first situation, for example, if we test the null hypothesis that average media literacy is 5.5 in the population, we may only be interested in showing that the population value is lower than the hypothesized value. Another example is a test on a regression coefficient or correlation coefficient. According to the null hypothesis, the coefficient is zero in the population. If we only want to use a brand advertisement if exposure to the advertisement increases brand awareness among consumers, we apply a right-sided test to the coefficient for the effect of exposure on brand awareness because we are only interested in a positive effect (larger than the zero).</p>
<p>In the second situation, we compare the scores of two groups on a dependent variable. If we compare average media literacy after an intervention to media literacy before the intervention (paired-samples <em>t</em> test), we must demonstrate an increase in media literacy before we are going to use the intervention on a large scale. Again, a one-sided test can be applied.</p>
<p>In contrast, we cannot meaningfully formulate a one-sided null hypothesis if we are comparing three groups or more. Even if we expect that Group A can only score higher than Group B and Group C, what about the difference between Group B and Group C? If we canâ€™t have meaningful one-sided null hypotheses, we cannot meaningfully distinguish between one-sided and two-sided null hypotheses.</p>
</div>
<div id="from-one-sided-to-two-sided-p-values-and-back-again" class="section level4 hasAnchor" number="4.2.14.3">
<h4><span class="header-section-number">4.2.14.3</span> From one-sided to two-sided <em>p</em> values and back again<a href="4.2-null-hypothesis-significance-testing.html#from-one-sided-to-two-sided-p-values-and-back-again" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Statistical software like SPSS usually reports either one-sided or two-sided <em>p</em> values. What if a one-sided <em>p</em> value is reported but you need a two-sided <em>p</em> value or the other way around?</p>
<p>In Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:onetwosided">4.19</a>, the sample mean is 3.9 and we have .015 probability of finding a sample mean of 3.9 or less if the null hypothesis is true. This probability is the surface under the curve to the left of the red line representing the sample mean. It is the one-sided <em>p</em> value that we obtain if we only take into account the possibility that the population mean can be smaller than the hypothesized value. We are only interested in the left tail of the sampling distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:onetwosided"></span>
<iframe src="https://sharon-klinkenberg.shinyapps.io/onetwosided/?showcase=0" width="775px" height="268px" data-external="1">
</iframe>
<p class="caption">
Figure 4.19: Halve a two-sided <em>p</em> value to obtain a one-sided <em>p</em> value, double a one-sided <em>p</em> value to obtain a two-sided <em>p</em> value.
</p>
</div>
<p>In a two-sided test, we have to take into account two different types of outcomes. Our sample outcome can be smaller or larger than the hypothesized population value. As a consequence, the <em>p</em> value must cover samples at opposite sides of the sampling distribution. We should not only take into account sample means that are smaller than 5.5 but also sample means that are just as much larger than the hypothesized population value. So our two-sided <em>p</em> value must include both the probability of .015 for the left tail and for the right tail of the distribution in Figure <a href="4.2-null-hypothesis-significance-testing.html#fig:onetwosided">4.19</a>. We must double the one-sided <em>p</em> value to obtain the two-sided <em>p</em> value.</p>
<p>In contrast, if our statistical software tells us the two-sided <em>p</em> value and we want to have the one-sided <em>p</em> value, we can simply halve the two-sided <em>p</em> value. The two-sided <em>p</em> value is divided equally between the left and right tails. If we are interested in just one tail, we can ignore the half of the <em>p</em> value that is situated in the other tail. Of course, this only makes sense if a one-sided test makes sense.</p>
<p>Be careful if you divide a two-sided <em>p</em> value to obtain a one-sided <em>p</em> value. If your left-sided test hypothesizes that average media literacy is below 5.5 but your sample mean is well above 5.5, the two-sided <em>p</em> value can be below .05. But your left-sided test can never be significant because a sample mean above 5.5 is fully in line with the null hypothesis. Check that the sample outcome is at the correct side of the hypothesized population value.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4.1-binarydecision.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4.3-confidence-intervals-to-test-hypotheses.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

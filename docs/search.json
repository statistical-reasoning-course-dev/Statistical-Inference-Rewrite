[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Introduction and Reader’s Guide\nThis book offers a non-technical but thorough introduction to statistical inference. It discusses a minimal set of concepts needed to understand both the possibilities and pitfalls of estimation, null hypothesis testing, moderation, and mediation analysis. It uses a minimum of formal notation.\n\nIntended Audience and Setting\nThis book is written as reading material for a follow-up course in statistics, in the bachelor of Communication Science at the University of Amsterdam. Students enrolled in this course have passed an introductory course in statistics that explained how to change research questions into variables and associations between variables, how to select and execute the correct analysis or test (in SPSS) to answer their research question, and how to interpret the results in a language that is both comprehensible for the average reader and complying with professional standards (APA standard for reporting test results). In addition, they have learned the very basics of inferential statistics: How to decide which null hypothesis to reject based on reported p values, and how to interpret confidence intervals.\nThis book is meant for use in a flipped-classroom setting. Students should read the text, watch embedded videos, and play with the interactive content before they meet in class. Class meetings are used to answer questions raised by the students, do group work to exercise with the concepts and techniques presented in the text, and do short tests to check understanding.\n\n\nInteractive Content\nThe interactive content in this book replaces simulations that used to be demonstrated during lectures. I expect that doing simulations yourself rather than watching them being done by someone else enhances understanding. I have tried to break down the simulations into smaller steps, confronting the student several times with essentially the same simulation, but with added complexity. I hope that this approach enhances understanding and remembrance and, at the same time, avoids frustration caused by complex dashboards offering all options at once.",
    "crumbs": [
      "Introduction and Reader's Guide"
    ]
  },
  {
    "objectID": "01-samplingdistr.html",
    "href": "01-samplingdistr.html",
    "title": "1  Sampling Distribution",
    "section": "",
    "text": "Summary\nWatch the micro lecture Video 1.1 on sampling distributions for an overview of the chapter.\nStatistical inference is about estimation and null hypothesis testing. We have collected data on a random sample and we want to draw conclusions (make inferences) about the population from which the sample was drawn. From the proportion of yellow candies in our sample bag, for instance, we want to estimate a plausible range of values for the proportion of yellow candies in a factory’s stock (confidence interval). Alternatively, we may want to test the null hypothesis that one fifth of the candies in a factory’s stock is yellow.\nThe sample does not offer a perfect miniature image of the population. If we would draw another random sample, it would have different characteristics. For instance, it would contain more or fewer yellow candies than the previous sample. To make an informed decision on the confidence interval or null hypothesis, we must compare the characteristic of the sample that we have drawn to the characteristics of the samples that we could have drawn.\nThe characteristics of the samples that we could have drawn constitute a sampling distribution. Sampling distributions are the central element in estimation and null hypothesis testing. In this chapter, we simulate sampling distributions to understand what they are. Here, simulation means that we let a computer draw many random samples from a population.\nIn Communication Science, we usually work with samples of human beings, for instance, users of social media, people looking for health information or entertainment, citizens preparing to cast a political vote, an organization’s stakeholders, or samples of media content such as tweets, tv advertisements, or newspaper articles. In the current and two subsequent chapters, however, we avoid the complexities of these samples.\nWe focus on a very tangible kind of sample, namely a bag of candies, which helps us understand the basic concepts of statistical inference: sampling distributions (the current chapter), probability distributions (Chapter 2), and estimation (Chapter 3). Once we thoroughly understand these concepts, we turn to Communication Science examples.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "01-samplingdistr.html#statistical-inference-making-the-most-of-your-data",
    "href": "01-samplingdistr.html#statistical-inference-making-the-most-of-your-data",
    "title": "1  Sampling Distribution",
    "section": "1.1 Statistical Inference: Making the Most of Your Data",
    "text": "1.1 Statistical Inference: Making the Most of Your Data\nStatistics is a tool for scientific research. It offers a range of techniques to check whether statements about the observable world are supported by data collected from that world. Scientific theories strive for general statements, that is, statements that apply to many situations. Checking these statements requires lots of data covering all situations addressed by theory.\nCollecting data, however, is expensive, so we would like to collect as little data as possible and still be able to draw conclusions about a much larger set. The cost and time involved in collecting large sets of data are also relevant to applied research, such as market research. In this context we also like to collect as little data as necessary.\nInferential statistics offers techniques for making statements about a larger set of observations from data collected for a smaller set of observations. The large set of observations about which we want to make a statement is called the population. The smaller set is called a sample. We want to generalize a statement about the sample to a statement about the population from which the sample was drawn.\nTraditionally, statistical inference is generalization from the data collected in a random sample to the population from which the sample was drawn. This approach is the focus of the present book because it is currently the most widely used type of statistical inference in the social sciences. We will, however, point out other approaches in Chapter 4.\nStatistical inference is conceptually complicated and for that reason quite often used incorrectly. We will therefore spend quite some time on the principles of statistical inference. Good understanding of the principles should help you to recognize and avoid incorrect use of statistical inference. In addition, it should help you to understand the controversies surrounding statistical inference and developments in the practice of applying statistical inference that are taking place. Investing time and energy in fully understanding the principles of statistical inference really pays off later.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "01-samplingdistr.html#sec-discreterandomvariable",
    "href": "01-samplingdistr.html#sec-discreterandomvariable",
    "title": "1  Sampling Distribution",
    "section": "1.2 A Discrete Random Variable: How Many Yellow Candies in My Bag?",
    "text": "1.2 A Discrete Random Variable: How Many Yellow Candies in My Bag?\nAn obvious but key insight in statistical inference is this: If we draw random samples from the same population, we are likely to obtain different samples. No two random samples from the same population need to be identical, even though they can be identical.\n\n1.2.1 Sample statistic\nWe are usually interested in a particular characteristic of the sample rather than in the exact nature of each observation within the sample. For instance, I happen to be very fond of yellow candies. If I buy a bag of candies, my first impulse is to tear the bag open and count the number of yellow candies. Am I lucky today? Does my bag contain a lot of yellow candies?\n\n\n\n\n\n\n\n\n\nFigure 1.1: How many yellow candies will our sample bag contain?\n\n\n\nThe number of yellow candies in a bag is an example of a sample statistic: a value describing a characteristic of the sample. Each bag, that is, each sample, has one outcome score on the sample statistic. For instance, one bag contains four yellow candies, another bag contains seven, and so on. All possible outcome scores constitute the sampling space. A bag of ten candies may contain 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, or 10 yellow candies. The numbers 0 to 10 are the sampling space of the sample statistic number of yellow candies in a bag.\nThe sample statistic is called a random variable. It is a variable because different samples can have different scores. The value of a variable may vary from sample to sample. It is a random variable because the score depends on chance, namely the chance that a particular sample is drawn.\n\n\n1.2.2 Sampling distribution\nSome sample statistic outcomes occur more often than other outcomes. We can see this if we draw very many random samples from a population and collect the frequencies of all outcome scores in a table or chart. We call the distribution of the outcome scores of very many samples a sampling distribution.\n\n\n\n\n\n\n\nFigure 1.2: What is a sampling distribution?\n\n\n\n\n\n1.2.3 Probability and probability distribution\nWhat is the probability of buying a bag with exactly five yellow candies? In statistical terminology, what is the probability of drawing a sample with five yellow candies as sample statistic outcome? This probability is the proportion of all possible samples that we could have drawn that happen to contain five yellow candies.\nOf course, the probability of a sample bag with exactly five yellow candies depends on the share of yellow candies in the population of all candies. Figure 1.3 displays the probabilities of a sample bag with a particular number of yellow candies if twenty per cent of the candies in the population are yellow. You can adjust the population share of yellow candies to see what happens.\n\n\n\n\n\n\n\nFigure 1.3: How does the probability of drawing a sample bag with two out of ten candies yellow depend on the proportion of yellow candies in the population?\n\n\n\nThe sampling distribution tells us all possible samples that we could have drawn. We can use the distribution of all samples to get the probability of buying a bag with exactly five yellow candies from the sampling distribution: We divide the number of samples with five yellow candies by the total number of samples we have drawn. For example, if 26 out of all 1000 samples have five yellow candies, the proportion of samples with five yellow candies is 26 / 1000 = 0.026. Then, the probability of drawing a sample with five yellow candies is 0.026 (we usually write: .026).\nIf we change the frequencies in the sampling distribution into proportions, we obtain the probability distribution of the sample statistic: A sampling space with a probability (between 0 and 1) for each outcome of the sample statistic. Because we are usually interested in probabilities, sampling distributions tend to have proportions, that is probabilities, instead of frequencies on the vertical axis. See Figure 1.4 for an example.\nFigure 1.3 displays the probability distribution of the number of yellow candies per bag of ten candies. This is an example of a discrete probability distribution because only a limited number of outcomes are possible. It is feasible to list the probability of each outcome separately.\nThe sampling distribution as a probability distribution conveys very important information. It tells us which outcomes we can expect, in our example, how many yellow candies we may find in our bag of ten candies. Moreover, it tells us the probability that a particular outcome may occur. If the sample is drawn from a population in which 20% of candies are yellow, we are quite likely to find zero, one, two, three, or four yellow candies in our bag. A bag with five yellow candies would be rare, six or seven candies would be very rare, and a bag with more than seven yellow candies is extremely unlikely but not impossible. If we buy such a bag, we know that we have been extremely lucky.\nWe may refer to probabilities both as a proportion, that is, a number between 0 and 1, and as a percentage: a number between 0% and 100%. Proportions are commonly considered to be the correct way to express probabilities. When we talk about probabilities, however, we tend to use percentages; we may, for example, say that the probabilities are fifty-fifty.\n\n\n1.2.4 Expected value or expectation\nWe haven’t yet thought about the value that we are most likely to encounter in the sample that we are going to draw. Intuitively, it must be related to the distribution of colours in the population of candies from which the sample was drawn. In other words, the share of yellow candies in the factory’s stock from which the bag was filled or in the machine that produces the candies, seems to be relevant to what we may expect to find in our sample.\n\n\n\n\n\n\n\nFigure 1.4: What is the expected value of a probability distribution?\n\n\n\nIf the share of yellow candies in the population is 0.20 (or 20%), we expect one out of each five candies in a bag (sample) to be yellow. In a bag with 10 candies, we would expect two candies to be yellow: one out of each five candies or the population proportion times the total number of candies in the sample = 0.20 * 10 = 2.0. This is the expected value.\nThe expected value of the proportion of yellow candies in the sample is equal to the proportion of yellow candies in the population. If you carefully inspect a sampling distribution (Figure 1.4), you will see that the expected value also equals the mean of the sampling distribution. This makes sense: Excess yellow candies in some bags must be compensated for by a shortage in other bags.\nThus we arrive at the definition of the expected value of a random variable:\n\n\n\n\n\n\nThe expected value is the average of the sampling distribution of a random variable.\n\n\n\nIn our example, the random variable is a sample statistic, more specifically, the number of yellow candies in a sample.\nThe sampling distribution is an example of a probability distribution, so, more generally, the expected value is the average of a probability distribution. The expected value is also called the expectation of a probability distribution.\n\n\n1.2.5 Unbiased estimator\nNote that the expected value of the proportion of yellow candies in the bag (sample statistic) equals the true proportion of yellow candies in the candy factory (population statistic). For this reason, the sample proportion is an unbiased estimator of the proportion in the population. More generally, a sample statistic is called an unbiased estimator of the population statistic if the expected value (mean of the sampling distribution) is equal to the population statistic. By the way, we usually refer to the population statistic as a parameter.\nMost but not all sample statistics are unbiased estimators of the population statistic. Think, for instance, of the actual number of yellow candies in the sample. This is certainly not an unbiased estimator of the number of yellow candies in the population. Because the population is so much larger than the sample, the population must contain many more yellow candies than the sample. If we were to estimate the number in the population (the parameter) from the number in the sample—for instance, we estimate that there are two yellow candies in the population of all candies because we have two in our sample of ten—we are going to vastly underestimate the number in the population. This estimate is downward biased: It is too low.\nIn contrast, the proportion in the sample is an unbiased estimator of the population proportion. That is why we do not use the number of yellow candies to generalize from our sample to the population. Instead, we use the proportion of yellow candies. You probably already did this intuitively.\nSometimes, we have to adjust the way in which we calculate a sample statistic to get an unbiased estimator. For instance, we must calculate the standard deviation and variance in the sample in a special way to obtain an unbiased estimate of the population standard deviation and variance. The exact calculation need not bother us, because our statistical software takes care of that. Our software only uses unbiased estimators.\n\n\n1.2.6 Representative sample\nBecause the share of yellow candies in the population represents the probability of drawing a yellow candy, we also expect 20% of the candies in our bag to be yellow. For the same reason we expect the shares of all other colours in our sample bag to be equal to their shares in the population. As a consequence, we expect a random sample to resemble the population from which it is drawn.\nA sample is representative of a population (in the strict sense) if variables in the sample are distributed in the same way as in the population. Of course, we know that a random sample is likely to differ from the population due to chance, so the actual sample that we have drawn is usually not representative of the population in the strict sense.\nBut we should expect it to be representative, so we say that it is in principle representative or representative in the statistical sense of the population. We can use probability theory to account for the misrepresentation in the actual sample that we draw. This is what we do when we use statistical inference to construct confidence intervals and test null hypotheses, as we will learn in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "01-samplingdistr.html#sec-cont-random-var",
    "href": "01-samplingdistr.html#sec-cont-random-var",
    "title": "1  Sampling Distribution",
    "section": "1.3 A Continuous Random Variable: Overweight And Underweight.",
    "text": "1.3 A Continuous Random Variable: Overweight And Underweight.\nLet us now look at another variable: the weight of candies in a bag. The weight of candies is perhaps more interesting to the average consumer than candy colour because candy weight is related to calories.\n\n1.3.1 Continuous variable\nWeight is a continuous variable because we can always think of a new weight between two other weights. For instance, consider two candy weights: 2.8 and 2.81 grams. It is easy to see that there can be a weight in between these two values, for instance, 2.803 grams. Between 2.8 and 2.803 we can discern an intermediate value such as 2.802. In principle, we could continue doing this endlessly, e.g., find a weight between 2.80195661 and 2.80195662 grams even if our scales may not be sufficiently precise to measure any further differences. It is the principle that counts. If we can always think of a new value in between two values, the variable is continuous.\n\n\n\n\n\n\nContinuous variable: We can always think of a new value in between two values.\n\n\n\n\n\n1.3.2 Continuous sample statistic\nWe are not interested in the weight of a single candy. If a relatively light candy is compensated for by a relatively heavy candy in the same bag, we still get the calories that we want. We are interested in the average weight of all candies in our sample bag, so average candy weight in our sample bag is our key sample statistic. We want to say something about the probabilities of average candy weight in the samples of candies that we can draw. Can we do that?\nWhen we turn to the probabilities of getting samples with a particular average candy weight, we run into problems with a continuous sample statistic. If we would want to know the probability of drawing a sample bag with an average candy weight of 2.8 grams, we should exclude sample bags with an average candy weight of 2.81 grams, or 2.801 grams, or 2.8000000001 grams, and so on. In fact, we are very unlikely to draw a sample bag with an average candy weight of exactly 2.8 grams, that is, with an infinite number of zeros trailing 2.8. In other words, the probability of such a sample bag is for all practical purposes zero and negligible.\nThis applies to every average candy weight, so all probabilities are virtually zero. The probability distribution of the sampling space, that is, of all possible outcomes, is going to be very boring: just (nearly) zeros. And it will take forever to list all possible outcomes within the sampling space, because we have an infinite number of possible outcomes. After all, we can always find a new average candy weight between two selected weights.\n\n\n1.3.3 Probability density\nWith a continuous sample statistic, we must look at a range of values instead of a single value. We can meaningfully talk about the probability of having a sample bag with an average candy weight of at least 2.8 grams or at most 2.8 grams. We choose a threshold, in this example 2.8 grams, and determine the probability of values above or below this threshold. We can also use two thresholds, for example the probability of an average candy weight between 2.75 and 2.85 grams. This is probably what you were thinking of when I referred to a bag with 2.8 grams as average candy weight.\nIf we cannot determine the probability of a single value, which we used to depict on the vertical axis in a plot of a sampling distribution, and we have to link probabilities to a range of values on the x axis, for example, average candy weight above/below 2.8 grams, how can we display probabilities? We have to display a probability as an area between the horizontal axis and a curve. This curve is called a probability density function, so if there is a label to the vertical axis of a continuous probability distribution, it is “Probability density” instead of “Probability”.\nFigure 1.5 shows an example of a continuous probability distribution for the average weight of candies in a sample bag. This is the familiar normal distribution so we could say that the normal curve is the probability density function here. The total area under this curve is set to one, so the area belonging to a range of sample outcomes (average candy weight) is 1 or less, as probabilities should be.\n\n\n\n\n\n\n\nFigure 1.5: How do we display probabilities in a continuous sampling distribution? Tip: Click on a slider handle and use your keyboard arrow keys to make small changes to the slider handle position.\n\n\n\nA probability density function can give us the probability of values between two thresholds. It can also give us the probability of values up to (and including) a threshold value, which is known as a left-hand probability, or the probability of values above (and including) a threshold value, which is called a right-hand probability. In a null hypothesis significance test (Chapter 4), right-hand and left-hand probabilities are used to calculate p values.\nWhy did I put (and including) between parentheses? It does not really matter whether we add the exact boundary value (2.8 grams) to the probability on the left or on the right because the probability of getting a bag with average candy weight at exactly 2.8 grams (with a very long trail of zero decimals) is negligible.\nAre you struggling with the idea of areas instead of heights (values on the vertical axis) as probabilities? Just realize that we could use the area of a bar in a histogram instead of the height as indication of the probability in discrete probability distributions, for example, Figure 1.4. The bars in a histogram are all equally wide, so (relative) differences between bar areas are equal to differences in bar height.\n\n\n1.3.4 Probabilities always sum to 1\nWhile you were playing with Figure 1.5, you may have noticed that displayed probabilities always add up to one. This is true for every probability distribution because it is part of the definition of a probability distribution.\nIn addition, you may have realized that we can use probability distributions in two ways. We can use them to say how likely or unlikely we are to draw a sample with the sample statistic value in a particular range. For example, what is the chance that we draw a sample bag with average candy weight over 2.9 grams? But we can also use a probability distribution to find the threshold values that separate the top ten per cent or the bottom five per cent in a distribution. If we want a sample bag with highest average candy weight, say, belonging to the ten per cent bags with highest average candy weight, what should be the minimum average candy weight in the sample bag?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "01-samplingdistr.html#concluding-remarks",
    "href": "01-samplingdistr.html#concluding-remarks",
    "title": "1  Sampling Distribution",
    "section": "1.4 Concluding Remarks",
    "text": "1.4 Concluding Remarks\nA communication scientist wants to know whether children are sufficiently aware of the dangers of media use. On a media literacy scale from one to ten, an average score of 5.5 or higher is assumed to be sufficient.\nIf we translate this to the simple candy bag example, we realize that the outcome in our sample does not have to be the true population value, for example twenty per cent. If twenty per cent of all candies in the population are yellow, we could very well draw a sample bag with fewer or more than twenty per cent yellow candies.\nAverage media literacy, then, can exceed 5.5 in our sample of children, even if average media literacy is below 5.5 in the population or the other way around. How we decide on this is discussed in later chapters.\n\n1.4.1 Sample characteristics as observations\nPerhaps the most confusing aspect of sampling distributions is the fact that samples are our cases (units of analysis) and sample characteristics are our observations. We are accustomed to think of observations as measurements on empirical things such as people or candies. We perceive each person or each candy as a case and we observe a characteristic that may change across cases (a variable), for instance the colour or weight of a candy.\nIn a sampling distribution, however, we observe samples (cases) and measure a sample statistic as the (random) variable. Each sample adds one observation to the sampling distribution and its sample statistic value is the value added to the sampling distribution.\n\n\n1.4.2 Means at three levels\nIf we are dealing with the proportion of yellow candies in a sample (bag), the sample statistic is a proportion and we want to know the proportion of yellow candies in the population. The sampling distribution collects a large number of sample proportions. The mean of the proportions in the sampling distribution (expected value) equals the proportion of yellow candies in the population, because a sample proportion is an unbiased estimator of the population proportion.\nThings become a little confusing if we are interested in a sample mean, such as the average weight of candies in a sample bag. Now we have means at three levels: the population, the sampling distribution, and the sample.\n\n\n\n\n\n\n\nFigure 1.6: What is the relation between the three distributions?\n\n\n\nThe sampling distribution, here, is a distribution of sample means but the sampling distribution itself also has a mean, which is called the expected value or expectation of the sampling distribution. Don’t let this confuse you. The mean of the sampling distribution is the average of the average weight of candies in every possible sample bag. This mean of means has the same value as our first mean, namely the average weight of the candies in the population because a sample mean is an unbiased estimator of the population mean.\nRemember this: The population and the sample consist of the same type of observations. In the current example, we are dealing with a sample and a population of candies. In contrast, the sampling distribution is based on a different type of observation, namely samples, for example, sample bags of candies.\nThe sampling distribution is the crucial link between the sample and the population. On the one hand the sampling distribution is connected to the population because the population statistic (parameter), for example, average weight of all candies, is equal to the mean of the sampling distribution. On the other hand, it is linked to the sample because it tells us which sample means we will find with what probabilities. We need the sampling distribution to make statements about the population based on our sample.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "01-samplingdistr.html#take-home-points",
    "href": "01-samplingdistr.html#take-home-points",
    "title": "1  Sampling Distribution",
    "section": "1.5 Take-Home Points",
    "text": "1.5 Take-Home Points\n\nValues of a sample statistic vary across random samples from the same population. But some values are more probable than other values.\nThe sampling distribution of a sample statistic tells us the probability of drawing a sample with a particular value of the sample statistic or a particular minimum and/or maximum value.\nIf a sample statistic is an unbiased estimator of a parameter, the parameter value equals the average of the sampling distribution, which is called the expected value or expectation.\nFor discrete sample statistics, the sampling distribution tells us the probability of individual sample outcomes. For continuous sample statistics, it tells us the probability density, which gives us the probability of drawing a sample with an outcome that is at least or at most a particular value, or an outcome that is between two values.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "02-probability.html",
    "href": "02-probability.html",
    "title": "2  Probability Models",
    "section": "",
    "text": "Summary\nWatch the micro lecture Video 2.1 on probability models for an overview of the chapter.\nIn the previous chapter, we drew a large number of samples from a population to obtain the sampling distribution of a sample statistic, for instance, the proportion of yellow candies or average candy weight in the sample. The procedure is quite simple: Draw a sample, calculate the desired sample statistic, add the sample statistic value to the sampling distribution, and repeat this thousands of times.\nAlthough this procedure is simple, it is not practical. In a research project, we would have to draw thousands of samples and administer a survey to each sample or collect data on the sample in some other way. This requires too much time and money to be of any practical value. So how do we create a sampling distribution, if we only collect data for a single sample? This chapter presents three ways of doing this: exact approaches, theoretical approximations, and bootstrapping.\nAfter studying this chapter, you should know the limitations of the three methods of creating a sampling distribution, when to use which method, and how to check the conditions for using a method.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#exact-approaches-to-the-sampling-distribution",
    "href": "02-probability.html#exact-approaches-to-the-sampling-distribution",
    "title": "2  Probability Models",
    "section": "2.1 Exact Approaches to the Sampling Distribution",
    "text": "2.1 Exact Approaches to the Sampling Distribution\nThe first approach we will discuss to constructing a sampling distribution has implicitly been demonstrated in the section on probability distributions (Section 1.2.3). In this section, we calculated the true sampling distribution of the proportion of yellow candies in a sample from the probabilities of the colours. If we know or think we know the proportion of yellow candies in the population, we can exactly calculate the probability that a sample of ten candies includes one, two, three, or ten yellow candies. See the section on discrete random variables for details (Section 1.2).\n\n\n\n\nTable 2.1: Number of heads for a toss of three coins.\n\n\n\n\n\n\nOutcome\nCombination\nProbability: Combination\nProbability: Outcome\n\n\n\n\n0\ntail-tail-tail\n1/2 * 1/2 * 1/2 = 1/8 = .125\n1/8 = .125\n\n\n1\ntail-tail-head\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n1\nhead-tail-tail\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n1\ntail-head-tail\n1/2 * 1/2 * 1/2 = 1/8 = .125\n3/8 = .375\n\n\n2\nhead-head-tail\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n2\nhead-tail-head\n1/2 * 1/2 * 1/2 = 1/8 = .125\n\n\n\n2\ntail-head-head\n1/2 * 1/2 * 1/2 = 1/8 = .125\n3/8 = .375\n\n\n3\nhead-head-head\n1/2 * 1/2 * 1/2 = 1/8 = .125\n1/8 = .125\n\n\nTotal\n8\n\n1.000\n\n\n\n\n\n\n\n\n\nExamine Table 2.1, and try to determine why the probability of throwing 2 times head out of 3 is .375.\n\nThe calculated probabilities of all possible sample statistic outcomes give us an exact approach to the sampling distribution. Note that I use the word approach instead of approximation here because the obtained sampling distribution is no longer an approximation, that is, more or less similar to the true sampling distribution. No, it is the true sampling distribution itself.\n\n2.1.1 Exact approaches for categorical data\nAn exact approach lists and counts all possible combinations. This can only be done if we work with discrete or categorical variables. For an unlimited number of categories (continues variables), we cannot list all possible combinations.\nA proportion is based on frequencies and frequencies are discrete (integer values), so we can use an exact approach to create a sampling distribution for one proportion such as the proportion of yellow candies in the example above. The exact approach uses the binomial probability formula to calculate probabilities. Consult the internet if you want to know this formula; we are not going to use it in this course.\nExact approaches are also available for the association between two categorical (nominal or ordinal) variables in a contingency table: Do some combinations of values for the two variables occur relatively frequently? For example, are yellow candies more often sticky than red candies? If candies are either sticky or not sticky and they have one out of a limited set of colours, we have two categorical variables. We can create an exact probability distribution for the combination of colour and stickiness. The Fisher-exact test is an example of an exact approach to the sampling distribution of the association between two categorical variables.\n\n\n2.1.2 Computer-intensive\nThe exact approach can be applied to discrete variables because they have a limited number of values. Discrete variables are usually measured at the nominal or ordinal level. If the number of categories becomes large, a lot of computing time can be needed to calculate the probabilities of all possible sample statistic outcomes. Exact approaches are said to be computer-intensive.\nIt is usually wise to set a limit to the time you allow your computer to work on an exact sampling distribution because otherwise the problem may keep your computer occupied for hours or days. In this course, we will set the time limit in SPSS on 5 minutes (Section 2.2)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-SPSS-exact",
    "href": "02-probability.html#sec-SPSS-exact",
    "title": "2  Probability Models",
    "section": "2.2 Exact Approaches in SPSS",
    "text": "2.2 Exact Approaches in SPSS\n\n2.2.1 Introduction\nTo perform an exact approach in SPSS, let us set an example for a research. In a preceding paragraph we suggested the question are yellow candies more often sticky than red candies? If we want to investigate this, we have two variables. The first variable is candy colour (yellow versus red) and the second variable is stickiness (sticky versus not sticky). These two variables are both categorical (with two categories).\nThus, we can create an exact probability distribution for the combination of colour and stickiness.\nIf SPSS offers an exact approach of the sampling distribution, the test dialog window contains an Exact button. We can find this in the dialog window for creating a contingency table (Analyze &gt; descriptive statistics &gt; crosstab) and in several legacy dialogs for non-parametric tests (Analyze &gt; nonparametric tests &gt; legacy diologs). In the Exact dialog, we check the Exact option and SPSS automatically sets an upper limit of five minutes to the execution of the command. We can leave this at five minutes (see Section 2.1.2 for why we have to do this).\nAs established, we have two categorical variables with two categories. Looking at the test selection table (Section 4.4), we know that we use a Chi Square test to investigate these variables. Thus, within the dialog window crosstabs we open the Statistics dialog to select Chi-sqaure and we open the Exact dialog to select the option Exact.\nNote. A Fisher’s exact test is automatically run when there is a 2x2 contingency table. When we perform this test on larger contingency tables we need to select the Exact option, which is why we teach it to you here.\n\n\n2.2.2 Instructions\nIn the video below you can see how to perform an Exact test in SPSS.\nLet’s say you are having some friends over and put some candy out on the table. During the night you and your friends notice some candies being way stickier than other candies. One of your friends makes the observations that it seems to differ by candy colour. You are sceptical and want to investigate this.\nYou might recognize that this example, and the video below, shows the example of candy colour and candy stickiness as described in Section 2.2.1. Thus, we still have two categorical variables which both have two categories. Hence, we have a 2x2 contingency table and perform an Exact test. Three ways are shown.\nFirst we show you the approach via the analyze &gt; nonparametric tests &gt; legacy dialogs &gt; chi-square test, where you only have to select the Exact option and the time limit of five minutes.\nThen we show how how to do an Exact test while testing a proportion. This is done via analyze &gt; nonparametric tests &gt; legacy dialogs &gt; binomial test. Again, you select the Exact option with a five minute time limit.\nLastly, the crosstabs option is shown. This is the method you will probably use the most. Here we select the Exact option with the five minute time limit, but we also select Chi-Square in the Statistics dialog. In the Statistics dialog we also select an effect size, you usually use four. For at least one nominal variable in your reserach, we use Cramer’s V for a symmetrical association and Lambda for an asymmetrical association. For all ordinal variables in your research, we use Gamma (van Goodman en Kruskal) for a symmetrical association and Somer’s d for an asymmetrical association. Thus, for this example (two nominal variables and a symmetrical association) we use Phi and Cramer's V. Under the Cells... dialog, we select the obersved counts and the column percentages. Then you can run the analysis, do not forget to click paste instead of OK as we want to work with a syntax. With a syntax you can always check your work.\nWatch the video for the step by step on how to perform the exact approach in SPSS, the details and some additional information.\n\n\n\n\n\n\nVideo 2.2: Perform an exact test in SPSS.\n\n\n\nIn the video below, we will interpret the results of an Exact test in SPSS.\nThe video shows you the different tables that yield from the analysis. Output from an analysis often starts with a type of summary. Here you can check things similar to descriptive analyses (e.g., case numbers, means, etc.). In the case of an exact test, we see the numbers of cases in the first table case processing summary.\nThe second table shows us the crosstabulation with the added percentages for all columns. The percentages alone can already give you a direction to where your results are headed. These numbers can really help with interpreting your results later on.\nIn the third table, chi square tests, we can see our test statistics and values like the p-value. In the third row you can find the Fisher’s Exact Test. This row is the row of results we will report for this test. The p-value tells us whether an effect is significant or not based on this sample. We use the criterium of a p-value below .05 to be significant. There is more to this, but we will elaborate in Chapter 4. If you find a significant results, like shown in the video, the percentages (in the crosstabulation or second table) can help you interpret which groups are differing from each other and in which way. It can help you make sense of the association you have just tested.\nThe fourth table, symmetric measures, shows us the effect size. In this case we conclude there is a moderate association between candy stickiness and colour.\nIn other words, the significant result in this video combined with the percentages can tell us that according to this data it seems that blue, green and orange candies tend te be stickier than red and yellow candies. Tell that to your friend who is hosting the next birthday party!\nWatch the video for the step by step on how to interpret the exact approach in SPSS, the details and some additional information.\n\n\n\n\n\n\nVideo 2.3: Interpret exact test results in SPSS.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-theoretical-approx",
    "href": "02-probability.html#sec-theoretical-approx",
    "title": "2  Probability Models",
    "section": "2.3 Theoretical Approximations of the Sampling Distribution",
    "text": "2.3 Theoretical Approximations of the Sampling Distribution\nYou have learned that the exact approach requires a lot of computer power, which is not such a practical method in the not so very distant pre-computer age. In those days, mathematicians and statisticians discovered that many sampling distributions look a lot like known mathematical functions. For example, the sampling distribution of the sample mean can be quite similar to the well-known bell-shape of the normal distribution or the closely related (Student) t distribution. The mathematical functions are called theoretical probability distributions. Most statistical tests use a theoretical probability distribution as approximation of the sampling distribution.\n\n\n\n\n\n\n\nFigure 2.1: Normal curve as theoretical approximation of a sampling distribution.\n\n\n\nThe normal distribution is a mathematical function linking continuous scores, e.g., a sample statistic such as the average weight in the sample, to right-hand and left-hand probabilities, that is, to the probability of finding at least, or at most, this score. Such a function is called a probability density function (Section 1.3).\nWe like to use a theoretical probability distribution as an approximation of the sampling distribution because it is convenient. A computer can calculate probabilities from the mathematical function very quickly. We also like theoretical probability distributions because they usually offer plausible arguments about chance and probabilities.\n\n2.3.1 Reasons for a bell-shaped probability distribution\nThe bell shape of the normal distribution makes sense. Our sample of candies is just as likely to be too heavy, as it is too light, so the sampling distribution of the sample mean should be symmetrical. A normal distribution is symmetrical.\nIn addition, it is more likely that our sample bag has an average weight that is near the true average candy weight in the population than an average weight that is much larger or much smaller than the true average. Bags with on average extremely heavy or extremely light candies may occur, but they are extremely rare (we are very lucky or very unlucky). From these intuitions we would expect a bell shape for the sampling distribution.\nFrom this argumentation, we conclude that the normal distribution is a reasonable model for the probability distribution of sample means. Actually, it has been proven that the normal distribution exactly represents the sampling distribution in particular cases, for instance the sampling distribution of the mean of a very large sample.\n\n\n2.3.2 Conditions for the use of theoretical probability distributions\nTheoretical probability distributions, then, are plausible models for sampling distributions. They are known or likely to have the same shape as the true sampling distributions under particular circumstances or conditions.\nIf we use a theoretical probability distribution, we must assume that the conditions for its use are met. We have to check the conditions and decide whether they are close enough to the ideal conditions. Close enough is of course a matter of judgement. In practice, rules of thumb have been developed to decide if the theoretical probability distribution can be used.\nFigure 2.2 shows an example in which the normal distribution is a good approximation for the sampling distribution of a proportion in some situations, but not in all situations.\n\n\n\n\n\n\n\nFigure 2.2: How does the shape of the sampling distribution of sample proportions change with sample size and proportion value?\n\n\n\nDo theoretical probability distributions fit the true sampling distribution? As you may have noticed while playing with Figure 2.2, this is not always the case. In general, theoretical probability distributions fit sampling distributions better if the sample is larger. In addition, the population value may be relevant to the fit of the theoretical probability distribution. The sampling distribution of a sample proportion is more symmetrical, like the normal distribution, if the proportion in the population is closer to .5.\nThis illustrates that we often have several conditions for a theoretical probability distribution to fit the sampling distribution. We should evaluate all of them at the same time. In the example of proportions, a large sample is less important if the true proportion is closer to .5 but it is more important for true proportions that are more distant from .5.\nThe rule of thumb for using the normal distribution as the sampling distribution of a sample proportion combines the two aspects by multiplying them and requiring the resulting product to be larger than five. If the probability of drawing a yellow candy is .2 and our sample size is 30, the product is .2 * 30 = 6, which is larger than five. So we may use the normal distribution as approximation of the sampling distribution.\nNote that this rule of thumb uses one minus the probability, if the probability is larger than .5. In other words, it uses the smaller of two probabilities: the probability that an observation has the characteristic and the probability that it has not. For example, if we want to test the probability of drawing a candy that is not yellow, the probability is .8 and we use 1 - 0.8 = 0.2, which is then multiplied by the sample size.\nApart from the normal distribution, there are several other theoretical probability distributions. We have the binomial distribution for a proportion, the t distribution for one or two sample means, regression coefficients, and correlation coefficients, the F distribution for comparison of variances and comparing means for three or more groups (analysis of variance, ANOVA), and the chi-squared distribution for frequency tables and contingency tables.\nFor most of these theoretical probability distributions, sample size is important. The larger the sample, the better. There are additional conditions that must be satisfied such as the distribution of the variable in the population. The rules of thumb are summarized in Table 2.2. Bootstrapping and exact tests can be used if conditions for theoretical probability distributions have not been met. Special conditions apply to regression analysis (see Chapter 6, Section 6.1.5).\n\n\n\n\nTable 2.2: ules of thumb for using theoretical probability distributions.\n\n\n\n\n\n\nDistribution\nSample statistic\nMinimum sample size\nOther requirements\n\n\n\n\nBinomial distribution\nproportion\n-\n-\n\n\n(Standard) normal distribution\nproportion\ntimes test proportion (&lt;= .5) &gt;= 5\n-\n\n\n(Standard) normal distribution\none or two means\n&gt; 100\nOR variable is normally distributed in the population and population standard deviation is known (for each group)\n\n\nt distribution\none or two means\neach group &gt; 30\nOR variable is normally distributed in each group's population\n\n\nt distribution\n(Pearson) correlation coefficient\n-\nvariables are normally distributed in the population\n\n\nt distribution\n(Spearman) rank correlation coefficient\n&gt; 30\n-\n\n\nt distribution\nregression coefficient\n20+ per independent variable\nSee Chapter 8.\n\n\nF distribution\n3+ means\nall groups are more or less of equal size\nOR all groups have the same population variance\n\n\nF distribution\ntwo variances\n-\nno conditions for Levene's F test\n\n\nchi-squared distribution\nrow or cell frequencies\nexpected frequency &gt;= 1 and 80% &gt;= 5\ncontingency table: 3+ rows or 3+ columns\n\n\n\n\n\n\n\n\nTable 2.2 shows the conditions that must be satisfied if we want to use a theoretical probability distribution to approximate a sampling distribution. Only if the conditions are met, the theoretical probability distribution resembles the sampling distribution sufficiently.\nIn Table 2.2 the minimal required sample sizes for using theoretical approximation of sampling distributions are presented. If you plan to do a t test, each group should contain more than thirty cases. So if you intend to apply t tests, recruit more than thirty participants for each experimental group or more than thirty respondents for each group in your survey. If you expect non-response, that is, sampled participants or respondents unwilling to participate in your research, you should recruit more participants or respondents to have more than thirty observations in the end.\nChi-squared tests require a minimum of five expected frequencies per category in a frequency distribution or cell in a contingency table. Your sample size should be at least the number of categories or cells times five to come even near this requirement. Regression analysis requires at least 20 cases per independent variable in the regression model.\nThe variation of sample size across groups is important in analysis of variance (ANOVA), which uses the F distribution. If the number of cases is more or less the same across all groups, we do not have to worry about the variances of the dependent variable for the groups in the population. To be on the safe side, then, it is recommended to design your sampling strategy in such a way that you end up with more or less equal group sizes if you plan to use analysis of variance.\n\n\n2.3.3 Checking conditions\nRules of thumb about sample size are easy to check once we have collected our sample. By contrast, rules of thumb that concern the scores in the population cannot be easily checked, because we do not have information on the population. If we already know what we want to know about the population, why would we draw a sample and do the research in the first place?\nWe can only use the data in our sample to make an educated guess about the distribution of a variable in the population. For example, if the scores in our sample are clearly normally distributed, it is plausible that the scores in the population are normally distributed.\nIn this situation, we do not know that the population distribution is normal but we assume it is. If the sample distribution is clearly not normally distributed, we had better not assume that the population is normally distributed. In short, we sometimes have to make assumptions when we decide on using a theoretical probability distribution.\nWe could use a histogram of the scores in our sample with a normal distribution curve added to evaluate whether a normal distribution applies. Sometimes, we have statistical tests to draw inferences about the population from a sample that we can use to check the conditions. We discuss these tests in a later chapter.\n\n\n2.3.4 More complicated sample statistics: differences\nUp to this point, we have focused on rather simple sample statistics such as the proportion of yellow candies or the average weight of candies in a sample. Table 2.2, however, contains more complicated sample statistics.\nIf we compare two groups, for instance, the average weight of yellow and red candies, the sample statistic for which we want to have a sampling distribution must take into account both the average weight of yellow candies and the average weight of red candies. The sample statistic that we are interested in is the difference between the averages of the two samples.\n\n\n\n\n\n\n\nFigure 2.3: How do we obtain a sampling distribution for the mean difference of two independent samples?\n\n\n\nIf we draw a sample from both the red and yellow candies in the population, we may calculate the means for both samples and the difference between the two means. For example, the average weight of red candies in the sample bag is 2.76 grams and the average for yellow candies is 2.82 grams. For this pair of samples, the statistic of interest is 2.76 - 2.82 = -0.06, that is, the difference in average weight. If we repeat this many, many times and collect all differences between means in a distribution, we obtain the sampling distribution that we need.\nThe sampling distribution of the difference between two means is similar to a t-distribution, so we may use the latter to approximate the former. Of course, the conditions for using the t-distribution must be met.\nIt is important to note that we do not create separate sampling distributions for the average weight of yellow candies and for the average weight of red candies and then look at the difference between the two sampling distributions. Instead, we create one sampling distribution for the statistic of interest, namely the difference between means. We cannot combine different sampling distributions into a new sampling distribution. We will see the importance of this when we discuss mediation (Chapter 9).\n\n\n2.3.5 Independent samples\nIf we compare two means, there are two fundamentally different situations that are sometimes difficult to distinguish. When comparing the average weight of yellow candies to the average weight of red candies, we are comparing two samples that are statistically independent (see Figure 2.3), which means that we could have drawn the samples separately.\nIn principle, we could distinguish between a population of yellow candies and a population of red candies, and sample yellow candies from the first population and separately sample red candies from the other population. Whether we sampled the colours separately or not does not matter. The fact that we could have done so implies that the sample of red candies is not affected by the sample of yellow candies or the other way around. The samples are statistically independent.\nThis is important for the way in which probabilities are calculated. Just think of the simple example of flipping two coins. The probability of having heads twice in a row is .5 times .5, that is .25, if the coins are fair and the result of the second coin does not depend on the result of the first coin. The second flip is not affected by the first flip.\nImagine that a magnetic field is activated if the first coin lands with heads up and that this magnetic field increases the odds that the second coin will also be heads. Now, the second toss is not independent of the first toss and the probability of getting heads twice is larger than .25.\n\n\n2.3.6 Dependent samples\nThe example of a manipulated second toss is applicable to repeated measurements. If we want to know how quickly the yellow colour fades when yellow candies are exposed to sun light, we may draw a sample of yellow candies once and measure the colourfulness of each candy at least twice: at the start and end of some time interval. We compare the colourfulness of a candy at the second measurement to its colourfulness at the first measurement.\n\n\n\n\n\n\n\nFigure 2.4: Dependent samples.\n\n\n\nIn this example, we are comparing two means, just like the yellow versus red candy weight example, but now the samples for both measurements are the same. It is impossible to draw the sample for the second measurement independently from the sample for the first measurement if we want to compare repeated measurements. Here, the second sample is fixed once we have drawn the first sample. The samples are statistically dependent; they are paired samples.\nWith dependent samples, probabilities have to be calculated in a different way, so we need a special sampling distribution. In the interactive content above, you may have noticed a relatively simple solution for two repeated measurements. We just calculate the difference between the two measurements for each candy in the sample and use the mean of this new difference variable as the sample statistic that we are interested in. The t-distribution, again, offers a good approximation of the sampling distribution of dependent samples if the samples are not too small.\nFor other applications, the actual sampling distributions can become quite complicated but we do not have to worry about that. If we choose the right technique, our statistical software will take care of this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#spss-and-theoretical-approximation-of-the-sampling-distribution",
    "href": "02-probability.html#spss-and-theoretical-approximation-of-the-sampling-distribution",
    "title": "2  Probability Models",
    "section": "2.4 SPSS and Theoretical Approximation of the Sampling Distribution",
    "text": "2.4 SPSS and Theoretical Approximation of the Sampling Distribution\nBy default, SPSS uses a theoretical probability distribution to approximate the sampling distribution. It chooses the correct theoretical distribution but you yourself should check if the conditions for using this distribution are met. For example, is the sample large enough or is it plausible that the variable is normally distributed in the population?\nIn one case, SPSS automatically selects an exact approach if the conditions for a theoretical approximation are not met. If you apply a chi-squared test to a contingency table in SPSS, SPSS will automatically apply Fisher’s exact test if the table has two rows and two columns. In all other cases, you have to select bootstrapping or an exact approach yourself if the conditions for a theoretical approximation are not met.\nWe are not going to practice with theoretical approximations in SPSS, now. Because theoretical approximation is the default approach in SPSS, we will encounter it in the exercises in later chapters. The default approach in SPSS means that if we conduct an analysis in SPSS without changing the settings to include an exact approach or bootstrapping (which you will learn in the next paragraph), we are using a theoertical approximation on our data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-boot-approx",
    "href": "02-probability.html#sec-boot-approx",
    "title": "2  Probability Models",
    "section": "2.5 The Bootstrap Approximation of the Sampling Distribution",
    "text": "2.5 The Bootstrap Approximation of the Sampling Distribution\nThe third way to obtain a sampling distribution is still based on the idea of drawing a large number of samples. However, we only draw one sample from the population for which we collect data. As a next step, we draw a large number of samples from our initial sample. The samples drawn in the second step are called bootstrap samples. The technique was developed by Bradley Efron (1979; 1987). For each bootstrap sample, we calculate the sample statistic of interest and we collect these as our sampling distribution. We usually want about 5,000 bootstrap samples for our sampling distribution.\n\n\n\n\n\n\n\nFigure 2.5: How do we create a sampling distribution with bootstrapping?\n\n\n\nIn Figure 2.5, an initial sample (left panel) has been drawn from a population containing five candy colours in equal proportions.\n\n\n\nThe bootstrap concept refers to the story in which Baron von Münchhausen saves himself by pulling himself and his horse by his bootstraps (or hair) out of a swamp. In a similar miraculous way, bootstrap samples resemble the sampling distribution even though they are drawn from a sample instead of the population. This miracle requires some explanation and it does not work always, as we will discuss in the remainder of this section.\nPicture: Baron von Münchhausen pulls himself and his horse out of a swamp. Theodor Hosemann (1807-1875), public domain, via Wikimedia Commons\n\n\n\n2.5.1 Sampling with and without replacement\nAs we will see in Chapter 3, for example Section 3.3, the size of a sample is very important to the shape of the sampling distribution. The sampling distribution of samples with twenty-five cases can be very different from the sampling distribution of samples with fifty cases. To construct a sampling distribution from bootstrap samples, the bootstrap samples must be exactly as large as the original sample.\nHow can we draw many different bootstrap samples from the original sample if each bootstrap sample must contain the same number of cases as the original sample?\n\n\n\n\n\n\n\nFigure 2.6: Sampling with and without replacement.\n\n\n\nIf we allow every case in the original sample to be sampled only once, each bootstrap sample contains all cases of the original sample, so it is an exact copy of the original sample. Thus, we cannot create different bootstrap samples.\nBy the way, we often use the type of sampling described above, which is called sampling without replacement. If a person is (randomly) chosen for our sample, we do not put this person back into the population so she or he can be chosen again. We want our respondents to fill out our questionnaire only once or participate in our experiment only once.\nIf we do allow the same person to be chosen more than once, we sample with replacement. The same person can occur more than once in a sample. Bootstrap samples are sampled with replacement from the original sample, so one bootstrap sample may differ from another. Some cases in the original sample may not be sampled for a bootstrap sample while other cases are sampled several times. You probably have noticed this in Figure 2.6. Sampling with replacement allows us to obtain different bootstrap samples from the original sample, and still have bootstrap samples of the same size as the original sample.\nIn conclusion, we sample bootstrap samples in a different way (with replacement) than participants for our research (without replacement).\n\n\n2.5.2 Limitations to bootstrapping\nDoes the bootstrapped sampling distribution always reflect the true sampling distribution?\n\n\n\n\n\n\n\nFigure 2.7: How is bootstrapping influenced by sample size? In the population, twenty per cent of the candies are yellow.\n\n\n\nWe can create a sampling distribution by sampling from our original sample with replacement. It is hardly a miracle that we obtain different samples with different sample statistics if we sample with replacement. Much more miraculous, however, is that this bootstrap distribution resembles the true sampling distribution that we would get if we draw lots of samples directly from the population.\nDoes this miracle always happen? No. The original sample that we have drawn from the population must be more or less representative of the population. The variables of interest in the sample should be distributed more or less the same as in the population. If this is not the case, the sampling distribution may give a distorted view of the true sampling distribution. This is the main limitation to the bootstrap approach to sampling distributions.\nA sample is more likely to be representative of the population if the sample is drawn in a truly random fashion and if the sample is large. But we can never be sure. There always is a chance that we have drawn a sample that does not reflect the population well.\n\n\n2.5.3 Any sample statistic can be bootstrapped\nThe big advantage of the bootstrap approach (bootstrapping) is that we can get a sampling distribution for any sample statistic that we are interested in. Every statistic that we can calculate for our original sample can also be calculated for each bootstrap sample. The sampling distribution is just the collection of the sample statistics calculated for all bootstrap samples.\nBootstrapping is more or less the only way to get a sampling distribution for the sample median, for example, the median weight of candies in a sample bag. We may create sampling distributions for the wildest and weirdest sample statistics, for instance the difference between sample mean and sample median squared. I would not know why you would be interested in the squared difference of sample mean and median, but there are very interesting statistics that we can only get at through bootstrapping. A case in point is the strength of an indirect effect in a mediation model (Chapter 9).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#sec-boot-spss",
    "href": "02-probability.html#sec-boot-spss",
    "title": "2  Probability Models",
    "section": "2.6 Bootstrapping in SPSS",
    "text": "2.6 Bootstrapping in SPSS\n\n2.6.1 Instructions\nThe last approach we will show you in SPSS is bootstrapping. In the video below you will see several analyses to which we add bootstrapping.\nRemember your birthday party with a lot of candy that sparks new questions every couple of minutes? This time one friend is getting a bit nauseous from the candy and another friend is doing just fine. They conclude they both have been eating the same amount of candy but one has a preference for strawberry flavoured and the other for lemon flavoured. Could it be that the friend who is constantly eating red candies has been eating more cnady than the friend that has been eating yellow candies? According to your friends, that could only be true if the red candies weigh more than the yellow candies.\nIn this case, we are investigating whether yellow and red candies weigh the same amount. Thus, we have two variables: candy colour (red and yellow; two categories) and candy weight (continuous). As you can see in Section 4.4, we can investigate these variables with an independent t-test. This independent t-test is shown in the video.\nAs you might remember from the exact approach, this data set contains more candy colours than red and yellow. It is important that we define the groups so that this analysis only includes red and yellow candies. Otherwise we will not find an answer to the question we have proposed above.\nYou might also remember from the theoretical approach that there are certain conditions we need to meet in order to be able to execute an independent samples t-test. Let’s say your friends have been eating quite some candy and therefore we do not have 30 yellow and 30 red candies left. Consequently, we do not meet the conditions. This leads us to resort to bootstrapping to still be able to use the independent sample t-test. Via the bootstrap button, we can check perform bootstrapping. Usually, we use 5000 bootstrap samples and 95% bias corrected confidence intervals. Please note that bootstrapping is random and therefore bootstrap results can always differ a little bit from each other, you might see this when working together with a friend or checking your answers on the preparatory assignments. Numbers can differ a little bit, this will not happen when we use the same seed - but we do not use that option in this course.\nWatch the video for the step by step on how to perform an independent t-test with bootstrapping, the details and some additional information.\n\n\n\n\n\n\nVideo 2.4: Perform bootstrapping in SPSS.\n\n\n\nNow that we have run the analysis in SPSS, we want to show you how to interpret this results. And thus, how to answer the question whether yellow and red candies weigh the same amount.\nAs with the Fisher’s exact test, the output starts - after a table with some bootstrap specifications - with a summary type table including descriptive like results. The group statistics, here we can find the number of cases (candies), means, standard deviations, etc. The means can already give you a sense of and a grip on your results, which can help with interpretation later on.\nThe table below, independent samples test, shows us the results of the independent samples t-test, including Leven’s F-test, the test statistic, p-value and bootstrapped confidence interval. Levene’s F-test is above .05 which leads us to conclude that equal variances are assumed (we will talk more about this during ANOVA, in chapter 5). This means we look at the upper row for our results. The t-value has a corresponding p-value that is also above .05 which menas that the effect of candy colour on candy weight is not significant.\nThe second part of the independent samples test shows us the mean difference which is very small, this makes sense given the fact that we found a not significant result. The bootstrapped confidence intervals show that the zero falls in the 95% interval. This means that there is a likelihood that the mean difference could be zero. Thus, looking at the intervals we would also conclude that the effect is not significant.\nIn conclusion, the results show us that we could conclude that the hypothesis made by your friends gets no support from our data. Red and yellow candies seem to weigh approximately the same. We could have already seen this coming after looking at the mean values.\nWatch the video for the step by step on how to interpret an independent t-test with bootstrapping, the details and some additional information.\n\n\n\n\n\n\nVideo 2.5: Interpret bootstrap results in SPSS.\n\n\n\nIn principle, any sample statistic can be bootstrapped. SPSS, however, does not bootstrap sample statistics that we had better not use because they give bad (biased) results. For example, SPSS does not bootstrap the minimum value, maximum value or the range between minimum and maximum value of a variable.\nSPSS reports bootstrapping results as confidence intervals. We will discuss confidence intervals in detail in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#when-do-we-use-which-approach-to-the-sampling-distribution",
    "href": "02-probability.html#when-do-we-use-which-approach-to-the-sampling-distribution",
    "title": "2  Probability Models",
    "section": "2.7 When Do We Use Which Approach to the Sampling Distribution?",
    "text": "2.7 When Do We Use Which Approach to the Sampling Distribution?\n\n\n\n\n\nDiagram for selecting the type of sampling distribution.\n\n\n\n\nBy default, SPSS uses a theoretical approximation of the sampling distribution. Select the right test in SPSS and SPSS ensures that an appropriate theoretical probability distribution is used. You, however, must check whether the sample meets the conditions for using this theoretical probability distribution, see Table 2.2.\nIf the conditions for using a theoretical probability distribution are not met or if we do not have a theoretical approximation to the sampling distribution, we use bootstrapping or an exact approach. We can always use bootstrapping but an exact approach is available only if the variables are categorical. An exact approach is more accurate than bootstrapping and approximation with a theoretical probability distribution, for example, the chi-squared distribution, so we prefer the exact approach over bootstrapping if we are dealing with categorical variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "02-probability.html#take-home-points",
    "href": "02-probability.html#take-home-points",
    "title": "2  Probability Models",
    "section": "2.8 Take-Home Points",
    "text": "2.8 Take-Home Points\n\nWe may create an exact sampling distribution or simulate a bootstrap sampling distribution in simple situations or if we have a lot of computing power.\nFor a bootstrap sampling distribution, we need about 5,000 bootstrap samples from our original sample.\nAn exact sampling distribution can only be used with categorical variables.\nWe can often approximate the sampling distribution of a sample statistic with a known theoretical probability distribution.\nApproximations only work well under conditions, which we have to check.\nConditions usually involve the size of the sample, sample type (independent vs. dependent/paired), and the shape or variance of the population distribution.\nIf these conditions are not met or we do not have a theoretical approximation to the sampling distribution, we use bootstrapping or exact tests.\nSamples are independent if, in principle, we can draw a sample for one group without taking into account the sample for another group of cases. Otherwise, the samples are dependent or paired.\n\n\n\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the Jackknife.” Ann.Statist. 7 (1): 1–26.\n\n\nEfron, Bradley. 1987. “Better Bootstrap Confidence Intervals.” Journal of the American Statistical Association 82 (397): 171–85. https://doi.org/10.1080/01621459.1987.10478410.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "03-estimation.html",
    "href": "03-estimation.html",
    "title": "3  Estimating a Parameter",
    "section": "",
    "text": "Summary\nIn this chapter, we set out to make educated guesses of a population value (parameter, often called “the true value”) based on our sample. This type of guessing is called estimation. Our first guess will be a single value for the population value. We merely guess that the population value is equal to the value of the sample statistic. This guess is the most precise guess that we can make, but, most likely, it is wrong.\nOur second guess uses the sampling distribution to make a statement about the approximate population value. In essence, we calculate an interval that we are confident will contain the population value. We can increase our confidence by widening the interval, but this decreases the precision of our guess.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "03-estimation.html#point-estimate",
    "href": "03-estimation.html#point-estimate",
    "title": "3  Estimating a Parameter",
    "section": "3.1 Point Estimate",
    "text": "3.1 Point Estimate\nIf we have to name one value for the population value, our best guess is the value of the sample statistic. For example, if 18% of the candies in our sample bag are yellow, our best guess for the proportion of yellow candies in the population of all candies from which this bag was filled, is .18. What other number can we give if we only have our sample? This type of guess is called a point estimate and we use it a lot.\nThe sample statistic is the best estimate of the population value only if the sample statistic is an unbiased estimator of the population value. As we have learned in Section 1.2.5, the true population value is equal to the mean of the sampling distribution for an unbiased estimator. The mean of the sampling distribution is the expected value for the sample.\nIn other words, an unbiased estimator neither systematically overestimates the population value, nor does it systematically underestimate the population value. With an unbiased estimator, then, there is no reason to prefer a value higher or lower than the sample value as our estimate of the population value.\nEven though the value of the statistic in the sample is our best guess, it is very unlikely that our sample statistic is exactly equal to the population value (parameter). The recurrent theme in our discussion of random samples is that a random sample differs from the population because of chance during the sampling process. The precise population value is highly unlikely to actually appear in our sample.\nThe sample statistic value is our best point estimate but it is nearly certain to be wrong. It may be slightly or far off the mark but it will hardly ever be spot on. For this reason, it is better to estimate a range within which the population value falls. Let us turn to this in the next section.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "03-estimation.html#interval-estimate-for-the-sample-statistic",
    "href": "03-estimation.html#interval-estimate-for-the-sample-statistic",
    "title": "3  Estimating a Parameter",
    "section": "3.2 Interval Estimate for the Sample Statistic",
    "text": "3.2 Interval Estimate for the Sample Statistic\nThe sampling distribution of a continuous sample statistic tells us the probability of finding a range of scores for the sample statistic in a random sample. For example, the average weight of candies in a sample bag is a continuous random variable. The sampling distribution tells us the probability of drawing a sample with average candy weight between 2.0 and 3.6 grams. We can use this range as our interval estimate.\nNote that we are reasoning from sampling distribution to sample now. This is not what we want to do in actual research, where we want to reason from sample to sampling distribution to population. We get to that in Section 3.5. For now, assume that we know the true sampling distribution.\nRemember that the average or expected value of a sampling distribution is equal to the population value if the estimator is unbiased. For example, the mean weight of yellow candies averaged over a very large number of samples is equal to the mean weight of yellow candies in the population. For an interval estimate, we now select the sample statistic values that are closest to the average of the sampling distribution.\nBetween which boundaries do we find the sample statistic values that are closest to the population value? Of course, we have to specify what we mean by “closest”. Which part of all samples do we want to include? A popular proportion is 95%, so we want to know the boundary values that include 95% of all samples that are closest to the population value. For example, between which boundaries is average candy weight situated for 95% of all samples that are closest to the average candy weight in the population?\n\n\n\n\n\n\n\nFigure 3.1: Within which interval do we find the sample results that are closest to the population value?\n\n\n\nFigure 3.1 shows the sampling distribution of average sample candy weight.\nSay, for instance, that 95% of all possible samples in the middle of the sampling distribution have an average candy weight ranging from 1.6 to 4.0 grams. The proportion .95 can be interpreted as a probability. Our sampling distribution tells us that we have 95% probability that the average weight of yellow candies lies between 1.6 and 4.0 grams in a random sample that we draw from this population.\nWe now have boundary values, that is, a range of sample statistic values, and a probability of drawing a sample with a statistic falling within this range. The probability shows our confidence in the estimate. It is called the confidence level of an interval estimate.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "03-estimation.html#sec-precisionsesamplesize",
    "href": "03-estimation.html#sec-precisionsesamplesize",
    "title": "3  Estimating a Parameter",
    "section": "3.3 Precision, Standard Error, and Sample Size",
    "text": "3.3 Precision, Standard Error, and Sample Size\nThe width of the estimated interval represents the precision of our estimate. The wider the interval, the less precise our estimate. With a less precise interval estimate, we will have to take into account a wider variety of outcomes in our sample.\n\n\n\n\n\n\n\nFigure 3.2: How does the confidence level affect the precision of an interval estimate?\n\n\n\nIf we want to predict something, we value precision. We would rather conclude that the average weight of candies in the next sample we draw is between 2.0 and 3.6 grams than between 1.6 and 4.0 grams. If we would be satisfied with a very imprecise estimate, we need not do any research at all. With relatively little knowledge about the candies that we are investigating, we could straightaway predict that the average candy weight is between zero and ten grams. The goal of our research is to find a more precise estimate.\nThere are several ways to increase the precision of our interval estimate, that is, to obtain a narrower interval for our estimate. The easiest and least useful way is to decrease our confidence that our estimate is correct. If we lower the confidence that we are right, we can discard a large number of other possible sample statistic outcomes and focus on a narrower range of sample outcomes around the true population value.\nThis method is not useful because we sacrifice our confidence that the range includes the outcome in the sample that we are going to draw. What is the use of a more precise estimate if we are less certain that it predicts correctly? Therefore, we usually do not change the confidence level and leave it at 95% or thereabouts (90%, 99%). It is important to be quite sure that our prediction will be right.\n\n3.3.1 Sample sizes\nA less practical but very useful method of narrowing the interval estimate is increasing sample size. If we buy a larger bag containing more candies, we get a better idea of average candy weight in the population and a better idea of the averages that we should expect in our sample.\n\n\n\n\n\n\n\nFigure 3.3: How does sample size affect the precision of an interval estimate?\n\n\n\nFigure 3.3 shows a sampling distribution of average candy weight in candy sample bags. The size of the horizontal arrow represents the precision of the interval estimate: the shorter the arrow, the more precise the interval estimate.\nAs you may have noticed while playing with Figure 3.3, a larger sample yields a narrower, that is, more precise interval. You may have expected intuitively that larger samples give more precise estimates because they offer more information. This intuition is correct.\nIn a larger sample, an observation above the mean is more likely to be compensated by an observation below the mean. Just because there are more observations, it is less likely that we sample relatively high scores but no or considerably fewer scores that are relatively low.\nThe larger the sample, the more the distribution of scores for a variable in the sample will resemble the distribution of scores for this variable in the population. As a consequence, a sample statistic value will be closer to the population value for this statistic.\nLarger samples resemble the population more closely, and therefore large samples drawn from the same population are more similar. The result is that the sample statistic values in the sampling distribution are less varied and more similar. They are more concentrated around the true population value. The middle 95% of all sample statistic values are closer to the centre, so the sampling distribution is more peaked.\n\n\n3.3.2 Standard error\nThe concentration of sample statistic values, such as average candy weight in a sample bag, around the centre (mean) of the sampling distribution is expressed by the standard deviation of the sampling distribution. Up until now, we have only paid attention to the centre of the sampling distribution, its mean, because it is the expected value in a sample and it is equal to the population value if the estimator is unbiased.\nNow, we start looking at the standard deviation of the sampling distribution as well, because it tells us how precise our interval estimate is going to be. The sampling distribution’s standard deviation is so important that it has received a special name: the standard error.\n\n\n\n\n\n\n\nFigure 3.4: How does sample size affect the standard error?\n\n\n\nThe word error reminds us that the standard error represents the size of the error that we are likely to make (on average under many repetitions) if we use the value of the sample statistic as a point estimate for the population value.\nLet us assume, for instance, that the standard error of the average weight of candies in samples is 0.6. Loosely stated, this means that the average difference between true average candy weight and average candy weight in a sample is 0.6 if we draw very many samples from the same population.\nThe smaller the standard error, the more the sample statistic values resemble the true population value, and the more precise our interval estimate is with a given confidence level, for instance, 95%. Because we like more precise interval estimates, we prefer small standard errors over high standard errors.\nIt is easy to obtain smaller standard errors: just increase sample size. See Figure 3.3, where larger samples yield more peaked sampling distributions. In a peaked distribution, values are closer to the mean and the standard error is smaller. In our example, average candy weights in larger sample bags are closer to the average candy weight in the population.\nIn practice, however, it is both time-consuming and expensive to draw a very large sample. Usually, we want to settle on the optimal size of the sample, namely a sample that is large enough to have interval estimates at the confidence level and precision that we need but as small as possible to save on time and expenses. We return to this matter in Section 4.2.3.\nThe standard error may also depend on other factors, such as the variation in population scores. In our example, more variation in the weight of candies in the population produces a larger standard error for average candy weight in a sample bag. If there are more very heavy candies and very light candies, it is easier to draw a sample with several heavy candies or with several very light candies. Average weight in these sample bags will be too high or too low. We cannot influence the variation in candy weights in the population, so let us ignore this factor influencing the standard error.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "03-estimation.html#sec-crit-values",
    "href": "03-estimation.html#sec-crit-values",
    "title": "3  Estimating a Parameter",
    "section": "3.4 Critical Values",
    "text": "3.4 Critical Values\nIn the preceding section, we learned that the standard error is related to the precision of the interval estimate. A larger standard error yields a less precise estimate, that is, with a wider interval estimate.\nWe are interested in the interval that includes a particular percentage of all samples that can be drawn, usually the 95% of all samples that are closest to the population value. In our current example, the 95% of all samples with average candy weight that is closest to average candy weight in the population (2.8 grams).\nIn theoretical probability distributions like the normal distribution, the percentage of samples is related to the standard error. If we know the standard error, we know the interval within which we find the 95% of samples that are closest to the population value.\n\n\n\n\n\n\n\nFigure 3.5: Standardized sample outcomes and the standard error.\n\n\n\nFigure 3.5 shows the sampling distribution of average candy weight per sample bag. It contains two horizontal axes, one with average candy weight in grams (bottom) and one with average candy weight in standard errors, also called z scores (top).\nIn Figure 3.5, we approximate the sampling distribution with a theoretical probability distribution, namely the normal distribution. The theoretical probability distribution links probabilities (areas under the curve) to sample statistic outcome values (scores on the horizontal axis). For example, we have 2.5% probability of drawing a sample bag with average candy weight below 1.2 grams or 2.5% probability of drawing a sample bag with average candy weight over 4.4 grams.\n\n3.4.1 Standardization and z scores\nThe average candy weights that are associated with 2.5% and 97.5% probabilities in Figure 3.5 depend on the sample that we have drawn. As you may notice while playing with Figure 3.3, changing the size of the sample also changes the average candy weights that mark the 2.5% and 97.5% probabilities.\nWe can simplify the situation if we standardize the sampling distribution: Subtract the mean of the sampling distribution from each sample mean in this distribution, and divide the result by the standard error. Thus, we transform the sampling distribution into a distribution of standardized scores. The mean of the new standardized variable is always zero.\nIf we use the normal distribution for standardized scores, which is called the standard-normal distribution or z distribution, there is a single z value that marks the boundary between the top 2.5% and the bottom 97.5% of any sample. This z value is 1.96. If we combine this value with -1.96, separating the bottom 2.5% of all samples from the rest, we obtain an interval [-1.96, 1.96] containing 95% of all samples that are closest to the mean of the sampling distribution.\nIn a standard-normal or z distribution, 1.96 is called a critical value. Together with its negative (-1.96), it separates the 95% sample statistic outcomes that are closest to the parameter, hence that are most likely to appear, from the 5% that are furthest away and least likely to appear. There are also critical z values for other probabilities, for instance, 1.64 for the middle 90% of all samples and 2.58 for the middle 99% in a standard-normal distribution.\n\n\n3.4.2 Interval estimates from critical values and standard errors\nCritical values in a theoretical probability distribution tell us the boundaries, or range, of the interval estimate expressed in standard errors. In a normal distribution, 95% of all sample means are situated no more than 1.96 standard errors from the population mean.\nIf the standard error is 0.5 and the population mean is 2.8 grams, we have 95% probability that the mean candy weight in a sample that we draw from this population lies between 1.82 grams (this is 1.96 times 0.5 subtracted from 2.8) and 3.78 grams.\nCritical values make it easy to calculate an interval estimate if we know the standard error. Just take the population value and add the critical value times the standard error to obtain the upper limit of the interval estimate. Subtract the critical value times the standard error from the population value to obtain the lower limit.\n\n\n\n\n\n\n\nLower limit of the interval estimate = population value – critical value * standard error.\nUpper limit of the interval estimate = population value + critical value * standard error.\n\n\n\n\n(Standard) normal distributions make life easier for us, because there is a fixed critical value for each probability, such as 1.96 for 95% probability, which is well-worth memorizing.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "03-estimation.html#sec-ci-parameter",
    "href": "03-estimation.html#sec-ci-parameter",
    "title": "3  Estimating a Parameter",
    "section": "3.5 Confidence Interval for a Parameter",
    "text": "3.5 Confidence Interval for a Parameter\nWorking through the preceding sections, you may have realized that estimating the value of a statistic in a new sample with a specific precision and probability is not our ultimate goal, as it does not fully represent reality. In reality, we do not know the population parameter, and the primary objective of statistics is to estimate this unknown population parameter.\nFor example, we don’t care much about the average weight of candies in our sample bag or in the next sample bag that we may buy. We want to say something about the average weight of candies in the population. How can we do this?\nIn addition, you may have realized that, if we want to construct the sampling distribution of sample means, we first need to know the precise population value, for instance, average candy weight in the population. After all, the average of the sampling distribution is equal to the population mean for an unbiased estimator. In the preceding paragraphs, we acted as if we knew the sampling distribution of sample means.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.6: Probabilities of a sample with a particular number of yellow candies if 20 per cent of the candies are yellow in the population.\n\n\n\nIn the exact approach to the sampling distribution of the proportion of yellow candies in a sample bag (Figure 3.6), for instance, we first need to know the proportion of yellow candies in the population. If we know the population proportion, we can calculate the exact probability of getting a sample bag with a particular proportion of yellow candies. But we don’t know the population proportion of yellow candies; we want to estimate it.\nIn the candy weight example, we first need to know the mean of population candy weight, then we can construct a theoretical probability distribution of sample means. But we do not know the population mean; we want to estimate it.\nA theoretical probability distribution can only be used as an approximation of a sampling distribution if we know some characteristics of the population. We know that the sampling distribution of sample means always has the bell shape of a normal distribution or t distribution. However, knowing the shape is not sufficient for using the theoretical distribution as an approximation of the sampling distribution.\nWe must also know the population mean because it specifies where the centre of the sampling distribution is located. So, we must know the population mean to use a theoretical probability distribution to estimate the population mean.\nBy the way, we also need the standard error to know how peaked or flat the bell shape is. The standard error can usually be estimated from the data in our sample. But let us not worry about how the standard error is being estimated and focus on estimating the population mean.\n\n3.5.1 Reverse reasoning from one sample mean\nIn the previous chapters, we were reasoning from population mean to sampling distribution of sample means, then to a single sample mean. Based on the known population mean and standard error, we could draw an interval estimate around the population mean of the sampling distribution (Figure 3.5). 95% of all sample means would fall within this interval estimate around the known population mean.\nIn practice though, the population mean is unknown. We only have the sample mean derived from our collected data. Using this sample mean, we would like to estimate the population mean.\n\n\n\n\n\n\n\nFigure 3.7: Confidence intervals from replications\n\n\n\nInstead of checking whether a sample mean is inside or outside of the interval estimate of the population mean (Section 3.4), we use the interval estimate around that one sample mean, to check whether this interval catches the population mean. These two ways are equivalent, but the second way does not require us to know the population mean. This is because the interval estimate around the sample means catches the population mean in 95% of the times, regardless if the population mean is known or not. We call such an interval estimate around the sample mean a 95% confidence interval.\nThe middle plot of Figure 3.7, shows the 95% confidence interval for a single sample, from the population distribution in the top graph. The green line indicates that the interval estimate around the sample mean catches the population mean. The average candy weight in this samples of \\(N=30\\) is 2.95 grams and the lower and upper boundary for 95% confidence interval are 2.59 grams and 3.32 grams. We use the blue vertical dashed line to indicate the population mean, which in reality we do not know. Though, in this simulation, we can see that the green 95% confidence interval catches the population mean.\nNow, increase the number of times of samples (replications) by adjusting the slider in Figure 3.7. The middle plot now shows how many of the samples with a 95% confidence intervals catch the population mean, indicated by the green lines. The red lines indicate that the 95% confidence interval does not catch the population mean. The percentage that catch the population mean approaches 95% as the number of samples gets higher.\nNow, also increase the sample size in Figure 3.7 by moving the slider. We see that, in the middle plot, all confidence intervals become narrower. We therefore are much more confident in our estimation of the population mean with a larger sample size.\nNote that the confidence intervals are not of the same with, and that the upper and lower bound for each sample is different. This is because the sample mean is different for each sample, and the standard error is calculated from the sample. It is therefore incorrect to say that you are 95% confident that the population mean is within the specific lower and upper bound of your sample. Instead, you are 95% confident that the interval estimate around the sample mean catches the population mean. This is a subtle difference, but indicates that only repeated sampling is the rationale for the confidence that the population mean is within the interval estimate.\n\n\n\n\n\n\n\nIf we were to repeat the experiment over and over, then 95% of the time the confidence intervals contain the true mean.\n— Hoekstra et al. (2014)\n\nIt is very important that we understand that the confidence level 95% is NOT the probability that the population parameter has a particular value, or that it falls within the interval.\nIn classic statistics (so called “Frequentists”), the population parameter is not a random variable but a fixed, unknown number, which does not have a probability.\nA confidence interval around a sample statistics from solely one data collection either catches (100%) or does not catch (0%) the population parameter. We just do not know which one is the case. That’s why we cannot make any conclusion about the population mean based on one confidence interval.\nOnly when we replicate data collection many many times, we do know that about 95% of all 95% confidence intervals will catch the population mean.\n\n\n\nNow imagine that you have a large sample for your research project. Looking at Figure 3.7, this would mean that if you would run the same research a hundred times, you would find the population mean within the 95% confidence interval in 95 of these hundred times. That is very reasuring, isn’t it?\nIn the bottom plot of Figure 3.7, we revisit Chapter 1, to illustrate how these 95% confidence intervals around the sample means are related to the sampling distribution. Each confidence interval in the middle plot is an approximation of the width of the sampling distribution in the bottom plot. The larger the samples size, the narrower the confidence intervals are, and the narrower the sampling distribution becomes in the bottom plot. The histogram represents the sample means from the number replications. Each sample mean comes from one replication.\nWe also see the theoretical approximation of this sampling distribution (as was discussed in Section 2.3) of sample means, which is a normal distribution. As the number of replications and sample size increases, the shape of the histogram gets closer to the shape of the theoretical approximation of the sampling distribution (green line), which in turn also gets narrower.\nThis sampling distribution is theoretically approximated by a normal distribution whose mean is the population mean and standard deviation is the standard error of the sample. To make our life easier, we can convert the sampling distribution, which is a normal distribution, to the standard normal distribution by converting to a z-score. The critical z value 1.96 and -1.96 together marks the upper and lower limit of the interval containing 95% of all samples with means closest to the population mean.\nAs a consequence, we are able to calculate 95% confidence interval around a sample mean by adding and subtracting 1.96 standard errors from that sample mean.\n\n\n\n\n\n\n\nConfidence interval lower limit = sample value – critical value * standard error.\nConfidence interval upper limit = sample value + critical value * standard error.\n\nFor example, the 95%-confidence interval for a sample mean:\n\nLower limit = sample mean - 1.96 * standard error.\nUpper limit = sample mean + 1.96 * standard error.\n\n\n\n\nHaven’t we seen this calculation before? Yes we did, in Section 3.4.2, where we estimated the interval around population mean for sample means. We now simply reverse the application, using the interval of sample mean to estimate the population mean instead of the other way around.\n\n\n\nJerzy Neyman introduced the concept of a confidence interval in 1937:\n“In what follows, we shall consider in full detail the problem of estimation by interval. We shall show that it can be solved entirely on the ground of the theory of probability as adopted in this paper, without appealing to any new principles or measures of uncertainty in our judgements”. (Neyman 1937: 347)\nPhoto of Jerzy Neyman by Ohonik, Commons Wikimedia, CC BY-SA 4.0]\n\n\n\n\n3.5.2 Confidence intervals with bootstrapping\nIf we approximate the sampling distribution with a theoretical probability distribution such as the normal (z) or t distribution, critical values and the standard error are used to calculate the confidence interval (see Section 3.5.1).\nThere are theoretical probability distributions that do not work with a standard error, such as the F distribution or chi-squared distribution. If we use those distributions to approximate the sampling distribution of a continuous sample statistic, for instance, the association between two categorical variables, we cannot use the formula for a confidence interval (Section 3.5.1) because we do not have a standard error. We must use bootstrapping to obtain a confidence interval.\nAs you might remember from Section 2.5, we simulate a sampling distribution if we bootstrap a statistic, for instance median candy weight in a sample bag. We can use this sampling distribution to construct a confidence interval. For example, we take the values separating the bottom 2.5% and the top 2.5% of all samples in the bootstrapped sampling distribution as the lower and upper limits of the 95% confidence interval. We will encounter the bootstrapping method for confidence intervals around regression coefficient of mediator again in chapter 11.\nIt is also possible to construct the entire sampling distribution in exact approaches to the sampling distribution. Both the standard error and percentiles can be used to create confidence intervals. This can be very demanding in terms of computer time, so exact approaches to the sampling distribution usually only report p values (see Section 4.2.6), not confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "03-estimation.html#sec-SPSS-CI",
    "href": "03-estimation.html#sec-SPSS-CI",
    "title": "3  Estimating a Parameter",
    "section": "3.6 Confidence Intervals in SPSS",
    "text": "3.6 Confidence Intervals in SPSS\nNow that you have learned about confidence intervals, it is useful to know how to add these confidence intervals to all analyses that you will execute in the future.\n\n3.6.1 Instruction\nIn the video below we will show you how to set confidence intervals in SPSS for several analyses. Think about t-tests (Chapter 2), ANOVA’s (Chapter 5) and regression analyses (Chapter 6, 7, 8, and 9). Since you have learned what confidence intervals are, and after this paragraph you also know how to implement the correct settings in SPSS we will ask of you to always report the confidence intervals of your results in addition to the test statistic, p-value and effect size. This provides a more complete picture of the research findings, we will elaborate more on this in the next Chapter.\nWhen we execute a t-test the 95% confidence interval is already set as default option. We would only have to change this setting if we would like a confidence interval with a different confidence level (e.g., 90%). In Figure 3.2 we showcased the effect of the confidence level on the precision of the interval.\nIn the case of an ANOVA, the confidence interval is not necessary for the F-statistic. However if we conduct a post-hoc test we do automatically get a confidence interval of 95%. This time we can change the confidence interval by adjusting the significance level in the Post Hoc dialog. When we use General Linear Model, which we do for two-way ANOVA, we can find the significance level option under the options dialog.\nWhen we conduct a regression analysis, we have to specifically ask for and turn on the option of confidence intervals. This can be done in the Statistics dialog. We have the opportunity to change the confidence level.\nLastly, when we want a confidence interval for a correlation coefficient we need to use the option bootstrap. Bootstrapping has been introduced in Section 2.5. Be aware that in every analysis we have the opportunity to bootstrap the intervals, how to add bootstrapping is shown in this video: Video 2.4 in Section 2.6.\nNote The confidence level is the relative width of the interval in terms of percentages, e.g. 90%, 95% or 99%. The confidence interval is the absolute width of the interval, thus the values belonging to the percentages, e.g. 4.3 to 5.6.\nWatch the video below for the step by step instructions, details and additional information.\n\n\n\n\n\n\nVideo 3.1: Displays the 95% confidence intervals in SPSS.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "03-estimation.html#take-home-points",
    "href": "03-estimation.html#take-home-points",
    "title": "3  Estimating a Parameter",
    "section": "3.7 Take-Home Points",
    "text": "3.7 Take-Home Points\n\nIf a sample statistic is an unbiased estimator, we can use it as a point estimate for the value of the statistic in the population.\nA point estimate may come close to the population value but it is almost certainly not correct.\nA 95% confidence interval is an interval estimate of the population value. We are 95% confident that the population value lies within this interval. Note that confidence is not a probability!\nA larger sample or a lower confidence level yields a narrower, that is, a more precise confidence interval.\nA larger sample yields a smaller standard error, which yields a more precise confidence interval because the limits of a 95% confidence interval fall one standard error times the critical value below and above the value of the sample statistic.\n\n\n\n\n\nHoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan Wagenmakers. 2014. “Robust Misinterpretation of Confidence Intervals.” Psychonomic Bulletin & Review 21: 1157–64.\n\n\nNeyman, Jerzy. 1937. “Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability.” Philosophical Transactions of the Royal Society of London.Series A, Mathematical and Physical Sciences 236 (767): 333–80.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Estimating a Parameter</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html",
    "href": "04-hypothesis.html",
    "title": "4  Hypothesis testing",
    "section": "",
    "text": "Summary\nIn the preceding chapter, we have learned that a confidence interval contains the population values that are plausible, given the sample that we have drawn. In the current chapter, we apply our knowledge of sampling distributions, probability models and parameter estimation to hypothesis testing.\nThis chapter explores various methods for testing hypotheses. While we primarily focus on the widely used null hypothesis significance testing (NHST), we also discuss how confidence intervals and Bayesian statistics can aid in making decisions about hypotheses.\nWe will first extensively cover the framework of null hypothesis significance testing (NHST). Section 4.2 covers key concepts such as the null and alternative hypotheses, significance level (alpha), power of a test, p-values, and effect sizes. The section also discusses one-sided and two-sided tests and the importance of sample size in determining the power of a test.\nIn Section 4.3 we offer guidelines for reporting statistical test results. It emphasizes clarity and transparency in presenting findings to different audiences, including fellow scientists and general readers. The section covers the necessary components of a statistical report, such as test statistics, p-values, effect sizes, and confidence intervals.\nSection 4.4 on Statistical Test Selection guides the selection of appropriate statistical tests based on the data and research questions. It provides a framework for choosing tests by considering factors such as the type of data, the number of groups being compared, and the study design. The section includes flowcharts and examples to illustrate the decision-making process.\nWe continue with a discussion of confidence intervals as an alternative to hypothesis testing in Section 4.5. It explains how confidence intervals provide a range of plausible values for population parameters and how they can be used to make inferences about hypotheses. The section also discusses bootstrapped confidence intervals and their application.\nWe follow up with Bayesian hypothesis testing, contrasting it with frequentist methods. We explain the Bayesian approach of updating prior beliefs with data to obtain posterior probabilities. The section (Section 4.6) covers the concepts of prior, likelihood, and posterior distributions, and how they are used to make decisions about hypotheses.\nIn the final section (Section 4.7) we critically examine the limitations and criticisms of null hypothesis significance testing. We discuss issues such as the misinterpretation of p-values, the overemphasis on statistical significance over practical significance, and the risks of data dredging and publication bias. The section advocates for a more nuanced understanding and reporting of statistical results.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#sec-binarydecision",
    "href": "04-hypothesis.html#sec-binarydecision",
    "title": "4  Hypothesis testing",
    "section": "4.1 Hypothesis",
    "text": "4.1 Hypothesis\nThe assumption that a researcher wants to test is called a research hypothesis. It is a statement about the empirical world that can be tested against data. Communication scientists, for instance, may hypothesize that:\n\na television station reaches half of all households in a country,\nmedia literacy is below a particular standard (for instance, 5.5 on a 10-point scale) among children,\nopinions about immigrants are not equally polarized among young and old voters,\n\nthe celebrity endorsing a fundraising campaign makes a difference to adult’s willingness to donate,\n\nmore exposure to brand advertisements increases brand awareness among consumers,\nand so on.\n\nThese are statements about populations: all households in a country, children, voters, adults, and consumers. As these examples illustrate, research hypotheses seldom refer to statistics such as means, proportions, variances, or correlations. Still, we need a statistic to test a hypothesis. The researcher must translate the research hypothesis into a new hypothesis that refers to a statistic in the population, for example, the population mean. The new hypothesis is called a statistical hypothesis.\nA statistical hypothesis is a statement about the empirical world that can be tested against data. It is a statement about the population, not the sample. For example, a hypothesis could be that the average age of a population is 30 years, that the average waight of candy bags is 500 grams, that the proportion of people that like a certain brand is .5, or that the correlation between two variables is .3. These are all statements about the population, not the sample.\nScientists test these hypotheses by following the empirical cycle (de Groot 1969), which involves a systematic process of induction, deduction, testing, and evaluation. Based on the results, hypotheses can be either rejected or not rejected. If the hypothesis is based on theory and previous research, the scientist uses previous knowledge. As a next step, the researcher tests the hypothesis against data collected for this purpose. If the data contradict the hypothesis, the hypothesis is rejected and the researcher has to improve the theory. If the data does not contradict the hypothesis, it is not rejected and, for the time being, the researcher does not have to change the theory.\nStatistical hypotheses usually come in pairs: a null hypothesis (H0) and an alternative hypothesis (H1 / HA). We met the null hypothesis in the preceding sections. We use it to create a (hypothetical) sampling distribution. To this end, a null hypothesis must specify one value for the population statistic that we are interested in, for example, .5 as the proportion of yellow candies.\n\n4.1.1 Null hypothesis\nThe null hypothesis reflects the skeptical stance in research. It assumes that there is nothing going on. There is no difference between experimental conditions, the new intervention is not better than the previous, there is no correlation between variables, there is no predictive value to your regression model, a coin is fair, and so forth. Equation 4.1 shows some examples of null hypotheses expressed in test statistics.\n\\[\n\\begin{eqnarray*}\nH_{0} & : \\theta & = .5 \\\\\nH_{0} & : \\mu & = 100 \\\\\nH_{0} & : t & = 0 \\\\\nH_{0} & : \\mu_1 & = \\mu_2 \\\\\n\\end{eqnarray*}\n\\tag{4.1}\\]\nThe null hypothesis does not always assume that the population parameter is zero; it can take any specified value. For instance, a null hypothesis might state that there is no difference in intelligence between communication science students and the general population. In this case, we could compare the average intelligence score of a sample of communication science students to the known population average of 100. Although we are testing whether the difference is zero, in practical terms, we express the null hypothesis as the sample mean being equal to 100.\nThough a null hypothesis can be expressed as a single value, that does not mean that we always get that specific value when we take a random sample. Given the null hypothesis that our candy factory machine produces bags with an average of 5 out of 10 yellow candies, there remains a probability that some bags will contain as few as one yellow candy or even none at all. This variability is illustrated in Figure 4.1, which shows that while values near the expected 5 yellow candies are most likely, deviations can occur.\n\n\n\n\n\n\n\n\nFigure 4.1: Discrete binomial distribution.\n\n\n\n\n\n\n\n4.1.2 Alternative hypothesis\nThe alternative hypothesis indicates what the researcher expects in terms of effects, differences, deviation from null. It is the operationalization of what you expect to find if your theory would be accurate. This would mean that our expected effect of difference would indeed reflect the true population value.\nLets assume for the case of our candy factory example, that the machines parameter is .2. We would expect the machine to produces bags with 2 out of 10 yellow candies, one in five yellow candies per bag. Assuming .2 as the machine’s parameter does not ensure that every bag will contain exactly 2 yellow candies. Some bags will contain 0, 1, 3, 4, 5, 6, 7, 8, 9, or even 10 yellow candies. The probabilities for each can again be visualized using the exact discrete binomial probability distribution (Figure 4.2) as we did for the null hypothesis.\n\n\n\n\n\n\n\n\nFigure 4.2: Discrete alternative binomial distribution.\n\n\n\n\n\nNote that the probability distribution for \\(H_0\\) indicates the null assumption about reality, while the probability distribution for \\(H_A\\) is based on the true population value. At this stage only the sample size, the amount of candies in a bag (10), the null assumption and our knowledge about reality has been used to determine the distribution. No data has been gathered yet in determining these distributions.\nWhen we do research, we do not know the true population value. If we did, no research would be needed of course. What we do have is theories, previous research, and other empirical evidence. Based on this, we can make an educated guess about the true population value. This educated guess is expressed as the alternative hypothesis. The alternative hypothesis is the hypothesis that the researcher expects to find if the theory is accurate. It is the hypothesis that the researcher wants to test using data.\nThe alternative hypothesis is usually formulated as either not equal to the null hypothesis, or greater than or less than the null hypothesis. Equation 4.2 shows some examples of alternative hypotheses expressed in test statistics.\n\\[\n\\begin{eqnarray*}\n\\text{Two-sided} \\\\\nH_{A} & : \\theta & \\neq .5 \\\\\nH_{A} & : \\hat{x} & \\neq \\mu \\\\\nH_{A} & : t & \\neq 0 \\\\\nH_{A} & : \\mu_1 & \\neq \\mu_2 \\\\\n\\text{One-sided} \\\\\nH_{A} & : \\theta & &lt; .5 \\\\\nH_{A} & : \\hat{x} & &gt; 100 \\\\\nH_{A} & : t & &gt; 0 \\\\\nH_{A} & : \\mu_1 & &lt; \\mu_2 \\\\\n\\end{eqnarray*}\n\\tag{4.2}\\]\nThe alternative hypothesis can be one-sided or two-sided. A two-sided alternative hypothesis states that the population value is not equal to the null hypothesis. A one-sided alternative hypothesis states that the population value is either greater than or less than the null hypothesis. The choice between a one-sided and two-sided test depends on the research question and the theory. We will cover one and two-sided testing more extensively in Section 4.2.12.\n\n\n4.1.3 Testing hypothesis\nIn the empirical cycle, the researcher tests the hypothesis against data collected for this purpose. The most widely used method for testing hypotheses is null hypothesis significance testing (NHST). As we will read in this chapter, there are other methods that can be used to test hypotheses, such as confidence intervals and Bayesian statistics.\nAll methods serve as decision frameworks that enable researchers to establish rules for evaluating their hypotheses. These rules are determined before data collection and are designed to minimize the risk of incorrect decisions. Null Hypothesis Significance Testing (NHST) manages this risk by defining it probabilistically. Confidence intervals provide a measure of accuracy through their width, while Bayesian statistics express this risk in terms of the credibility interval.\nIn the next chapters, we will cover the logic behind NHST, confidence intervals, and Bayesian statistics. We will also discuss how to select the appropriate statistical test for your research question, and how to report the results of your statistical tests.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#sec-null-hypothesis-significance-testing",
    "href": "04-hypothesis.html#sec-null-hypothesis-significance-testing",
    "title": "4  Hypothesis testing",
    "section": "4.2 Null Hypothesis Significance Testing",
    "text": "4.2 Null Hypothesis Significance Testing\nNull Hypothesis Significance Testing (NHST) is the most widely used method for statistical inference in the social sciences and beyond. The logic underlying NHST is called the Neyman Pearson approach (Lehmann 1993). Though these names are not widely known, the work of Jerzy Neyman (1894–1981) and Egon Pearson (1895–1980) still has a profound impact on the way current research is conducted, reviews are considered, and papers are published.\nThe Neyman Pearson approach ensures tight control on the probability of making correct and incorrect decisions. It is a decision framework that gives you a clear criterion and also an indication of what the probability is that your decision is wrong. The decision in this regard, is either the acceptance or rejection of the \\(H_0\\) hypothesis.\nThe Neyman Pearson approach is about choosing your desired probability of making correct and incorrect decisions, setting up the right conditions for this, and making a decision. It considers the following:\n\nAlpha - Determine your desired risk of drawing the wrong conclusion.\nPower - Determine your desired probability of drawing the correct conclusion.\nThe true effect size\nThe sample size needed to achieve desired power.\nConduct your research with this sample size.\nDetermine the test statistic.\nDetermine if \\(p\\)-value \\(\\leq \\alpha\\). If so, reject \\(H_0\\).\n\nThe two decisions can be visualized in a \\(2 \\times 2\\) table where in reality \\(H_0\\) can be true or false, and the decision can either be to reject \\(H_0\\) or not. Figure 4.3 illustrates the correct and incorrect decisions that can be made. The green squares obviously indicate that it is a good decision to reject \\(H_0\\) when it is in fact false, and not to reject \\(H_0\\) if it is in reality true. And the red squares indicate that it is a wrong decision to reject \\(H_0\\) when it is actually true (Type I error), or not reject \\(H_0\\) if it is in reality false (Type II error).\n\n\n\n\n\n\nFigure 4.3: NHST decision table.\n\n\n\nIntuitively it is easy to understand that you would want the probability of an incorrect decision to be low, and the probability of a correct decision to be high. But how do we actually set these probabilities? Let’s consider the amount of yellow candies from the candy factory again. In Section 1.2 we learned that the factory produces candy bags where one fifth of the candies are supposed to be yellow. Now suppose we don’t know this and our null hypothesis would be that half of the candies would be yellow. In Figure 1.4 you can set the parameter values to .5 and .2 and see what the discrete probability distributions look like.\nAs the candy factory produces bags with ten candies, we can look at both probability distributions. Figure 4.4 shows both distributions.\n\n\n\n\\(H_0\\) Distribution\n\nHalf of the candies in the bag are yellow\nThe parameter of the candy machine is .5\nWith expected value 5 out of 10\n\n\n\n\n\\(H_T\\) True distribution\n\nOne fifth of the candies in the bag are yellow\nThe parameter of the candy machine is .2\nWith expected value 2 out of 10\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Discrete alternative binomial distributions.\n\n\n\n\n\nWe will use both distributions in Figure 4.4 to clarify the different components within the Neyman Pearson approach later in this chapter. For now, take a good look at both probability distributions, and consider a bag of candy containing 4 yellow candies. Are you able to determine if this bag is the result of a manufacturing process that produces bags with 20% or 50% yellow candies?\nDoing research is essentially the same. You collect one sample, and have to determine if the effect of your study is non existent (\\(H_0 = \\text{true}\\)) or that there is something going on (\\(H_0 \\neq \\text{true}\\)).\n\n4.2.1 Alpha\nThe first step in the Neyman Pearson approach is to set the desired type I error rate, also known as the significance level, \\(\\alpha\\). This is the probability of rejecting the null hypothesis when it is in reality true. In the \\(2 \\times 2\\) decision table in Figure 4.3, this corresponds to the top left quadrant.\nAs a researcher, you decide how much risk you are willing to take to make a type I error. As the Neyman Pearson approach is a decision framework, you have to set this probability before you start collecting data. The most common value for \\(\\alpha\\) is .05, which means that you accept a 5% chance of making a type I error of rejecting the null hypothesis when it is in reality true.\nIn our yellow candy example, assuming the null hypothesis to be true, relates to the parameter value of .5 and the associated probability distribution shown in Figure 4.1. We have already determined that if \\(H_0\\) is true, it is still possible we could get a bag with 0 or 10 yellow candies. Deciding to reject the null hypothesis in any of these cases, would be wrong, because the null hypothesis is assumed to be true. The exact probabilities can be found on the y-axis of Figure 4.1, and are also shown in the Table 4.1 below. Looking at the probability of getting 0 or 10 candies in Table 4.1, we see that together this amounts to .002 or 0.2%. If we would decide to only reject the null hypothesis if we would get 0 or 10 candies, this would be a wrong decision, but we would also know that the chance of such a decision is pretty low. Our type I error, alpha, significance level, would be .002.\n\n\n\n\nTable 4.1: Probabilities of drawing a certain amount of yellow candies from a bag of 10 candies, assuming the null hypothesis to be true.\n\n\n\n\n\n\n#Y\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nPr H0\n0.001\n0.010\n0.044\n0.117\n0.205\n0.246\n0.205\n0.117\n0.044\n0.010\n0.001\n\n\n\n\n\n\n\n\nChoosing such an alpha level would result in a threshold between 0 and 1 and 9 and 10. We call this the critical value associated with the chosen alpha level. Where on the outside of the threshold we would reject the null hypothesis, and inside the threshold we would not reject the null hypothesis. So, if that is our decision criterion, we would reject the null hypothesis if we would draw a bag with 0 or 10 yellow candies, and not reject the null hypothesis if we would draw a bag with 1, 2, 3, 4, 5, 6, 7, 8, or 9 yellow candies. Amounting to a type I error rate of .002 or 0.2%. Figure 4.5 shows the critical values for the null hypothesis distribution, and indicate what the decision would be for values on the outside and inside of the decision boundary.\n\n\n\n\n\n\n\n\nFigure 4.5: H0 binomial distribution with critical values.\n\n\n\n\n\nIn the social sciences, we allow ourselves to make a wrong decision more often. We usually set the alpha level to .05. For our discrete example setting the alpha level to .05 is not really possible. Looking at Table 4.1, we could raise the significance level to .022 if we would reject the null hypothesis if we would draw 0, 1 or 9, 10 yellow candies. This would result in a type I error rate of 2.2%. Though if we would also reject the null hypothesis with 2 or 8 yellow candies, we would have a type I error rate of 11%. For a discrete probability distribution with a limited number of outcomes, it is not always possible to set the alpha level exactly to .05.\nFor continuous probability distributions, such as the normal distribution, it is possible to set the alpha level to exactly .05. For example the null hypothesis that average media literacy in the population of children equals 5.5 on a scale from one to ten.\nFor such continuous variables, we can estimate a sampling distribution around the hypothesized population value using a theoretical approach (Section 2.3). Remember (Section 1.2.4) that the population value is the expected value of the sampling distribution, that is, its mean (if the estimator is unbiased). The sampling distribution, then, is centered around the population value specified in the null hypothesis. This sampling distribution tells us the probabilities of all possible sample outcomes if the null hypothesis is true. It allows us to identify the most unlikely samples. In Step 2 in Figure 4.6, we set the alpha level to .05. This means that we cut off 2.5% of the area in each tail of the sampling distribution. The critical values are the values that separate the 2.5% of the area in each tail from the 95% of the area in the middle. If we assume the population parameter to be 5.5, rejecting the null hypothesis would again be a wrong decision. Thus setting the boundary by using an alpha level of .05, would yield a wrong decision in 5% of the samples we take. Just like the discrete candy color case, we decide to reject \\(H_0\\) on the outside of the critical value and not reject \\(H_0\\) on the inside of the critical value. In Step 4 in Figure 4.6, we add the result of a sample. You can redraw multiple samples by clicking the button in the app.\nNote that the reasoning for the discrete case and the continuous case is the same. The only difference is that for the continuous case we can set the alpha level exactly to .05.\n\n\n\n\n\n\n\nFigure 4.6: Sampling distribution of average media literacy according to the null hypothesis.\n\n\n\n\n\n4.2.2 1 - Alpha\nThe decision to not reject the null hypothesis when it is in reality true is indicated by \\(1 - \\alpha\\). It does not go by any other name, but in terms of probability, it is directly dependent on your desired type I error rate, your chosen alpha level. It therefore corresponds to the probabilities in Table 4.1 of 1, 2, 3, 4, 5, 6, 7, 8, or 9 yellow candies in the candy factory example. We have a 99.8% (1 - .002) chance of making the correct decision to not reject \\(H_0\\) when we assume it to be true. The inside of the critical value in Figure 4.5 is the area where we do not reject the null hypothesis. In the \\(2 \\times 2\\) decision table in Figure 4.3, this corresponds to the bottom left green quadrant.\nNow that we have determined our critical value (for our particular sample size) based on our desired alpha, significance level, we can use this critical value to look at the power.\n\n\n4.2.3 Power (true)\nThe true power is the probability of making the correct decision to reject the null hypothesis when it is in fact false. In the \\(2 \\times 2\\) decision table in Figure 4.3, this corresponds to the top right quadrant. As we have already set our decision criterion by choosing our alpha level in the previous step, we already know when we decide to reject the null hypothesis. In Figure 4.5, based on the null distribution, we determined our type I error to be 0.2%, if we would reject the null hypothesis if we would draw 0 or 10 yellow candies. The critical value would in that case be between 0 and 1 and 9 and 10. We use this same critical value to determine the power of the test, as it establishes our decision boundary.\nAs the right column of Figure 4.3 only states that \\(H_0 = \\text{FALSE}\\), it does not state what this entails. Within the Neyman Pearson approach, this would be the true population value with its associated probability distribution. We already established that this would be the distribution with a parameter value of .2. In Figure 4.7, we see that our decision criterion is still the same. That we decide to reject the null when we sample 0 or 10 yellow candies. But the distribution has now changed.\n\n\n\n\n\n\n\n\nFigure 4.7: True binomial distributions with critical values.\n\n\n\n\n\nWith this distribution based on the actually true population parameter, deciding to reject the null would be a good decision. Though, we can also see that if this is true, if the parameter truly is .2, getting a bag with 0 or 10 yellow candies does not happen that often. The probabilities for 10 yellow candies is almost none, and the probability for getting 0 yellow candies is about 11%. This means that if this hypothesis is true, if our sample originates from this distribution, we would only make the decision to reject the null hypothesis in 11% of the samples we get out of it. So, the power of the test, correctly rejecting the null is only 11%.\n\nThe only way to increase the power is to increase the sample size of the study, or increase the type I error.\n\nAs stated earlier we would rather have a higher probability of making the correct decision. In the social sciences we are striving for a power of .80. This means that we want to make the correct decision in 80% of the cases when the null hypothesis is false. In our candy factory example, this would mean that we would want to reject the null hypothesis in 80% of the replications. With our machine producing bags with 10 candies, this is just not possible. The only way to increase the power is to increase the sample size of the study. In the candy factory example, this would mean that we would have to increase the number of candies in the candy bags. We will come back to this in the Section 4.2.11 on sample size.\nOne more thing to note, is that the true power of the test can only be determined if you know the true population value. In practice, we do not know if the null or the alternative hypothesis is true. We can only calculate the power of the test when we assume some alternative hypothesis. It is good practice to base your assumptions about the alternative hypothesis on previous research, theory, or other empirical evidence. This is mostly expressed as the expected effect size, the expected difference between the null and the alternative hypothesis.\nIn all statistical software, the power of the test is not calculated based on the true effect size, but on the found effect size in your sample. This is called the observed power and will be covered in Section 4.2.8.\n\n\n4.2.4 Beta\nThe probability of making a type II error is indicated by \\(\\beta\\). It is the probability of not rejecting the null hypothesis when it is in reality false. In the \\(2 \\times 2\\) decision table in Figure 4.3, this corresponds to the bottom right quadrant. The power of the test is \\(1 - \\beta\\). In our candy factory example, the power of the test is .11, so the probability of making a type II error is .89. It is the sum of the probabilities of getting 1, 2, 3, 4, 5, 6, 7, 8, or 9 yellow candies, when the machine actually produces bags with 2 yellow candies with the corresponding probabilities as shown in Figure 4.7.\n\n\n4.2.5 Test statistic\nIn Section 1.2.1 we discussed the sample statistic, and defined it as any value describing a characteristic of the sample. This could be the mean, or the proportion, or the correlation, or the regression coefficient. It is a value that is calculated from the sample. Note that conversions of the sample statistic, such as the difference between two sample means, or the ratio of two sample variances, \\(t\\)-values, \\(F\\)-values, and \\(\\chi^2\\)-values are also sample statistics.\nThe test statistic is a sample statistic that is used to test the null hypothesis. In our candy factory example, the test statistic would be the number of yellow candies in the bag we sample. If we would draw a bag with 4 yellow candies, the test statistic would be 4.\nIn the previous sections, we have determined our decision criterion, the critical value, based on our desired alpha level. We have also determined the power of the test, based on the alternative hypothesis. The test statistic is used to determine if we reject the null hypothesis or not. If the test statistic is equal to the critical value or more extreme, we reject the null hypothesis. If the test statistic is inside the critical value, we do not reject the null hypothesis.\nLooking at Figure 4.5, we see that the critical value is between 0 and 1 and 9 and 10. If we would draw a bag with 4 yellow candies, we can check if the value 4 is inside or outside the critical value. As 4 is inside the critical value, we would not reject the null hypothesis.\n\nThe test statistic is the value that is used to decide if we reject the null hypothesis or not.\n\nFor continuous variables, as described in Figure 4.6, the test statistic is the sample mean. If the sample mean is outside the critical value, we reject the null hypothesis. If the sample mean is inside the critical value, we do not reject the null hypothesis. If you select Step 4 in Figure 4.6, and draw a few samples, you can see if the test statistic, the sample mean, is inside or outside the critical value. Again, the reasoning for continuous variables is the same as for the discrete variables.\n\n\n4.2.6 P-value\nWe have learned that a test is statistically significant if the test statistic is in the rejection region. Statistical software, however, usually does not report the rejection region for the sample statistic. Instead, it reports the p-value of the test, which is sometimes referred to as significance or Sig. in SPSS.\n\nThe p-value is the probability of obtaining a test statistic at least as extreme as the result actually observed, under the assumption that the null hypothesis is true.\n\nIn the previous section we considered a sample with 4 yellow candies. The p-value gives the probability of randomly drawing a sample that is as extreme or more extreme than our current sample assuming that the null hypothesis is true. “As extreme or more extreme” here means as far or further removed from the value specified by the null hypothesis. Concretely, in our case that means the probability of drawing a sample with 4 or fewer yellow candies. The p-value considers the probability of such a sample, but also ads the probability of getting a sample with less yellow candies. This is what is meant with “at least as extreme”. this is not really intuitive, but it refers to the less likely test statistics, iIn our case 0, 1, 2 and 3, are even less probable than 4 yellow candies. The assumption that the null hypothesis is true indicates that we need to look at the probabilities from the sampling distribution that is created based on the null distribution hypothesis. Looking at Table 4.1, we see that the probability of drawing a random sample with 0, 1, 2, 3 or 4 yellow candies under the null distribution is 0.001 + 0.010 + 0.044 + 0.117 + 0.205 = 0.377 according to the sampling distribution belonging to the null hypothesis. This 0.377 is the p-value. The conditional (conditional on H0 being true) probability of getting a sample that is as or less likely than the test statistic that we have of our current sample.\n\nRejecting the null hypothesis does not mean that this hypothesis is false or that the alternative hypothesis is true. Please, never forget this.\n\nThe reasoning applied when comparing our test statistic to the critical value is the same as when comparing the p-value to the alpha level. If the p-value is smaller or equal to than the alpha level, we reject the null hypothesis. If the p-value is larger than the alpha level, we do not reject the null hypothesis.\nIf the test statistic is within the critical values, the p-value is always larger than the alpha level. If the test statistic lies outside the critical value, the p-value is always smaller than the alpha level. In the case that the test statistic is exactly the same as the critical value, the p-value is exactly equal to the alpha level, we still decide to reject the null hypothesis.\n\n\n\n\n\n\nReject \\(H_0\\) when \\(p\\)-value \\(\\leq \\alpha\\)\n\n\n\nAs both the p-value and the alpha level assume the null to be true, you can find both probabilities under the null distribution. For continuous variables, the p-value is the area under the curve of the probability distribution that is more extreme than the sample mean. The significance level is chosen by you as a researcher and is fixed.\n\nIt is important to remember that a p-value is a probability under the assumption that the null hypothesis is true. Therefore, it is a conditional probability.\n\nCompare it to the probability that we throw sixes with a dice. This probability is one out of six under the assumption that the dice is fair. Probabilities rest on assumptions. If the assumptions are violated, we cannot calculate probabilities.\nIf the dice is not fair, we don’t know the probability of throwing sixes. In the same way, we have no clue whatsoever of the probability of drawing a sample like the one we have if the null hypothesis is not true in the population.\nFigure 4.8 shows a t-distribution, which represents the null distribution. A statistical test was set up with an alpha level of 5% (blue area). The and the p-value (red area) indicates the probability of drawing a random sample with a t-value of 2 or values that are even further removed from the null hypothesis (more extreem). The figure shows what this test would look like for a two sided test (left) and a one two sided hypothesis test (right). We will cover one and two sided testing in Section 4.2.12. For now, just notice that, looking at the left graph, the p-value is greater than 0.05, because the test statistic is not as or more extreme than the critical value. In other words, the test is not significant. In the one-sided test depicted on the right, the p-value lies in the rejection region and is, thus, significant.\n\n\n\n\n\n\n\n\nFigure 4.8: T-distributions with alpha level and p-value.\n\n\n\n\n\nIn Figure 4.8, the blue vertical boundaries represent the critical value associated with a chosen alpha level of 5%, the blue area under the curve. The red vertical line represents the t-value from the sample, which in this example was 2. The red area under the curve represents the p-value, the probability of getting this t-value or more extreme.\nFigure 4.9 represents the sampling distribution of average media literacy. You can take a sample and play around with the population mean according to some null hypothesis. If the mean in the sample is outside the critical value, it falls in the alpha rejection region.\n\n\n\n\n\n\n\nFigure 4.9: Sampling distribution of average media literacy according to the null hypothesis.\n\n\n\nThe reasoning is again the same as for discrete variables. If the p-value is smaller or equal to the alpha level, we reject the null hypothesis. If the p-value is larger than the alpha level, we do not reject the null hypothesis.\n\n\n4.2.7 True effect size\nThe true effect size is the difference between the null hypothesis and the true population value. This can also be expressed in terms of the test statistic. For example, if the IQ scores for communication science students are 120 in the population, the true affect size can be expressed as 20 IQ points, but also as a t-value. The true effect size denotes the genuine effect within the population, representing the actual difference, correlation, or parameter value.\nIn the candy factory example, the true effect size is .5 - .2 = .3. This is the difference in the proportion of yellow candies in the bags. In Figure 4.10 you can see the difference in the two distributions. The true effect size is the difference in the expected value of the two distributions. In absolute terms, it is 5 - 3 expected number of yellow candies in the bag. In terms of the parameter it is the proportion .5 - .2.\n\n\n\n\n\n\n\n\nFigure 4.10: Effect in discrete binomial distributions.\n\n\n\n\n\nTrue refers to the actual difference in the population, which is unknown to us. In our candy factory example, we can only observe the sample from a candy bag and make assumptions based on the null and alternative hypotheses.\nDepending on the true value in the population, a true effect size could be small, medium, or large. In order to detect small true effect sizes, we need a large sample size. A larger sample offers more precision, so the difference between our sample outcome and the hypothesized value is more often sufficient to reject the null hypothesis. For example, we would reject the null hypothesis that average candy weight is 2.8 grams in the population if average weight in our sample bag is 2.70 grams and our sample is large. But we may not reject this null hypothesis if we have the same outcome in a small sample bag.\nThe larger our sample, the more sensitive our test will be, so we will get statistically significant results more often. If we think of our statistical test as a security metal detector, a more sensitive detector will go off more often.\n\n4.2.7.1 Practical relevance\nInvestigating the effects of a new medicine on a person’s health, we may require some minimum level of health improvement to make the new medicine worthwhile medically or economically. If a particular level of improvement is clinically important, it is practically relevant (sometimes called practically significant).\nIf we have decided on a minimum level of improvement that is relevant to us, we want our test to be statistically significant if the average true health improvement in the population is at least of this size. We want to reject the null hypothesis of no improvement in this situation.\n\n\n\n\n\n\n\nA larger sample size makes a statistical test more sensitive. The test will pick up (be statistically significant for) smaller effect sizes.\nA larger effect size is more easily picked up by a statistical test. Larger effect sizes yield statistically significant results more easily, so they require smaller samples.\n\n\n\n\nFor media interventions such as health, political, or advertisement campaigns, one could think of a minimum change of attitude affected by the campaign in relation to campaign costs. A choice between different campaigns could be based on their efficiency in terms of attitudinal change per cost unit.\nNote the important difference between practical relevance and statistical significance. Practical relevance is what we are interested in. If the new medicine is sufficiently effective, we want our statistical test to signal it. In the security metal detector example: If a person carries too much metal, we want the detector to pick it up.\nStatistical significance is just a tool that we use to signal practically relevant effects. Statistical significance is not meaningful in itself. For example, we do not want to have a security detector responding to a minimal quantity of metal in a person’s dental filling. Statistical significance is important only if it signals practical relevance. We will return to this topic in Section 4.2.11 on sample size.\n\n\n\n4.2.8 Observed effect size\nIn Section 4.2.7 we discussed the true effect, the difference between the null hypothesis and the true alternative hypothesis. The problem is that we do not know the true effect, we do not know which of the two hypothesis is actually true.\nWe can only estimate the true effect using the sample statistic. The difference between the sample statistic and the null hypothesis is called the observed effect size. In the candy factory example, the observed effect size is the difference between the number of yellow candies in the sample and the number of yellow candies in the null hypothesis. If the null hypothesis is that the machine produces bags with 5 yellow candies, and the sample contains 4 yellow candies, the observed effect size is 1.\nThe same definition holds for the continuous case. If the null hypothesis is that the average media literacy in the population is 5.5, and the sample mean is 3.9, the observed effect size is 1.6. Or if we hypothesize that average candy weight in the population is 2.8 grams and we find an average candy weight in our sample bag of 2.75 grams, the effect size is -0.05 grams. If a difference of 0.05 grams is a great deal to us, the effect is practically relevant.\nNote that the effect sizes depend on the scale on which we measure the sample outcome. The unstandardized effect size of average candy weight changes if we measure candy weight in grams, micro grams, kilograms, or ounces. Of course, changing the scale does not affect the meaning of the effect size but the number that we are looking at is very different: 0.05 grams, 50 milligrams, 0.00005 kilos, or 0.00176 ounces. For this reason, we do not have rules of thumb for interpreting these unstandardized effect sizes in terms of small, medium, or large effects. But we do have rules of thumb for standardized effect sizes. Unstandardized effect sizes are very useful for reporting the practical results of your study, but they are not very useful for comparing studies or for meta-analysis.\nYou can imagine that estimating the true effect size on just one sample is not very reliable. The observed effect size could be the result of our sample being the result of the null being true, or the alternative being true. The way researchers try to get a notion of the true effect size is by replicating the study. If the observed effect size is consistent over multiple replications, we can be more confident that the average observed effect size is the true effect size. This is what we will cover in Section 4.2.10 about meta analysis.\n\n4.2.8.1 Cohen’s d\nIn scientific research, we rarely have precise norms for raw differences (unstandardized effects) that are practically relevant or substantial. For example, what would be a practically relevant attitude change among people exposed to a health campaign?\nTo avoid answering this difficult question, we can take the variation in scores (standard deviation) into account. In the context of the candies example, we will not be impressed by a small difference between observed and expected (hypothesized) average candy weight if candy weights vary a lot. In contrast, if candy weight is quite constant, a small average difference can be important.\nFor this reason, standardized effect sizes for sample means divide the difference between the sample mean and the hypothesized population mean by the standard deviation in the sample. Thus, we take into account the variation in scores. This standardized observed effect size for tests on one or two means is known as Cohen’s d. Equation 4.3 illustrates how the sample mean \\(\\bar{x}\\) is compared to the hypothesized population mean \\(\\mu_{H_0}\\), and how this difference is standardized by deviding through the standard deviation \\(s\\). In appendix Section 1 we will cover the calculation of the paired and independent t-tests.\n\\[\nd = \\frac{\\bar{x} - \\mu_{H_0}}{s_x}\n\\tag{4.3}\\]\nUsing an inventory of published results of tests on one or two means, Cohen (1969) proposed rules of thumb for standardized effect sizes (ignore a negative sign if it occurs):\n\n0.2: weak (small) effect,\n0.5: moderate (medium) effect,\n0.8: strong (large) effect.\n\nNote that Cohen’s d can take values above one. These are not errors, they reflect very strong or huge effects (Sawilowsky 2009).\n\n\n4.2.8.2 Association as effect size\nMeasures of association such as Pearson’s product-moment correlation coefficient or Spearman’s rank correlation coefficient express effect size if the null hypothesis expects no correlation in the population. If zero correlation is expected, a correlation coefficient calculated for the sample expresses the difference between what is observed (sample correlation) and what is expected (zero correlation in the population).\nEffect size is also zero according to the standard null hypotheses used for tests on the regression coefficient (b), R2 for the regression model, and eta2 for analysis of variance. As a result, we can use the standardized regression coefficient (Beta in SPSS and b* according to APA), R2, and eta2 as standardized effect sizes.\nBecause they are standardized, we can interpret their effect sizes using rules of thumb. The rule of thumb for interpreting a standardized regression coefficient (b*) or a correlation coefficient, for example, could be:\n\nVery weak: between 0 and .10\nWeak: between .10 and .30\nModerate: between .30 and .50\nStrong: between .50 and .80\nVery strong: between .80 and 1.00\nPerfect association: 1.00\n\nNote that we ignore the sign (plus or minus) of the effect when we interpret its size.\n\n\n\n4.2.9 Post hoc power\nJust as the observed effect size is based on the test statistic acquired from your sample, so is the post hoc power. It is also known as: observed, retrospective, achieved power (O’Keefe 2007).\n\nThe power of a test assuming a population effect size equal to the observed effect size in the current sample.\n— (O’Keefe 2007)\n\nThe post hoc power refers to the probability of rejecting the null hypothesis assuming the alternative hypothesis has a population mean equal to the observed sample mean or more accurately the observed test statistic.\n\n\n\n\n\n\n\n\nFigure 4.11: Discrete binomial distributions showing post hoc power.\n\n\n\n\n\nFigure 4.11 shows the post hoc power for a sample of 10 candies. The null hypothesis is that the machine produces bags with 5 yellow candies. The alternative hypothesis is that the machine produces bags with 2 yellow candies. But the post hoc power assumes the found test statistic of 4 candies to be the alternative population parameter of .4. Following the same decision criterion as defined in the previous sections, the post hoc power is almost zero. This is the probability of 0 or 10 yellow candies under the alternative distribution when rejecting the null hypothesis on the outside of the critical values.\nYou can imagine that if we look at a different candy bag and we would find 7 yellow candies, the post hoc power would not be the same. The post hoc power does not have much practical use, though SPSS produces this when you ask it, it is obvious that multiple replications of a research study will yield different results. As the true population mean is not a random variable, the actual power is fixed and should not vary.\n\n\n\n4.2.10 Meta analysis\nAs mentioned in Section 4.2.8, the observed effect size is based on the sample statistic, and is likely to differ with every sample you take. If our research hypothesis is actually true, a random sample from a population described by the sampling distribution of the alternative hypothesis would be mot likely to result in us holding a bag with 2 yellow candies. But as we have seen in Figure 4.2, getting 4 yellow candies is reasonably probable as well.\nNow imagine that we would take multiple samples, and calculate the observed effect size for each sample. If we would plot these observed effect sizes, we would get a distribution of observed effect sizes.\nIn research we can conduct replication studies to see if the observed effect size is consistent over multiple replications. If this is the case, we can be more confident that the average observed effect size is the true effect size and we can determine the true population mean. As we have seen in Chapter 1, it is in practical to draw many number of samples to create a sampling distribution. But we can use the results from multiple studies to get an indication of the true population mean.\nImagine that we get a hundred bags of candy (100 replications) and we consistently find 7 to 9 yellow candies, this would give us an indication that the true population value is 8. It would also indicate that our initial alternative hypothesis is highly unlikely. This is essentially what meta analysis is about. Collecting effect sizes from multiple studies and combining them to get an indication of the true effect size.\n\nMeta-analysis is a good example of combining research efforts to increase our understanding. It is useful to obtain more precise estimates of population values or effects. Meta-analysis is strongly recommended as a research strategy by Geoff Cumming, who coined the concept New Statistics. See Cumming’s book (2012), website, or YouTube channel if you are curious to learn more.\n\n\n4.2.11 Sample size\nAs stated in Section 4.2.3, the only way to increase the power of a test is to increase the sample size. In the candy factory example, the sample size is the total number of candies in the bag. With only 10 candies in the bag, the power of the test is only 0.11. To reach our desired power of 80%, we clearly need to increase the sample size. In Figure 4.12, we increased the number of candies in the bag to 20. We can see on the x-axis that the possible outcome space for the number of yellow candies in the bag is now 0 to 20. This still assumes our \\(H_0\\) to be true, and the parameter of the machine is still \\(\\theta = .5\\), half of the candies in the bag should be yellow. Though the parameter is still the same, the expected value when we have bags of 20 candies is now \\(.5 \\times 20 = 10\\), right in the middle of our distribution.\nFigure 4.12 still follows the reasoning scheme we have setup earlier. We decide to reject \\(H_0\\) on the outside of our critical values (Red vertical line). We determined the position of the critical value based on our chosen alpha level. Because our outcome space is larger we can be more accurate in striving for an \\(\\alpha = .05\\). Our alpha is now 4.1%, we get this by adding the yellow bars 0, 1, 2, 3, 4, 5 and 15 up until 20, under the null distribution. This is not exactly 5 percent, but shifting the critical value inwards, would make the alpha level to high. So, this is close enough.\nWith this sample size, we can acquire our desired power of 80%. If we would assume our alternative hypothesis to be true, our decision to reject the null when you get 5 or less yellow candies, would be correct 80% of the time. The power of 80% is the sum of the light yellow bars under the assumption that \\(H_A\\) is true on the outside of our critical value. So, the power is the probability of getting 0, 1, 2, 3 ,4 ,5 or 15, 16,17, 18, 19 ,20 yellow candies under the alternative distribution.\n\n\n\n\n\n\n\n\nFigure 4.12: Discrete binomial distributions for higher sample size.\n\n\n\n\n\nThe same reasoning is applied when using continuous sample statistics. Let’s revisit the candy weight example. We could have a null hypothesis that the average yellow candy weight is the same as the weight of all other candy colors. But if in reality the yellow candies would be heavier, let’s say with an effect size of .3, we would need to determine what sample size we would need to get a power of 80% and a alpha of 5%.\nFigure 4.13 shows the relation between sample size, power, alpha and effect size. You can play around with the sliders de determine what sample size you would need to obtain a power of 80% for an effect size of .3.\n\n\n\n\n\n\n\nFigure 4.13: How does test power depend on effect size, type of test, significance level, and sample size? Sampling distributions of the sample mean under the null hypothesis (H0, left-hand curve) and under the assumed true value of the population mean (H1, right-hand curve) for a one-sample t test.\n\n\n\nFor continuous sample statistics, we choose an alpha level, and we can see the critical value in the null distribution. The alpha level of 5% is the area under the curve of the null distribution on the outside of the critical values. The power is the area under the alternative distribution that is outside the critical values.\nThe reasoning is again the same as in the discrete case, when we use categorical sample statistics. We first determine our desired alpha and power, make sure our sample size is large enough to get the desired power, for our effect size of interest. Then, when we collect our data, we can calculate our test statistic and determine if we can reject the null hypothesis or not, being confident that we will be wrong in our conclusion in 5% of the cases, and that we will be right in 80% of the cases when the alternative hypothesis is actually true.\n\n4.2.11.1 How to determine sample size\nAs stated in Section 4.2.3 about the power of a test, we already considered that we do not know the parameter for the alternative distribution and that we therefore also don’t know the true effect size. We stated that you can make an educated guess about the true effect size based on previous research, theory, or other empirical evidence.\nIn research you can take these assumptions into account by conducting a power analysis. A power analysis is a statistical method to determine the sample size you need to get a desired power for a given effect size.\nIt can be difficult to specify the effect size that we should expect or that is practically relevant. If there is little prior research comparable to our new project, we cannot reasonably specify an effect size and calculate sample size. Though, if there are meta analyses available for your research topic of interest or you have the effect sizes from a few previous studies, you can use programs such as G*Power to calculate the sample size you need to get a desired power for a given effect size. G*Power is a stand alone program that can be downloaded for free from the internet, and is specifically designed to calculate the required sample size for a wide range of statistical tests.\n\nDownload G*Power here\n\nIn G*Power you can specify the test you want to conduct, the effect size you expect, the alpha level you want to use, and the power you want to achieve. G*Power will then calculate the sample size you need to get the desired power for the given effect size.\nFor our candy color example, we can use G*Power to calculate the sample size we need to get a power of 80% for a given effect size of .3.\n\n\n\n\n\n\nFigure 4.14: Power analysis in G*Power for a binomial distribution\n\n\n\nIn Figure 4.14 you can see that for the binomial test we have set the proportion p1 to .5 (\\(H_0\\)) and the proportion p2 (\\(H_A\\)) to .2, indirectly setting the effect size to .3. We have set the alpha level to 5% and the power to 80%. By hitting the calculate button, G*Power will calculate the sample size we need. In this case we need 20 candies in the bag to get a power of 80%. The plot shows exactly the same information as in Figure 4.12, though with lines instead of bars.\nAs mentioned in Section 4.2.7 about the true effect size, the sensitivity of a test is determined by the sample size. The larger the sample size, the more sensitive the test will be. This means that if we want to detect a small effect size, we need a large sample size. If we want to detect a large effect size, we can suffice with a smaller sample.\nTry to determine what sample size you would need using Figure 4.13, if you would want to detect an effect size of .2, .5 or .8 with a power of 80% and an alpha level of 5%. You can see that the sample size ranges from 197 to about 15 for these effect sizes.\nSomething to consider is that with extremely large sample sizes you will very easily find significant results. Even if these results are not practically relevant. This is why it is important to determine the sample size you need before you start collecting data.\n\n\n\n4.2.12 One-Sided and Two-Sided Tests\nAs was explained in Section 4.1.2, the alternative hypothesis can be one-sided or two-sided. The choice between a one-sided or two-sided test is based on the research question. In our media literacy example, we could have a one-sided alternative hypothesis that the average media literacy is below 5.5. This would be the case if we hypothesize that children on average score very low on media literacy. We could also have a different hypothesis, that a media literacy intervention program will increase media literacy. Both would be a one-sided alternative hypothesis. We could also have no idea about the media literacy of children, and just want to know if children score below or above 5.5 on media literacy. This would be a two-sided alternative hypothesis. Equation 4.4 formalizes these different hypothesis.\n\\[\n\\begin{eqnarray*}\n\\text{Two-sided} \\\\\nH_{A} & : \\theta & \\neq 5.5 \\\\\n\\text{One-sided} \\\\\nH_{A} & : \\theta & &lt; 5.5 \\\\\nH_{A} & : \\theta & &gt; 5.5 \\\\\n\\end{eqnarray*}\n\\tag{4.4}\\]\nIn null hypothesis significance testing, testing one or two-sided has some consequences for the critical values. In a two-sided test, the critical values are on both sides of the null hypothesis value. In a one-sided test, the critical value is only on one side of the null hypothesis value. If we are using an alpha significance level of 5%, the critical value for a two-sided test results in 2.5% on both sides, while for a one sided test the 5% would only be on one side of the null distribution.\n\n\n\n\n\n\n\nFigure 4.15: One-sided and two-sided tests of a null hypothesis.\n\n\n\nIn the right-sided test of the media literacy hypothesis, the researcher is not interested in demonstrating that average media literacy among children can be lower than 5.5. She only wants to test if it is above 5.5, because an average score above 5.5 indicates that the intervention worked.\nIf it is deemed important to note values well over 5.5 as well as values well below 5.5, the alternative hypotheses should be two-sided. Then, a sample average well below 5.5 would also have resulted in a rejection of the null hypothesis.\nFigure 4.15 shows the \\(H_0\\) distribution of the sample mean 5.5. The dark blue areas represent the 5% probability for a two-sided te st, 2.5% on either side. The light blue areas represent the 5% probability for a one-sided (right-sided) test. The critical value for the one-sided test is 7.8, and the critical values for the two-sided test are 2.9 and 8.1. The critical value is the value that separates the rejection region from the non-rejection region. Where the rejection region are the values on the x-axis that are on the outside of the critical value.\nYou can take a sample and see the result of the sample in the figure. You can then determine if the sample mean is significant at a 5% significance level for a right-sided test, and a two-sided test.\n\n4.2.12.1 From one-sided to two-sided p values and back again\nStatistical software like SPSS usually reports either one-sided or two-sided p values. What if a one-sided p value is reported but you need a two-sided p value or the other way around?\nIn Figure 4.16, the sample mean is 3.9 and we have .015 probability of finding a sample mean of 3.9 or less if the null hypothesis is true that average media literacy is 5.5 in the population. This probability is the surface under the curve to the left of the solid red line representing the sample mean. It is the one-sided p value that we obtain if we only take into account the possibility that the population mean can be smaller than the hypothesized value. We are only interested in the left tail of the sampling distribution.\n\n\n\n\n\n\n\nFigure 4.16: Halve a two-sided p value to obtain a one-sided p value, double a one-sided p value to obtain a two-sided p value.\n\n\n\nIn a two-sided test, we have to take into account two different types of outcomes. Our sample outcome can be smaller or larger than the hypothesized population value. The p-value still represents the probability of drawing a random sample with a sample statistic (here the mean) that is as extreme or more extreme than the sample statistics in our current sample. In the one-sided test example described above, more extreme can only mean “even smaller”. In a two-sided test, more extreme means even more distant from the null hypothesis on either end of the sampling distribution.\nIn Figure 4.16, you can see tha sample meen as indicated by the solid red line. The dotted red line is the mirror image of the sample mean on the other side of the hypothesized population mean. When testing two-sided, we not only consider the sample mean, but also its mirror opposite. The two-sided p value is the probability of finding a sample mean as extreme or more extreme than the sample mean in the sample, and also its mirror opposite. Hence, the two-sided p value is the sum of the probabilities for both the left tail and the right tail of the sampling distribution. As these tails are symmetrical, the two-sided p value is twice the one-sided p value.\nSo, if our statistical software tells us the two-sided p value and we want to have the one-sided p value, we can simply halve the two-sided p value. The two-sided p value is divided equally between the left and right tails. If we are interested in just one tail, we can ignore the half of the p value that is situated in the other tail.\nBe careful if you divide a two-sided p value to obtain a one-sided p value. If your left-sided test hypothesizes that average media literacy is below 5.5 but your sample mean is well above 5.5, the two-sided p value can be below .05. But your left-sided test can never be significant because a sample mean above 5.5 is fully in line with the null hypothesis. Check that the sample outcome is at the correct side of the hypothesized population value.\nYou might have already realized that if you use the same alpha criterion for rejecting the null hypothesis (e.g. 5%) as is usually done, it is easier to reject a one-sided null hypothesis, because the entire 5% of most extreme samples is located on one side of the distribution, whereas a two-sided null hypothesis would require us to highlight 2.5% of samples in the lower tail of the distribution and 2.5% in the upper tail. To avoid making too many unnecessary type 1 errors (Section 4.7.3), we should always have a good theoretical justification for using one-sided null hypotheses and tests.\nOne final warning: Two-sided tests are only relevant if the probability distribution that you are using to test your hypothesis is symmetrical. If you are using a non-symmetrical distribution, such as the chi-square distribution, or the F-distribution you should always use a one-sided test. This is because such distributions do not have negative values, and the critical values are always on the right side of the distribution. As the F-value, for example, represents a signal to noise ratio, it can never be negative.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#sec-reporting",
    "href": "04-hypothesis.html#sec-reporting",
    "title": "4  Hypothesis testing",
    "section": "4.3 Reporting test results",
    "text": "4.3 Reporting test results\n\n4.3.1 Reporting to fellow scientists\nFellow scientists need to be able to see the precise statistical test results. According to the APA guidelines, we should report the test statistic, the associated degrees of freedom (if any), the value of the test statistic, the p value of the test statistic, and the confidence interval (if any). APA requires a particular format for presenting statistical results and it demands that the results are included at the end of a sentence.\nThe statistical results for a t test on one mean, for example, would be:t (67) = 2.73, p = .004, 95% CI [4.13, 4.87].\n\nThe degrees of freedom are between parentheses directly after the name of the test statistic. Chi-squared tests add sample size to the degrees of freedom, for instance: chi-squared (12, N = 89) = 23.14, p = .027.\nThe value of the test statistic is 2.73 in this example.\nThe p value is .004. Note that we report all results with two decimal places except probabilities, which are reported with three decimals. We are usually interested in small probabilities—less than .05—so we need the third decimal here. If SPSS rounds the p value to .000, report: p &lt; .001. Add (one-sided) after the p value if the test is one-sided.\nThe 95% confidence interval is 4.13 to 4.87, so we 95% confident that the population mean lies within the CI. Add (bootstrapped) after the confidence interval if the confidence interval is bootstrapped.\n\nNot all tests produce all results reported in the example above. For example, a z test does not have degrees of freedom and F or chi-squared tests do not have confidence intervals. Exact tests or bootstrap tests usually do not have a test statistic. Just report the items that your statistical software produces, and give them in the correct format.\n\n\n4.3.2 Reporting to the general reader\nFor fellow scientists and especially for the general reader, it is important to read an interpretation of the results that clarifies both the subject of the test and the test results. Make sure that you tell your reader who or what the test is about:\n\nWhat is the population that you investigate?\n\nWhat are the variables?\n\nWhat are the values of the relevant sample statistics?\n\nWhich comparison(s) do you make?\n\nAre the results statistically significant and, if so, what are the estimates for the population?\n\nIf the results are statistically significant, how large are the differences or associations?\n\nA test on one proportion, for example, the proportion of all households reached by a television station, could be reported as follows:\n\n\n\n\n\n\n“The television station reaches significantly and substantially (61%) more than half of all households in Greece in 2012, z = 4.01, p &lt; .001.”\n\n\n\nThe interpretation of this test tells us the population (“all households in Greece”), the variable (“reaching a household”) and the sample statistic of interest (61%, indicating a proportion). It tells us that the result is statistically significant, which a fellow scientist can check with the reported p value.\nFinally, the interpretation tells us that the difference from .5 is substantial. Sometimes, we can express the difference in a number, which is called the effect size, and give a more precise interpretation (see Section 4.2.3 for more information).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#sec-test-selection",
    "href": "04-hypothesis.html#sec-test-selection",
    "title": "4  Hypothesis testing",
    "section": "4.4 Statistical test selection",
    "text": "4.4 Statistical test selection\nKnowing what statistical test fits your research question is crucial for the success of your research. If you do not know what test to apply or even choose the wrong test, you may draw the wrong conclusions. This can lead to a waste of time and resources, and it can even lead to harm if the wrong conclusions are used to make decisions.\nStatistics such as means, proportions, variances, and correlations are calculated on variables. For translating a research hypothesis into a statistical hypothesis, the researcher has to recognize the dependent and independent variables addressed by the research hypothesis and their variable types. The main distinction is between dichotomies (two groups), (other) categorical variables (three or more groups), and numerical variables. Once you have identified the variables, the flow chart in Figure 4.17 helps you to identify the right statistical test.\n\n\n\n\n\n\n\n\nFigure 4.17: Flow chart for selecting a test in SPSS.\n\n\n\n\n\nYou use the flow chart by identifying the type of your dependent variable (categorical or numerical) and the number and type of your independent variable (categorical or numerical). The flow chart then guides you to the right statistical test.\nConsider the following example. You want to measure the difference in media literacy between man and women, and want to control for age. You measure media literacy on a scale from 1 to 7, and age in years. You have a numerical dependent variable (media literacy) and a categorical independent (biological sex), and a numerical independent variable (age). As your dependent variable is numerical, you follow the flow chart to the right. As you have two independent variables, you follow the flow chart to the right to indicate that you have both a categorical and numerical independent variable. The flow chart guides you to the F test multiple regression model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#sec-null-ci",
    "href": "04-hypothesis.html#sec-null-ci",
    "title": "4  Hypothesis testing",
    "section": "4.5 Confidence Intervals to test hypotheses",
    "text": "4.5 Confidence Intervals to test hypotheses\nIn Chapter 3, we learned how to calculate a confidence interval for the population mean. We also learned that the confidence interval is a range of values that is likely to contain the true population mean. We learned that the true population mean falls within the confidence in 95% of a hundred samples. We can use this knowledge to test hypotheses. If we, again, have the hypothesis that the average media literacy in the population is 5.5, we can use the confidence interval to test this hypothesis. If we draw a sample and calculate the confidence interval, we can see if the hypothesized population mean falls within the confidence interval. If it does, we can conclude that we are probably right. If it does not, we can conclude that we are probably wrong.\nWith the hypothesis that we can improve media literacy through some intervention program, we can also use the confidence interval to test this hypothesis. If the lower bound of the confidence interval is higher than 5.5, we can conclude that the intervention program works.\n\n4.5.1 Estimation in addidion to NHST\nFollowing up on a report commissioned by the American Psychological Association APA (Wilkinson 1999), the 6th edition of the Publication Manual of the American Psychological Association recommends reporting and interpreting confidence intervals in addition to null hypothesis significance testing.\nEstimation is becoming more important: Assessing the precision of our statements about the population rather than just rejecting or not rejecting our hypothesis about the population. This is an important step forward and it is easy to accomplish with your statistical software.\n\n\n\n\n\n\n\n\nFigure 4.18: What is the most sensible interpretation of the results represented by the confidence interval for the regression coefficient, which estimates brand awareness from campaign exposure?\n\n\n\n\n\nFigure 4.18 shows six confidence intervals for a population value, for instance, the effect of exposure to advertisements on brand awareness, and the sample result as point estimate (dot). The horizontal axis is labeled by the size of the effect: the difference between the effect in the sample and the absence of an effect according to the null hypothesis.\nA confidence interval shows us whether or not our null hypothesis must be rejected. The rule is simple: If the value of the null hypothesis is within the confidence interval, the null hypothesis must not be rejected. By the way, note that a confidence interval allows us to test a null hypothesis other than the nil (Section 3.5.1). If we hypothesize that the effect of exposure on brand awareness is 0.1, we reject this null hypothesis if the confidence interval of the regression coefficient does not include 0.1. SPSS usually tests nil hypotheses, this can be adjusted for some tests, but not all. Though it is almost always possible to visualize the confidence interval in graphs created in SPSS.\nConfidence intervals allow us to draw a more nuanced conclusion. A confidence interval displays our uncertainty about the result. If the confidence interval is wide, we are quite uncertain about the true population value. If a wide confidence interval includes the null hypothesis, but the value specified in the null hypothesis is located near one of its boundaries (e.g., Confidence Interval D in Figure 4.18), we do not reject the null hypothesis. However, it still is plausible that the population value is substantially different from the hypothesized value.\nFor example, we could interpret Confidence Interval D in Figure 4.18 in the following way:\n\nThe effect of exposure to advertisements on brand awareness is of moderate size in the sample (b* = 0.28). It is, however, not statistically significant, t (23) = 1.62, p = .119, 95% CI [-0.1, 3.2], meaning that we are not sufficiently confident that there is a positive effect in the population. It is important to note that the sample is small (N = 25– this number is not included in the figure–), so test power is probably low, meaning that it is difficult to reject a false null hypothesis. On the basis of the confidence interval we conclude that the effect can be weak and negative, but the plausible effects are predominantly positive, including strong positive effects. One additional daily exposure may decrease predicted brand awareness by 0.1, but it may also increase brand awareness by up to 3.2 points on a scale from 1 (unaware of the brand) to 7 (highly aware of the brand). The latter effect is substantial: A single additional exposure to advertisements would lead to a substantial change in brand awareness.\n\nWe should report that the population value seems to be larger (smaller) than specified in the null hypothesis but that we do not have sufficient confidence in this result because the test is not statistically significant. This is better than reporting that there is no difference because the statistical test is not significant.\n\n\n\n\n\n\nThe fashion of speaking of a null hypothesis as “accepted when false”, whenever a test of significance gives us no strong reason for rejecting it, and when in fact it is in some way imperfect, shows real ignorance of the research workers’ attitude, by suggesting that in such a case he has come to an irreversible decision.\nThe worker’s real attitude in such a case might be, according to the circumstances:\n\n“The possible deviation from truth of my working hypothesis, to examine which the test is appropriate, seems not to be of sufficient magnitude to warrant any immediate modification.”\n\nOr it might be:\n\n“The deviation is in the direction expected for certain influences which seemed to me not improbable, and to this extent my suspicion has been confirmed; but the body of data available so far is not by itself sufficient to demonstrate their reality.”\n\n(Fisher 1955: 73)\nSir Ronald Aylmer Fisher, Wikimedia Commons\n\n\n\nIn a similar way, a very narrow confidence interval including the null hypothesis (e.g., Confidence Interval B in Figure 4.18) and a very narrow confidence interval near the null hypothesis but excluding it (e.g., Confidence Interval C in Figure 4.18) should not yield opposite conclusions because the statistical test is significant in the second but not in the first situation. After all, even for the significant situation, we know with high confidence (narrow confidence interval) that the population value is close to the hypothesized value.\nFor example, we could interpret Confidence Interval C in Figure 4.18 in the following way:\n\nThe effect of exposure to advertisements on brand awareness is statistically significant, t (273) = 3.67, p &lt; .001, 95% CI [0.1, 0.5]. On the basis of the confidence interval we are confident that the effect is positive but small (maximum b* = 0.05). One additional daily exposure increases predicted brand awareness by 0.1 to 0.5 on a scale from 1 (unaware of the brand) to 7 (highly aware of the brand). We need a lot of additional exposure to advertisements before brand awareness changes substantially.\n\nIn addition, it is good practice to include confidence intervals in research report figures. Especially in figures depicting moderation (see Section 5.3.1), confidence intervals can help to interpret where differences between multiple groups are likely to occur.\n\nUsing confidence intervals in this way, we avoid the problem that statistically non-significant effects are not published. Not publishing non-significant results, either because of self-selection by the researcher or selection by journal editors and reviewers, offers a misleading view of research results.\nIf results are not published, they cannot be used to design new research projects. For example, effect sizes that are not statistically significant are just as helpful to determine test power and sample size as statistically significant effect sizes. An independent variable without statistically significant effect may have a significant effect in a new research project and should not be discarded if the potential effect size is so substantial that it is practically relevant. Moreover, combining results from several research projects helps making more precise estimates of population values, which brings us to meta-analysis.\n\n\n4.5.2 Bootstrapped confidence intervals\nUsing the confidence interval is the easiest and sometimes the only way of testing a null hypothesis if we create the sampling distribution with bootstrapping. For instance, we may use the median as the preferred measure of central tendency rather than the mean if the distribution of scores is quite skewed and the sample is not very large. In this situation, a theoretical probability distribution for the sample median is not known, so we resort to bootstrapping.\nBootstrapping creates an empirical sampling distribution: a lot of samples with a median calculated for each sample. A confidence interval can be created from this sampling distribution (see Section 3.5.2). If our null hypothesis about the population median is included in the 95% confidence interval, we do not reject the null hypothesis. Otherwise, we reject it. We will encounter this in Chapter 9 about mediation, where indirect effects are tested with bootstrapped confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#sec-bayesian-hypothesis-testing",
    "href": "04-hypothesis.html#sec-bayesian-hypothesis-testing",
    "title": "4  Hypothesis testing",
    "section": "4.6 Bayesian hypothesis testing",
    "text": "4.6 Bayesian hypothesis testing\nA more radical way of including previous knowledge in statistical inference is Bayesian inference. Bayesian inference regards the sample that we draw as a means to update the knowledge that we already have or think we have on the population. Our previous knowledge is our starting point and we are not going to just discard our previous knowledge if a new sample points in a different direction, as we do when we reject a null hypothesis.\nThink of Bayesian inference as a process similar to predicting the weather. If I try to predict tomorrow’s weather, I am using all my weather experience to make a prediction. If my prediction turns out to be more or less correct, I don’t change the way I predict the weather. But if my prediction is patently wrong, I try to reconsider the way I predict the weather, for example, paying attention to new indicators of weather change.\nBayesian inference uses a concept of probability that is fundamentally different from the type of inference presented in previous chapters, which is usually called frequentist inference. Bayesian inference does not assume that there is a true population value. Instead, it regards the population value as a random variable, that is, as something with a probability.\nAgain, think of predicting the weather. I am not saying to myself: “Let us hypothesize that tomorrow will be a rainy day. If this is correct, what is the probability that the weather today looks like it does?” Instead, I think of the probability that it will rain tomorrow. Bayesian probabilities are much more in line with our everyday concept of probability than the dice-based probabilities of frequentist inference.\nRemember that we are not allowed to interpret the 95% confidence interval as a probability (Chapter 3)? We should never conclude that the parameter is between the upper and lower limits of our confidence interval with 95 per cent probability. This is because a parameter does not have a probability in frequentist inference. The credible interval (sometimes called posterior interval) is the Bayesian equivalent of the confidence interval. In Bayesian inference, a parameter has a probability, so we are allowed to say that the parameter lies within the credible interval with 95% probability. This interpretation is much more in line with our intuitive notion of probabilities.\nBayesian inference is intuitively appealing but it has not yet spread widely in the social and behavioral sciences. Therefore, I merely mention this strand of statistical inference and I refrain from giving details. Its popularity, however, is increasing, so you may come in contact with Bayesian inference sooner or later.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#sec-critical-discussion",
    "href": "04-hypothesis.html#sec-critical-discussion",
    "title": "4  Hypothesis testing",
    "section": "4.7 Critical Discussion",
    "text": "4.7 Critical Discussion\n\n4.7.1 Criticisms of Null Hypothesis Significance Testing\nIn null hypothesis significance testing, we totally rely on the test’s p value. If this value is below .05 or another significance level we specified for our test, we reject the null hypothesis and we do not reject it otherwise. Based on this decision, we draw a conclusion about the effect in the population. Is this a wise thing to do? Watch Video 4.1.\n\n\n\n\n\n\nVideo 4.1: The dance of the p values by Geoff Cumming.\n\n\n\nAs the video indicates, only focusing on the p-value can provide wildly misleading results. Specially with low sample sizes, you can find significant effects that do not correspond with results we find while using confidence intervals. Blindly following the p-value mantra is considered to be bad practice.\nI hope that by now, Section 4.2 has prepared you to critically reflect on this video. In his simulation, Cumming correctly states that “studies have found that in many areas of Psychology, the median size effect is .5”. Though blaming the p-value instead of questionable research practices is a bit misleading. We have learned that we should strive for a power of 80% and set our sample size accordingly. Looking at the overlap of the \\(H_0\\) and \\(H_A\\) distributions in the video it is clear to see that both distributions overlap.\nMost criticism of null hypothesis significance testing focuses on the p-value as a decision criterion. This critique is justified when not taking every aspect of the Neyman Pearson approach into consideration. The result has been an enormous amount of under powered studies and a failure to replicate seminal studies from the last decade.\n\n\n4.7.2 Statistical significance is not a measure of effect size\nWhen our sample is small, say a few dozens of cases, the power to reject a null hypothesis is rather small, so it often happens that we retain the null hypothesis even if it is wrong. There is a lot of uncertainty about the population if our sample is small. So we must be lucky to draw a sample that is sufficiently at odds with the null hypothesis to reject it.\nIf our sample is large or very large (a few thousand cases), small differences between what we expect according to our alternative hypothesis can be statistically significant even if the differences are too small to be of any practical value. A statistically significant result does not have to be practically relevant. All in all, statistical significance on it’s own, does not tell us much about the effect in the population.\n\n\n\n\n\n\n\nFigure 4.19: Any effect can be statistically significant.\n\n\n\nIt is a common mistake to think that statistical significance is a measure of the strength, importance, or practical relevance of an effect. In Video 4.1, this mistaken interpretation is expressed by the type of sound associated with a p value: the lower the p value of the test, the more joyous the sound.\nIt is wrong to use statistical significance as a measure of strength or importance. In a large sample, even irrelevant results can be significant and in small samples, as demonstrated in the video, results can sometimes be significant and sometimes be insignificant. We have learned in Section 4.1 that our decision is a binary one, so, never forget:\n\n\n\n\n\n\nA statistically significant result ONLY means that the null hypothesis must be rejected.\n\n\n\nIf we want to say something about the magnitude of an effect in the population, we should use effect size. All we have is the effect size measured in our sample and a statistical test usually telling us whether or not we should reject the null hypothesis that there is no effect in the population.\nIf the statistical test is significant, we conclude that an effect probably exists in the population. We may use the effect size in the sample as a point estimate of the population effect. This effect size should be at the core of our interpretation. Is it large (strong), small (weak), or perhaps tiny and practically irrelevant?\nIf the statistical test is not significant, it is tempting to conclude that the null hypothesis is true, namely, that there is no effect in the population. If so, we do not have to interpret the effect that we find in our sample. But this is not right. Finding insufficient evidence for rejecting the null hypothesis does not prove that the null hypothesis is true. Even if the null hypothesis is false, we can draw a sample that does not reject the null hypothesis.\nIn a two-sided significance test, the null hypothesis specifies one particular value for the sample outcome. If the outcome is continuous, for instance, a mean or regression coefficient, the null hypothesis can hardly ever be true, strictly speaking. The true population value is very likely not exactly the same as the hypothesized value. It may be only slightly different, but it is different.\n\n\n\n\n\n\nA statistically non-significant result does NOT mean that the null hypothesis is true.\n\n\n\nWhen we evaluate a p value, we had better take into account the probability that we correctly reject the null hypothesis, which is test power. If test power is low, as it often is in social scientific research with small effect sizes and not very large samples, we should realize that there can be an interesting difference between true and hypothesized population values even if the test is not statistically significant. Though, in recent years, the focus on pre-registration of required sample sizes has increased the power in many studies.\nWith low power, we have high probability of not rejecting a false null hypothesis (type II error) even if the true population value is quite different from the hypothesized value. For example, a small sample of candies drawn from a population with average candy weight of 3.0 grams may not reject the null hypothesis that average candy weight is 2.8 grams in the population. The non-significant test result should not make us conclude that there is no interesting effect. The test may not pick up substantively interesting effects.\nIn contrast, if our test has very high power, we should expect effects to be statistically significant, even tiny effects that are totally irrelevant from a substantive point of view. For example, an effect of exposure on attitude of 0.01 on a 10-point scale is likely to be statistically significant in a very large sample but it is probably substantively uninteresting.\nIn a way, a statistically non-significant result is more interesting than a significant result in a test with high power. If it is easy to get significant results even for small effect sizes (high power), a non-significant result probably indicates that the true effect in the population is very small. In this situation, we are most confident that the effect is close to zero or absent in the population.\nBy now, however, you understand that test power is affected by sample size. You should realize that null hypotheses are easily rejected in large samples but they are more difficult to reject in small samples. A significant test result in a small sample suggests a substantive effect in the population but not necessarily so in a large sample. A non-significant test result in a small sample does not mean that the effect size in the population is too small to be of interest. Don’t let your selection of interesting results be guided only by statistical significance.\n\n\n4.7.3 Capitalization on Chance\nThe relation between null hypothesis testing and confidence intervals (Section 4.5) may have given the impression that we can test a range of null hypotheses using just one sample and one confidence interval. For instance, we could simultaneously test the null hypotheses that average media literacy among children is 5.5, 4.5, or 3.5. Just check if these values are inside or outside the confidence interval and we are done, right?\nThis impression is wrong. The probabilities that we calculate using one sample assume that we only apply one test to the data. If we test the original null hypothesis that average media literacy is 5.5, we run a risk of five per cent to reject the null hypothesis if the null hypothesis is true. The significance level is the probability of making a type I error (Section 4.2.1).\nIf we apply a second test to the same sample, for example, testing the null hypothesis that average media literacy is 4.5, we again run this risk of five per cent. The probability of not rejecting a true null hypothesis is .95, so the probability of not rejecting two true null hypotheses is .95 * .95 = 0.9025. The risk of rejecting at least one true null hypothesis in two tests is 1 - 0.9025 = .0975. This risk is dramatically higher than the significance level (.05) that we want to use. The situation becomes even worse if we do three or more tests on the same sample.\nThe phenomenon that we are dealing with probabilities of making type I errors that are higher (inflated type I errors) than the significance level that we want to use, is called capitalization on chance. Applying more than one test to the same data is one way to capitalize on chance. If you do a lot of tests on the same data, you are likely to find some statistically significant results even if all null hypotheses are true.\n\n4.7.3.1 Example of capitalization on chance\nThis type of capitalization on chance may occur, for example, if we want to compare average media literacy among three groups: second, fourth, and sixth grade students. We can use a t test to test if average media literacy among fourth grade students is higher than among second grade students. We need a second t test to compare average media literacy of sixth grade students to second grade students, and a third one to compare sixth to fourth grade students.\nIf we execute three tests, the probability of rejecting at least one true null hypothesis of no difference is much higher than five per cent if we use a significance level of five per cent for each single t test. In other words, we are more likely to obtain at least one statistically significant result than we want.\n\n\n4.7.3.2 Correcting for capitalization on chance\nWe can correct in several ways for this type of capitalization on chance; one such way is the Bonferroni correction. This correction divides the significance level that we use for each test by the number of tests that we do. In our example, we do three t tests on pairs of groups, so we divide the significance level of five per cent by three. The resulting significance level for each t test is .0167. If a t test’s p value is below .0167, we reject the null hypothesis, but we do not reject it otherwise.\nThe Bonferroni correction is a rather stringent correction. However, it has a simple logic that directly links to the problem of capitalization on chance. Therefore, it is a good technique to help understand the problem, which is the main goal we want to attain, here. We will skip better, but more complicated alternatives to the Bonferroni correction.\nIt has been argued that we do not have to apply a correction for capitalization on chance if we specify a hypothesis beforehand for each test that we execute. Formulating hypotheses does not solve the problem of capitalization on chance. The probability of rejecting at least one true null hypothesis still increases with the number of tests that we execute. If all hypotheses and associated tests are reported (as recommended in Wasserstein and Lazar 2016), however, the reader of the report can evaluate capitalization on chance. If one out of twenty tests at five per cent significance level turns out to be statistically significant, this is what we would expect based on chance if all null hypotheses are true. The evidence for rejecting this null hypothesis is less convincing than if only one test was applied and that test turned out to be statistically significant.\n\n\n\n4.7.4 What If I Do Not Have a Random Sample?\nIn our approach to statistical inference, we have always assumed that we have drawn a random sample. That in our research we truly sample from the population of interest, not a subset or convenience sample. What if we do not have a random sample? Can we still estimate confidence intervals or test null hypotheses?\nIf you carefully read reports of scientific research, you will encounter examples of statistical inference on non-random samples or data that are not samples at all but rather represent an entire population, for instance, all people visiting a particular web site. Here, statistical inference is clearly being applied to data that are not sampled at random from an observable population. The fact that it happens, however, is not a guarantee that it is right.\nWe should note that statistical inference based on a random sample is the most convincing type of inference because we know the nature of the uncertainty in the data, namely chance variation introduced by random sampling. Think of exact methods for creating a sampling distribution. If we know the distribution of candy colours in the population of all candies, we can calculate the exact probability of drawing a sample bag with, for example, 25 per cent of all candies being yellow if we carefully draw the sample at random.\nWe can calculate the probability because we understand the process of random sampling. For example, we know that each candy has the same probability to be included in the sample. The uncertainty or probabilities arise from the way we designed our data collection, namely as a random sample from a much larger population.\nIn summary, we work with an observable population and we know how chance affects our sample if we draw a random sample. We do not have an observable population or we do not know the workings of chance if we want to apply statistical inference to data that are not collected as a random sample. In this situation, we have to substantiate the claim that our data set can be treated as a random sample. For example, we can argue that the data set is a random sample from a population of all people who visit a particular web site. Or that we do not want to infer to the entire population but only to a subset.\n\n\n4.7.5 Specifying hypotheses afterwards\nAs journals favor research results that are statistically significant, researchers may be tempted to first look at the data and then formulate a hypothesis. It is easy to specify a null hypothesis that will be rejected. If we first look at the data and then specify a null hypothesis, we can always find a null hypothesis that is rejected. This is called HARKing (Hypothesizing After the Results are Known). This is plain cheating and it must be avoided at all times. The temptation arises because career opportunities are better for researchers that have high citation indices and non significant findings are less likely to be published and cited.\nNowadays, many journals require that researchers specify their hypotheses before they collect data. This is called pre-registration. Pre-registration is a good way to avoid HARKing. If we specify our hypotheses before we collect data, we cannot be accused of HARKing. We can still test other hypotheses than the ones we pre-registered, but we should report that we did so.\n\n\n4.7.6 Replication\nReplication refers to the process of repeating research to determine whether the results of a previous study are the same. Replication is a cornerstone of the scientific method. In the Nayman-Pearson decision theory, we have seen that in order to accurately determine the true population value, the true effect size, we can use the observed effect size from multiple studies. Through meta analysis, we can combine the results of multiple studies to get a more precise estimate of the population value / true effect size. To enable meta-analysis, the same effects have to be studied in a reasonably comparable manner in multiple studies (i.e. they need to be replicated). The gold standard is a direct replication which exactly repeats all procedures and measures used in the original study. More often, you might see conceptual replications which study the same effect with slightly different procedures or in a slightly different population. Conceptual replications might still add or detract from our confidence about the existence or size of a given effect but usually leave us uncertain about whether any differences in effect size between the original study and conceptual replication are due to the differences between the studies. In any case, making bold claims based on a single study is risky. If we have a single study that shows a significant effect, we should be cautious in interpreting the results. We should wait for a replication of the study to confirm the results.\nThough Bayesian statistics allows to incorporate prior knowledge in the analysis, researchers do actively need to replicate to incorporate new data in the analysis. Running a Bayesian analysis on a single study suffers from the same problems as running a frequentist analysis on a single study. Replication is therefore important in both statistical paradigms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "04-hypothesis.html#take-home-points",
    "href": "04-hypothesis.html#take-home-points",
    "title": "4  Hypothesis testing",
    "section": "4.8 Take home points",
    "text": "4.8 Take home points\nHypothesis:\n\nStatistical inference includes estimation and hypothesis testing.\nHypothesis testing involves rejecting or not rejecting a hypothesis based on data.\n\nNull Hypothesis Significance Testing:\n\nInvolves null and alternative hypotheses, significance level, p-values, and test power.\nImportance of sample size and effect sizes.\n\nReporting Test Results:\n\nEmphasizes clarity and transparency.\nReport test statistics, p-values, effect sizes, and confidence intervals.\n\nStatistical Test Selection:\n\nChoose tests based on data type, groups compared, and study design.\nIncludes decision-making frameworks and examples.\n\nConfidence Intervals:\n\nProvide a range of plausible values for population parameters.\nCan be used to infer hypotheses, with bootstrapped intervals as an alternative.\n\nBayesian Hypothesis Testing:\n\nBayesian approach updates prior beliefs with data.\nUtilizes prior, likelihood, and posterior distributions for decision-making.\n\nCritical Discussion:\n\nExamines limitations of null hypothesis significance testing.\nDiscusses misinterpretation of p-values, overemphasis on significance, and publication bias.\n\n\n\n\n\nCohen, Jacob. 1969. Statistical Power Analysis for the Behavioral Sciences. San Diego, CA: Academic Press.\n\n\nCumming, Geoff. 2012. Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis. New York: Routledge.\n\n\nde Groot, Adrianus Dingeman. 1969. Methodology: Foundations of Inference and Research in the Behavioral Sciences. Book, Whole. The Hague: Mouton.\n\n\nFisher, Ronald Aylmer. 1955. “Statistical Methods and Scientific Induction.” Journal of the Royal Statistical Society.Series B (Methodological) 17 (1): 69–78. http://www.jstor.org/stable/2983785.\n\n\nLehmann, E. L. 1993. “The Fisher, Neyman-Pearson Theories of Testing Hypotheses: One Theory or Two?” Journal of the American Statistical Association 88 (424): 1242–49. https://doi.org/10.1080/01621459.1993.10476404.\n\n\nO’Keefe, Daniel J. 2007. “Brief Report: Post Hoc Power, Observed Power, a Priori Power, Retrospective Power, Prospective Power, Achieved Power: Sorting Out Appropriate Uses of Statistical Power Analyses.” Communication Methods and Measures 1 (4): 291–99. https://doi.org/10.1080/19312450701641375.\n\n\nSawilowsky, Shlomo. 2009. “New Effect Size Rules of Thumb.” Journal of Modern Applied Statistical Methods 8 (2). https://doi.org/10.22237/jmasm/1257035100.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA Statement on p-Values: Context, Process, and Purpose.” The American Statistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWilkinson, Leland. 1999. “Statistical Methods in Psychology Journals: Guidelines and Explanations.” American Psychologist 54 (8): 594.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "07-anova.html",
    "href": "07-anova.html",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Summary\nWatch the micro lecture (Video 5.1) on moderation with analysis of variance for an overview of the chapter.\nImagine you are a health communication researcher. Your goal is to identify the most effective way of persuading people to accept an important new vaccine once it becomes available. Based on theory, you created three versions of a communication campaign aiming to increase vaccine acceptance: The first version of the campaign uses autonomy-supportive language, that is it addresses people in a way that respects their freedom to choose, rather than attempting to pressure or force them to accept the vaccine. The second version of the campaign uses controlling language, that is it attempts to pressure or command people to accept the vaccine, for instance by using threats or guilt-appeals. The third version of the campaign uses neutral language that is neither explicitly autonomy-supportive nor controlling. Version 3 is meant to serve as a control condition. Which communication strategy is most effective?\nYou suspect that the answer to this question might depend on the characteristics of the person who is exposed to the campaign, such as their health literacy. Health literacy is the extent to which one is skilled in finding, understanding, and using health information to make good decisions about their health. Are those with higher health literacy more likely to be persuaded by a different communication strategy than those with low health literacy?\nTo find out, you and your team ran an experiment, randomly assigning participants who vary in health literacy (high vs. low) to be exposed to one of the three campaigns. Vaccine acceptance was measured as the main dependent variable after exposure to a randomly assigned campaign.\nTo identify the most effective campaign, we first need to compare the outcome scores (average vaccine acceptance) across more than two groups (participants who saw the neutral campaign, the autonomy-supportive campaign or the controlling campaign). To this end, we use analysis of variance. The null hypothesis tested in analysis of variance states that all groups have the same average outcome score in the population.\nThis null hypothesis is similar to the one we test in an independent-samples t test for two groups. With three or more groups, we must use the variance of the group means (between-groups variance) to test the null hypothesis. If the between-groups variance is zero, all group means are equal.\nIn addition to between-groups variance, we have to take into account the variance of outcome scores within groups (within-groups variance). Within-groups variance is related to the fact that we may obtain different group means even if we draw random samples from populations with the same means. The ratio of between-groups variance over within-groups variance gives us the F test statistic, which has an F distribution.\nDifferences in average outcome scores for groups on one independent variable (usually called factor in analysis of variance) are called a main effect. A main effect represents an overall or average effect of a factor. If we have only one factor in our model, for instance, the language used in a vaccination campaign, we apply a one-way analysis of variance. With two factors, we have a two-way analysis of variance, and so on.\nIn our example, we are interested in a second independent variable, namely health literacy. With two or more factors, we can have interaction effects in addition to main effects. An interaction effect is the joint effect of two or more factors on the dependent variable. An interaction effect is best understood as different effects of one factor across different groups on another factor. For example, autonomy supportive language may increase vaccine acceptance for people with high health literacy but controlling language might work best for people with low health literacy.\nThe phenomenon that a variable can have different effects for different groups on another variable is called moderation. We usually think of one factor as the predictor (or independent variable) and the other factor as the moderator. The moderator (e.g., health literacy) changes the effect of the predictor (e.g., campaign style) on the dependent variable (e.g., vaccine acceptance).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "07-anova.html#different-means-for-three-or-more-groups",
    "href": "07-anova.html#different-means-for-three-or-more-groups",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "5.1 Different Means for Three or More Groups",
    "text": "5.1 Different Means for Three or More Groups\nCommunication scientists have shown that campaign messages are more likely to be effective if they are designed based on theoretical insights (Fishbein and Cappella 2006). For instance, Protection Motivation Theory (Rogers 1975) suggests that controlling language including threats can be persuasive by motivating people to take action to avoid harm and Self-Determination Theory (Deci and Ryan 2013) supports the idea that autonomy-supportive language exerts impact by fostering a sense of choice and personal motivation.\nImagine that we want to test if using theory to design language used in a campaign makes a difference to people’s vaccine acceptance. We will be using either autonomy supportive language or controlling language in the theory-based campaigns, and we will include a campaign with neutral language as a control condition.\nLet us design an experiment to investigate the effects of the language used in a campaign (campaign style). We sample a number of people (participants) and then randomly assign each participant to the campaign with autonomy supportive language, the campaign with controlling language, or the campaign with neutral language (control group). Our independent variable campaign style is a factor with three experimental conditions (autonomy supportive, controlling, neutral).\nOur dependent variable is vaccine acceptance, a numeric scale from 1 (“Would definitely refuse to get the vaccine”) to 10 (“Would definitely get the vaccine as soon as possible”). We will compare the average outcome scores among groups. If groups with autonomy supportive or controlling language have a systematically higher average vaccine acceptance than the group with neutral language, we conclude that using theory to design campaign language has a positive effect on vaccine acceptance.\nIn statistical terminology, we have a categorical independent variable (or: factor) and a numerical dependent variable. In experiments, we usually have a very limited set of treatment levels, so our independent variable is categorical. Analysis of variance was developed for this kind of data (Fisher 1919), so it is widely used in the context of experiments.\n\n5.1.1 Mean differences as effects\nFigure 5.1 shows the vaccine acceptance scores for twelve participants in our experiment. Four participants saw a campaign with autonomy supportive language, four saw a campaign with contolling language, and four saw a campaign with neutral language.\n\n\n\n\n\n\n\nFigure 5.1: How do group means relate to effect size?\n\n\n\nA group’s average score on the dependent variable represents the group’s score level. The group averages in Figure 5.1 tell us for which campaign style the average vaccine acceptance is higher and for which campaign style it is lower.\nRandom assignment of participants to experimental groups (here: which campaign is shown) creates groups that are, in theory, equal on all imaginable characteristics except the experimental treatment(s) administered by the researcher. Participants who saw a campaign with autonomy supportive language should have more or less the same average age, knowledge, and so on as participants who saw a campaign with controlling or neutral language. After all, each experimental group is just a random sample of participants.\nIf random assignment was done successfully, differences between group means can only be caused by the experimental treatment (we will discuss this in more detail in Chapter 8). Mean differences are said to represent the effect of experimental treatment in analysis of variance.\nAnalysis of variance was developed for the analysis of randomized experiments, where effects can be interpreted as causal effects. Note, however, that analysis of variance can also be applied to non-experimental data. Although mean differences are still called effects in the latter type of analysis, these do not have to be causal effects.\nIn analysis of variance, then, we are simply interested in differences between group means. The conclusion for a sample is easy: Which groups have higher average scores on the dependent variable and which groups have lower scores? A means plot, such as Figure 5.2, aids interpretation and helps communicating results to the reader. On average, participants who saw a campaign with autonomy supportive or controlling language have higher vaccine acceptance than participants who saw a campaign with neutral language. In other words, our two theory-based campaigns were more effective than the neutral control condition.\n\n\n\n\n\n\n\n\nFigure 5.2: A means plot showing that average vaccine literacy is higher in the controlling and autonomy supportive language conditions than in the neutral language condition (Error bars = 95% Confidence Intervals). As a reading instruction, effects of language condition are represented by arrows and dashed lines.\n\n\n\n\n\nEffect size in an analysis of variance refers to the overall differences between group means. We use eta2 as effect size, which gives the proportion of variance in the dependent variable (acceptance of vaccination information) explained or predicted by the group variable (experimental condition).\nThis proportion is informative and precise. If you want to classify the effect size in more general terms, you should take the square root of eta2 to obtain eta. As a measure of association, eta can be interpreted with the following rules of thumb:\n\n0.1 (0 ≤ eta2 &lt; .2) = small or weak effect,\n0.3 (.2 ≤ eta2 &lt; .4) = medium-sized or moderate effect,\n0.5 (.4 ≤ eta2) = large or strong effect.\n\n\n\n5.1.2 Between-groups variance and within-groups variance\nFor a better understanding of eta2 and the statistical test of an analysis of variance model, we have to compare the individual scores to the group averages and to the overall average. Figure 5.3 adds overall average acceptance of vaccination information to the plot (horizontal black line) with participants’ scores and average experimental group scores (coloured horizontal lines).\n\n\n\n\n\n\n\nFigure 5.3: Which part of score differences tells us about the differences between groups?\n\n\n\nLet us assume that we have measured vaccine acceptance for a sample of 12 participants in our study as depicted in Figure 5.3. Once we have our data, we first have a look at the percentage of variance that is explained, eta2. What does it mean if we say that a percentage of the variance is explained when we interpret eta2?\nThe variance that we want to explain consists of the differences between the scores of the participants on the dependent variable and the overall or grand mean of all outcome scores. Remember that a variance measures deviations from the mean. The dotted black arrows in Figure 5.3 express the distances between outcome scores and the grand average. Squaring, summing, and averaging these distances over all observations gives us the total variance in outcome scores.\nThe goal of our experiment is to explain why some of our participants’ vaccince acceptance is far above the grand mean (horizontal black line in Figure 5.3) while others score a lot lower. We hypothesized that participants are influenced by the campaign style (language used) that they have seen. If a certain campaign style has a positive effect, the average acceptance should be higher for participants confronted with this campaign style.\nIf we know the group to which a participant belongs—which language was used in the campaign they saw—we can use the average outcome score for the group as the predicted outcome for each group member—their vaccine acceptance due to the language used in the campaign they saw. The predicted group scores are represented by the coloured horizontal lines for group means in Figure 5.3.\nNow what part of the variance in outcome scores (dotted black arrows in Figure 5.3) is explained by the experimental treatment? If we use the experimental treatment as predictor of vaccine acceptance, we predict that a participant’s acceptance equals their group average (horizontal coloured line) instead of the overall average (horizontal black line), which we use if we do not take into account the participant’s experimental treatment.\nSo the difference between the overall average and the group average is what we predict and explain by the experimental treatment. This difference is represented by the solid black arrows in Figure 5.3. The variance of the predicted scores is obtained if we average the squared sizes of the solid black arrows for all participants. This variance is called the between-groups variance.\nPlaying with the group means in Figure 5.3, you may have noticed that eta2 is high if there are large differences between group means. In this situation we have high between-groups variance—large black arrows—so we can predict a lot of the variation in outcome scores between participants.\nIn contrast, small differences between group averages allow us to predict only a small part of the variation in outcome scores. If all group means are equal, we can predict none of the variation in outcome scores because the between-groups variance is zero. As we will see in Section 5.1.3, zero between-groups variance is central to the null hypothesis in analysis of variance.\nThe experimental treatment predicts that a participant’s vaccine acceptance equals the average acceptance of the participant’s group. It cannot predict or explain that a participant’s vaccine acceptance score is slightly different from their group mean (the red double-sided arrows in Figure 5.3). Within-groups variance in outcome scores is what we cannot predict with our experimental treatment; it is prediction error. In some SPSS output, it is therefore labeled as “Error”.\n\n\n5.1.3 F test on the model\nAverage group scores tell us whether the experimental treatment has effects within the sample (Section 5.1.1). If the group who saw a campaign with controlling language has higher average acceptance of vaccination information than the group who saw a campaign with neutral language, we conclude that using controlling language makes a difference in the sample. But how about the population?\nIf we want to test whether the difference that we find in the sample also applies to the population, we use the null hypothesis that all average outcome scores are equal in the population from which the samples were drawn. In our example, the null hypothesis states that vaccine acceptance of people in the population does not differ between those exposed to a campaign with autonomy supportive language, one with controlling language, or a campaign with neutral language.\nWe use the variance in group means as the number that expresses the differences between group means. If all groups have the same average outcome score, the between-groups variance is zero. The larger the differences, the larger the between-groups variance (see Section 5.1.2).\nWe cannot just use the between-groups variance as the test statistic because we have to take into account chance differences between sample means. Even if we draw different samples from the same population, the sample means will be different because we draw samples at random. These sample mean differences are due to chance, they do not reflect true differences between groups in the population.\nWe have to correct for chance differences and this is done by taking the ratio of between-groups variance over within-groups variance. This ratio gives us the relative size of observed differences between group means over group mean differences that we expect by chance.\nOur test statistic, then, is the ratio of two variances: between-groups variance and within-groups variance. The F distribution approximates the sampling distribution of the ratio of two variances, so we can use this probability distribution to test the significance of the group mean differences we observe in our sample.\nLong story short: We test the null hypothesis that all groups have the same population means in an analysis of variance. But behind the scenes, we actually test between-groups variance against within-groups variance. That is why it is called analysis of variance.\n\n\n5.1.4 Assumptions for the F test in analysis of variance\nThere are two important assumptions that we must make if we use the F distribution in analysis of variance: (1) independent samples and (2) homogeneous population variances.\n\n5.1.4.1 Independent samples\nThe first assumption is that the groups can be regarded as independent samples. As in an independent-samples t test, it must be possible in principle to draw a separate sample for each group in the analysis. Because this is a matter of principle instead of how we actually draw the sample, we have to argue that the assumption is reasonable. We cannot check the assumption against the data.\nHere is an example of an argument that we can make. In an experiment, we usually draw one sample of participants and, as a next step, we assign participants randomly to one of the experimental conditions. We could have easily drawn a separate sample for each experimental group. For example, we first draw a participant for the first condition: seeing an autonomy supportive campaign. Next, we draw a participant for the second condition, e.g., the controlling campaign. The two draws are independent: whomever we have drawn for the autonomy supportive condition is irrelevant to whom we draw for the controlling condition. Therefore, draws are independent and the samples can be regarded as independent.\nSituations where samples cannot be regarded as independent are the same as in the case of dependent/paired-samples t tests (see Section 2.3.6). For example, samples of first and second observations in a repeated measurement design should not be regarded as independent samples. Some analysis of variance models can handle repeated measurements but we do not discuss them here.\n\n\n5.1.4.2 Homogeneous population variances\nThe F test on the null hypothesis of no effect (the nil) in analysis of variance assumes that the groups are drawn from the same population. This implies that they have the same average score on the dependent variable in the population as well as the same variance of outcome scores. The null hypothesis tests the equality of population means but we must assume that the groups have equal dependent variable variances in the population.\nWe can use a statistical test to decide whether or not the population variances are equal (homogeneous). This is Levene’s F test, which is also used in combination with independent samples t tests. The test’s null hypothesis is that the population variances of the groups are equal. If we do not reject the null hypothesis, we decide that the assumption of equal population variances is plausible.\nThe assumption of equal population variances is less important if group samples are more or less of equal size (a balanced design, see Section 5.3.2). We use a rule of thumb that groups are of equal size if the size of the largest group is less than 10% (of the largest group) larger than the size of the smallest group. If this is the case, we do not care about the assumption of homogeneous population variances.\n\n\n\n5.1.5 Which groups have different average scores?\nAnalysis of variance tests the null hypothesis of equal population means but it does not yield confidence intervals for group means. It does not always tell us which groups score significantly higher or lower.\n\n\n\n\n\n\n\nFigure 5.4: Which groups have different average outcome scores in the population? The p values belong to independent-samples t tests on the means of two groups.\n\n\n\nIf the F test is statistically significant, we reject the null hypothesis that all groups have the same population mean on the dependent variable. In our current example, we reject the null hypothesis that average vaccine acceptance is equal for people who saw a campaign with autonomy supportive, controlling or neutral language. In other words, we reject the null hypothesis that the campaign style (or lagnuage used) does not matter to vaccine acceptance.\n\n5.1.5.1 Pairwise comparisons as post-hoc tests\nWith a statistically significant F test for the analysis of variance model, several questions remain to be answered. Does a controlling campaign style increase or decrease the acceptance of vaccination information? Are both campaign styles equally effective? The F test does not provide answers to these questions. We have to compare groups one by one to see which condition (campaign style) is associated with a higher level vaccine acceptance.\nIn a pairwise comparison, we have two groups, for instance, participants confronted with an autonomy supportive campaign and participants who saw a campaign with neutral language. We want to compare the two groups on a numeric dependent variable, namely their vaccine acceptance. An independent-samples t test is appropriate here.\nWith three groups, we can make three pairs: autonomy supportive versus controlling, autonomy supportive versus neutral, and controlling versus neutral. We have to execute three t tests on the same data. We already know that there are most likely differences in average scores, so the t tests are executed after the fact, in Latin post hoc. Hence the name post-hoc tests.\nApplying more than one test to the same data increases the probability of finding at least one statistically significant difference even if there are no differences at all in the population. Section 4.7.3 discussed this phenomenon as capitalization on chance and it offered a way to correct for this problem, namely Bonferroni correction. We ought to apply this correction to the independent-samples t tests that we execute if the analysis of variance F test is statistically significant.\nThe Bonferroni correction divides the significance level by the number of tests that we do. In our example, we do three t tests on pairs of groups, so we divide the significance level of five per cent by three. The resulting significance level for each t test is .0167. If a t test’s p value is below .0167, we reject the null hypothesis, but we do not reject it otherwise.\n\n\n5.1.5.2 Two steps in analysis of variance\nAnalysis of variance, then, consists of two steps. In the first step, we test the general null hypothesis that all groups have equal average scores on the dependent variable in the population. If we cannot reject this null hypothesis, we have too little evidence to conclude that there are differences between the groups. Our analysis of variance stops here, although it is recommended to report the confidence intervals of the group means to inform the reader. Perhaps our sample was just too small to reject the null hypothesis.\nIf the F test is statistically significant, we proceed to the second step. Here, we apply independent-samples t tests with Bonferroni correction to each pair of groups to see which groups have significantly different means. In our example, we would compare the autonomy supportive and controlling groups to the group with neutral language to see if a strong campaign style increases acceptance of vaccination information, and, if so, how much. In addition, we would compare the autonomy supportive and controlling groups to see if one campaign style is more effective than the other.\n\n\n5.1.5.3 Contradictory results\nIt may happen that the F test on the model is statistically significant but none of the post-hoc tests is statistically significant. This mainly happens when the p value of the F test is near .05. Perhaps the correction for capitalization on chance is too strong; this is known to be the case with the Bonferroni correction. Alternatively, the sample can be too small for the post-hoc test. Note that we have fewer observations in a post-hoc test than in the F test because we only look at two of the groups.\nThis situation illustrates the limitations of null hypothesis significance tests (Section 4.7). Remember that the 5 per cent significance level remains an arbitrary boundary and statistical significance depends a lot on sample size. So do not panic if the F and t tests have contradictory results.\nA statistically significant F test tells us that we may be quite confident that at least two group means are different in the population. If none of the post-hoc t tests is statistically significant, we should note that it is difficult to pinpoint the differences. Nevertheless, we should report the sample means of the groups (and their standard deviations) as well as the confidence intervals of their differences as reported in the post-hoc test. The two groups that have most different sample means are most likely to have different population means.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "07-anova.html#sec-onewaySPSS",
    "href": "07-anova.html#sec-onewaySPSS",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "5.2 One-Way Analysis of Variance in SPSS",
    "text": "5.2 One-Way Analysis of Variance in SPSS\n\n5.2.1 Instructions\nIn the video below we walk you through conducting and interpreting a one-way analysis of variance in SPSS. We will continue using our vaccination campaign example. The research question that is discussed in the video below is ‘Does the style of language used in a campaign vaccine acceptance among people seeing the campaign?’. The data set includes the variable lang_cond (language condition), which is a categorical variable. Participants are either watching a campaign with autonomy supportive language, controlling language or neutral. You are researching whether the language style used in the campaign has an effect on people’s vaccine acceptance. Vaccine acceptance has been measured before (accept_pre) and after (accept_post) watching the campaign.\n\n\n\n\n\n\nVideo 5.2: Execute one-way analysis of variance in SPSS.\n\n\n\nWhen we want to research the effect of a categorical variable (campaign style) on a numerical variable (accept_post), we use a one-way analysis of variance (see the test selection table Section 4.4). You can get the one-way ANOVA window by clicking Analyze &gt; Compare Means &gt; One-Way ANOVA. The dependent variable is the vaccine acceptance after watching the campaign, so this variable should be added to the Dependent List:. The independent variable, also called factor in ANOVA, is the campaign style (i.e. the type of language used) of the campaign which should be added to Factor:. Before running your ANOVA, we select some additional options. Select Post Hoc... and select the Bonferroni correction for multiple testing. Under Options... we select Descriptive to obtain the group means and Homogeneity of variance test for Levene’s F-test to check assumptions. You would only select bootstrapping if you cannot use the theoretical approximation for the F-distribution (revise Chapter 2 if needed). You can select Estimate effect size for overall tests if you want to obtain eta2. Please click paste and run the command from your syntax.\nIn addition to the SPSS analysis, it is always a good idea to visualize the results of your analysis. You can do this by creating a means plot with error bars to clearly show the differences between the groups. Video 5.3 below shows you how to create a means plot in SPSS.\n\n\n\n\n\n\nVideo 5.3: Visualizing the results of a one-way analysis of variance in SPSS.\n\n\n\nIn the output we can start interpreting our results. Video 5.4 runs us through the interpretation of the output. The first table, Descriptives provides us with the group means. Here we can immediately see that the vaccine acceptance is lower for those who viewed the campaign with neutral language. In this table we can also check the group sizes, an assumption for ANOVA, the sizes are fairly equal (45, 49, 49). You can see that the Test of Homogeneity of Variances is not significant indicating the assumption is met on both fronts (equal group sizes and equal variances).\n\n\n\n\n\n\nVideo 5.4: Interpreting the output of a one-way analysis of variance in SPSS.\n\n\n\nTo interpret the actual test results, we study the ANOVA table. We can see the between groups and within groups results, you find the F-value in the Between Groups row, with a p-value below .05 indicating a significant result. The table below the ANOVA table, ANOVA Effect Sizes provides an insight in the size of the effect - i.e., the size of the difference between the groups. Eta-squared is the effect size that we report for ANOVA, as reported in the video all versions above SPSS v26 show the partial eta-squared meaning that you need to calculate eta-squared by hand. We will walk you through this process later in this chapter.\nAs said we have found a significant F-value indicating that there is a significant difference between the groups. We have a factor with three groups (Autonomy Supportive, Controlling, Neutral) thus we need further analysis to tell which group(s) differ. For this we inspect the Post Hoc Tests, in the table `Multiple Comparisons. Here you can see the results of several t-tests (as you might remember from Section 4.4 we can compare two groups on a numerical variable with a t-test). The p-values in this table are corrected for capitalization on chance due to multiple testing (that is why we selected Bonferroni earlier). These results show us that the groups Neutral and Autonomy Supportive differ significantly from each other, and so do the groups Neutral and Controlling (i.e., both comparisons have a p-value below .05 and the confidence interval does not include the zero). The difference between Autonomy Supportive and Controlling is not significant (i.e., the p-value is larger than .05 and the confidence interval includes the zero). It can help to also take a look at the Means Plots which allows you to visualize the effects.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "07-anova.html#different-means-for-two-factors",
    "href": "07-anova.html#different-means-for-two-factors",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "5.3 Different Means for Two Factors",
    "text": "5.3 Different Means for Two Factors\nThe participants in the experiment do not only differ because they see different campaign styles. In addition, personal characteristics could impact how participants perceive each campaign. In our example, we are particularly interested in participants’ health literacy, categorizing the participants in two groups: low and high health literacy. We can then ask the question: Does the effect of the language used in campaigns on vaccine acceptance differ between people with low and those with high health literacy?\n\n\n\n\n\n\n\nFigure 5.5: How do group means inform us about (main) effects in analysis of variance?\n\n\n\nIn the preceding section, we have looked at the effect of a single factor on acceptance of vaccination information, namely, the language used in a campaign to which participants are exposed. Thus, we take into account two variables: one independent variable and one dependent variable. This is an example of bivariate analysis.\nUsually, however, we expect an outcome to depend on more than one variable. Vaccine acceptance does not only depend on the language used in a campaign. It is easy to think of more factors, such as a person’s previous experience with vaccines, their personal health, their beliefs, and so on.\nIt is straightforward to include more factors in an analysis of variance. These can be additional experimental treatments in the context of an experiment as well as participant characteristics that are not manipulated by the researcher. For example, we may hypothesize that people with high health literacy are generally more accepting of vaccines than people with low health literacy.\n\n5.3.1 Two-way analysis of variance\nIf we use one factor, the analysis is called one-way analysis of variance. With two factors, it is called two-way analysis of variance, and with three factors… well, you probably already guessed that name. An analysis of variance with two or more factors can also be called a multi-way or factorial analysis of variance.\nA two-way analysis of variance using a factor with three levels, for instance, exposure to three different campaign styles, and a second factor with two levels, for example, low versus high health literacy, is called a 3x2 (say: three by two) factorial design.\n\n\n5.3.2 Balanced design\nIn analysis of variance with two or more factors, we aim to have a balanced design. A balanced design in analysis of variance exists when there is an equal number of observations. It is important that this equal number of observations exists across every combination of factor levels. This means that each group being compared in the analysis has the same number of data points (in our case: participants). In an experiment, we can ensure a balanced design if we have the same number of participants in each combination of levels on all factors. In other words, a factorial design is balanced if we have the same number of observations in each subgroup. A subgroup contains the participants that have the same level on both factors just like a cell in a contingency table.\nBalanced designs are important because they lead to more robust results. In addition, a balanced design indicates statistical independence. Statistical independence entails that the factors do not influence each other, meaning that the effect of one factor does not change based on the level of another factor. In other words, knowing the value or effect of one factor provides no information about the value of the other factor. We aim for statistical independence in analysis of variance.\n\n\n\n\nTable 5.1: Number of observations per subgroup in a balanced 3x2 factorial design.\n\n\n\n\n\n\n\nHigh.Health.Literacy\nLow.Health.Literacy\n\n\n\n\nAutonomy\n2\n2\n\n\nControl\n2\n2\n\n\nNeutral\n2\n2\n\n\n\n\n\n\n\n\nTable 5.1 shows an example of a balanced 3x2 factorial design. Each subgroup (cell) contains two participants (cases). Equal distributions of frequencies across columns or across rows indicate a balanced design. In the example, we see a balanced design with equal distributions across columns (and rows). This means that the factors are statisically independent.\nIn practice, it may not always be possible to have exactly the same number of observations for each subgroup. A participant may drop out from the experiment, a measurement may go wrong, and so on. If the numbers of observations are more or less the same for all subgroups, the factors are nearly independent, which is okay. We can use the same rule of thumb for a balanced design as for the conditions of an F test in analysis of variance: If the size of the smallest subgroup is less than ten per cent smaller than the size of the largest group, we call a factorial design balanced.\nAn example: if Table 5.1 would read 10-9-9 for both columns, the largest subgroup exists of 10 participants. Ten per cent of the largest group (10) is one participant. The smallest group (9) differs the maximum of 1 participant from the largest group. Thus the design would still be balanced. If one group would exist of eight participants, the difference of two participants would exceed the ten per cent of one participant and therefore be unbalanced.\nA balanced design is desired but not necessary. Unbalanced designs can be analyzed but estimation is more complicated (a problem for the computer, not for us) and the assumption of equal population variances for all groups (Levene’s F test) is more important (a problem for us, not for the computer) because we do not have equal group sizes. Note that the requirement of equal group sizes applies to the subgroups in a two-way analysis of variance. With a balanced design, we ensure that we have the same number of observations in all subgroups, so we are on the safe side.\n\n\n5.3.3 Main effects in two-way analysis of variance\nA two-way analysis of variance tests the effects of both factors on the dependent variable in one go. It tests the null hypothesis that people exposed to an autonomy supportive campaign have the same average vaccine acceptance in the population as people exposed to a controlling campaign and as those who are exposed to a neutral campaign. It also tests the null hypothesis that people with low health literacy and people with high health literacy have the same average vaccine acceptance in the population.\n\n\n\n\n\n\n\n\nFigure 5.6: Means plots for the main effects of language condition and health literacy on vaccine acceptance (Error bars = 95% Confidence Intervals). As a reading instruction, effects of langugae conditions and having high health literacy are represented by arrows and dashed lines.\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.7: Means plots for the main effects of language condition and health literacy on vaccine acceptance (Error bars = 95% Confidence Intervals). As a reading instruction, effects of langugae conditions and having high health literacy are represented by arrows and dashed lines.\n\n\n\n\n\nThe tested effects are main effects because they represent the effect of one factor. They express an overall or average difference between the mean scores of the groups on the dependent variable. The main effect of the campaign style factor shows the mean differences for campaign groups if we do not distinguish between low and high health literacy. Likewise, the main effect for health literacy shows the average difference in vaccine acceptance between those with low and those with high health literacy without taking into account the language used in the campaign to which they were exposed.\nWe could have used two separate one-way analyses of variance to test the same effects. Moreover, we could have tested the difference between low and high health literacy with an independent-samples t test. The results would have been the same (if the design is balanced.) But there is an important advantage to using a two-way analysis of variance, to which we turn in the next section.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "07-anova.html#sec-moderationanova",
    "href": "07-anova.html#sec-moderationanova",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "5.4 Moderation: Group-Level Differences that Depend on Context",
    "text": "5.4 Moderation: Group-Level Differences that Depend on Context\nIn the preceding section, we have analyzed the effects both of campaign style and health literacy on vaccine acceptance. The two main effects isolate the influence of campaign style on acceptance of vaccination information from the effect of health literacy and the other way around.\nBut does campaign style always have the same effect? Even if there is a main effect of campaign style on vaccine acceptance across all participants, it is possible that this effect differs when we are zooming in to specific sub-groups of our sample, for instance comparing people high and those with low health literacy. In other words, do people with high and people with low health literacy differ in their response to different campaign styles? For instance, people with high health literacy might feel confident in their ability to find and use health information in order to make their own, informed decisions about health behaviors like vaccination. These individuals might prefer autonomy-supportive over controlling communication styles. Individuals with low health literacy might not show this preference for autonomy-supportive language as they might feel less confident to use that autonomy to make complicated decisions on their own. Thus, we could expect that the effect of campaign style differs between those with high and low health literacy.\nIf the effect of a factor is different for different groups on another factor, the first factor’s effect is moderated by the second factor. The phenomenon that effects are moderated is called moderation. Both factors are independent variables. To distinguish between them, we will henceforth refer to them as the predictor (here: campaign style) and the moderator (here: health literacy).\nWith moderation, factors have a combined effect. The context (group score on the moderator) affects the effect of the other predictor on the dependent variable. The conceptual diagram for moderation expresses the effect of the moderator on the effect of the predictor as an arrow pointing at another arrow. Figure 5.8 shows the conceptual diagram for participant’s health literacy moderating the effect of campaign style on vaccine acceptance.\n\n\n\n\n\n\n\n\nFigure 5.8: Conceptual diagram of moderation.\n\n\n\n\n\n\n5.4.1 Types of moderation\nModeration as different effects for different groups is best interpreted using a cross-tabulation of group means, which is visualized as a means plot. In a group means table, the Totals row and column contain the means for each factor separately, for example the means for low and high (factor health literacy) or the means for the language used in the campaigns (factor campaign style). These means represent the main effects. In contrast, the means in the cells of the table are the means of the subgroups, which represent moderation. Draw them in a means plot for easy interpretation.\nIn a means plot, we use the groups of the predictor on the horizontal axis, for example, the three camapign styles. The average score on the dependent variable is used as the vertical axis. Finally, we plot the average scores for every predictor-moderator group, for instance, an campaign-literacy combination, and we link the means that belong to the same moderator group, for example, the means for people with high and the means for people with low health literacy (Figure 5.9).\n\n\n\n\n\n\n\nFigure 5.9: How can we recognize main effects and moderation in a means plot?\n\n\n\nModeration happens a lot in communication science for the simple reason that the effects of messages are stronger for people who are more susceptible to the message. If you know more people who have adopted a new product or a healthy/risky lifestyle, you are more likely to be persuaded by media campaigns to also adopt that product or lifestyle. If you are more impressionable in general, media messages are more effective.\n\n5.4.1.1 Effect strength moderation\nModeration refers to contexts that strengthen or diminish the effect of, for instance, a media campaign. Let us refer to this type of moderation as effect strength moderation. In our current example, we would hypothesize that the effect of using controlling language is stronger for participants with low health literacy than for participants with high health literacy.\nIn analysis of variance, effects are differences between average outcome scores. The effect of controlling language on vaccine acceptance, for instance, is the difference between the average vaccine of participants exposed to controlling language and the average score of participants who were exposed to a campaign with neutral language.\nDifferent effects of controlling language for participants with low and participants with high health literacy imply different differences! The difference in average vaccine acceptance scores between people with low health literacy exposed to controlling language and people with low health literacy who are exposed to neutral language is different from the difference in average acceptance scores for people with high health literacy. We have four subgroups with average acceptance scores that we have to compare. We have six subgroups if we also include the autonomy supportive effect.\n\n\n\n\n\n\n\n\nFigure 5.10: Moderation as a stronger effect within a particular context.\n\n\n\n\n\nA means plot is a very convenient tool to interpret different differences. Connect the means of the subgroups by lines that belong to the same group on the factor you use as moderator. Each line in the plot represents the effect differences within one moderator group. If a line goes up or down, predictor groups have different means, so the predictor has an effect within that moderator group. A flat (horizontal) line tells us that there is no effect at all within that moderator group\nThe distances between the lines show the difference of the differences. If the lines for people with low and people with high health literacy are parallel, the difference between campaign styles is the same for people with low and people with high health literacy. Then, the effects are the same and there is no moderation. In contrast, if the lines are not parallel but diverge or converge, the differences are different for people with low and people with high health literacy and there is moderation.\nA special case of effect strength moderation is the situation in which the effect is absent (zero) in one context and present in another context. A trivial example would be the effect of an anti-smoking campaign on smoking frequency. For smokers (one context), smoking frequency may go down with campaign exposure and the campaign may have an effect. For non-smokers (another context), smoking frequency cannot go down and the campaign cannot have this effect.\nExcept for trivial cases such as the effect of anti-smoking campaigns on non-smokers, it does not make much sense to distinguish sharply between moderation in which the effect is strengthened and moderation in which the effect is present versus absent. In non-trivial cases, it is very rare that an effect is precisely zero. (See Holbert and Park (2019) for a different view on this matter.)\n\n\n5.4.1.2 Effect direction moderation\nIn the other type of moderation, the effect in one group is the opposite of the effect in another group. In Figure 5.11, for example, autonomy supportive language increases the average vaccine acceptance among people with high health literacy in comparison to the group who was exposed to neutral language. In contrast, average acceptance for people with low health literacy exposed to the autonomy supportive campaign is lower than the average for people with low health literacy exposed to the campaign with neutral language. Let us call this effect direction moderation. People with low health literacy reverse the autonomy supportive effect that we find for people with high health literacy.\n\n\n\n\n\n\n\n\nFigure 5.11: Moderation as a positive effect in one context and a negative effect in another context.\n\n\n\n\n\nIn an extreme situation, the effect in one group can compensate for the effect in another group if it is about as strong but of the opposite direction (Figure 5.12). Imagine that controlling language convinces people with low health literacy to accept vaccination by giving them the confident guidance they need but discourages people with high health literacy to accept vaccination because it robs them of their independence. In contrast, autonomy supportive language could work better for people with high health literacy, because it preserves their freedom to make their own decision, but discourage people with low health literacy, because it does not meet their need for clarity.\n\n\n\n\n\n\n\n\nFigure 5.12: Moderation as opposite effects in different contexts.\n\n\n\n\n\nIn this situation, the main effect of campaign style on vaccine acceptance is close to zero. If we average over people with low and people with high health literacy, we obtain the means represented by the three grey dots. There is no net difference between autonomy supportive, controlling, and neutral language used in a campaign.\nThis does not mean that the campaign style does not matter. On the contrary, the interaction effect tells us that one campaign style is effective for one group but counterproductive for another group. The second part of the conclusion is just as important as the first part. The campaigner should avoid decreasing the vaccine acceptance among particular target groups.\n\n\n\n5.4.2 Testing main and interaction effects\nThe effect of a single factor is called a main effect, as we learned in Section 5.3.3. A main effect reflects the difference between means for groups within one factor. The main effect of health literacy, for instance, could be that people with high health literacy are, on average, more willing to accept vaccination than people with low health literacy. A two-way analysis of variance includes two main effects, one for each factor (see Section 5.3.1), for example a main effect of health literacy and a main effect of campaign style.\nFor moderation, however, we compare average scores of subgroups. For instance, people exposed to the controlling language campaign can be split into two sub-groups: those with high or low health literacy. Expressed more formally, sub-groups combine a level on one factor with a level on another factor. In Figure 5.13, we compare average vaccine acceptance for combinations of campaign style and participant’s health literacy. The effect of differences among subgroups on the dependent variable is called an interaction effect. Just like a main effect, an interaction effect is tested with an F test and its effect size is expressed by eta2.\n\n\n\n\n\n\n\nFigure 5.13: How can we recognize main effects and moderation in a means plot?\n\n\n\nInterpretation of moderation requires some training because we must look beyond main effects. The fact that people with high health literacy score on average higher than people with low health literacy is irrelevant to moderation but it does affect all subgroup mean scores. So the fact that the red line is above the blue line in Figure 5.13 is not relevant to moderation.\nModeration concerns the differences between subgroups that remain if we remove the overall differences between groups, that is, the differences that are captured by the main effects. The remaining differences between subgroup average scores provide us with a between-groups variance. In addition, the variation of outcome scores within subgroups yields a within-groups variance. Note that within-groups variance is not visible in Figure 5.13 because the vaccine acceptance scores of the individual participants are not shown.\nWe can use the between-groups and within-groups variances to execute an F test just like the F test we use for main effects. The null hypothesis of the F test on an interaction effect states that the subgroups have the same population averages if we correct for the main effects. Our statistical software takes care of this correction if we include the main effects in the model, which we should always do.\nAlternatively, we can formulate the null hypothesis of the test on the interaction effect as equal effects of the predictor for all moderator groups in the population. In the current example, the null hypothesis could be that the effect of campaign style is the same for both levels of health literacy in the population. Or, that the effect of health literacy is the same for different campaign styles in the population.\n\n\n\n\n\n\nNull hypothesis of the test on the interaction effect: equal effects of the predictor for all moderator groups in the population.\n\n\n\nModeration between three or more factors is possible. These are called higher-order interactions. It is wise to include all main effects and lower-order interactions if we test a higher-order interaction. As a result, our model becomes very complicated and hard to interpret. If a (first-order) interaction between two predictors must be interpreted as different differences, an interaction between three factors must be interpreted as different differences in differences. That’s difficult to imagine, so let us avoid them.\n\n\n5.4.3 Assumptions for two-way analysis of variance\nThe assumptions for two-way analysis of variance are the same as for one-way analysis of variance (Section 5.1.4). Just note that equal group sizes and equal population variances now apply to the subgroups formed by the combination of the two factors.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "07-anova.html#reporting-two-way-analysis-of-variance",
    "href": "07-anova.html#reporting-two-way-analysis-of-variance",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "5.5 Reporting Two-Way Analysis of Variance",
    "text": "5.5 Reporting Two-Way Analysis of Variance\nThe main purpose of reporting a two-way analysis of variance is to show the reader the differences between average group scores on the dependent variable between groups on the same factor (main effects) and different differences for groups on a second factor (interaction effect). A means plot is very suitable for this purpose. Conventionally, we place the predictor groups on the horizontal axis and we draw different lines for the moderator groups. But you can switch them if this produces a more appealing graph.\n\n\n\n\n\n\n\n\nFigure 5.14: An example of a means plot.\n\n\n\n\n\nFor the statistically informed reader, you should include the following information somewhere in your report:\n\nThat you used analysis of variance and the analysis of variance type (one-way, two-way, or multi-way).\nThe test results for every effect, consisting of the test name (F), the degrees of freedom, and the significance (p-value). APA prescribes the following format if you report the test result within your text: F (df1, df2) = F-value, p = p-value. Note that df1 is the degrees of freedom of the factor (between-groups) and df2 is the degrees of freedom of the error (within-groups).\nFor each effect report eta-squared (eta2) and interpret it in terms of effect size. If you have to calculate eta-squared by hand, divide the between-groups sum of squares of an effect by the total sum of squares (SPSS: corrected total). If SPSS calculates eta-squared, also report the confidence interval for eta-squared.\nFor each effect worth interpretation, clarify which group or subgroup scores higher. Report the group means and their standard deviations or the mean difference with its confidence interval and p-value (from the post-hoc tests) here. Note that the SPSS menu only supplies post-hoc tests for main effects of factors with more than two levels (groups).\nPay special attention to an interaction effect. Explain how an effect (differences between groups) of the predictor differs across groups on the moderator. This results in sentences containing three variables. For example: “Autonomy supportive language increases acceptance of vaccination information more among people with high health literacy than among people with low health literacy.” Do you recognize the three variables (predictor, moderator, and dependent variable) here?\nAs always, do not forget to mention the units (cases) and the meaning of the variables (factors and outcome). They describe the topic of the analysis.\nReport it if the main assumption is violated, that is, if you have (sub)groups of unequal size (i.e., an unbalanced design) and the test on homogeneous variances (Leven’s F-test) is statistically significant. Report Levene’s test just like you report the F test of a main effect (see above). If the assumption is violated, we still report and interpret the results of the analysis of variance but we warn that the results may not be trustworthy and should be treated with caution.\n\nA two-way analysis of variance may produce many numeric results to report. It is recommended to present them as a table (in the text or in an appendix). If you report the table, include the error, the sums of squares and mean squares in the same way that SPSS reports them. Table 5.2 presents an example.\n\n\n\n\nTable 5.2\n\n\n\n\n\n\n\n\n\nSum of Squares\n\n\ndf\n\n\nMean Square\n\n\nF\n\n\np\n\n\n\n\n\n\nhealth literacy\n\n\n26.37\n\n\n1\n\n\n26.37\n\n\n11.86\n\n\n0.001\n\n\n\n\nlanguage condition\n\n\n38.05\n\n\n2\n\n\n19.03\n\n\n8.56\n\n\n&lt; 0.001\n\n\n\n\nlanguage condition*health literacy\n\n\n20.60\n\n\n2\n\n\n10.30\n\n\n4.63\n\n\n0.011\n\n\n\n\nerror\n\n\n304.54\n\n\n137\n\n\n2.22\n\n\n\n\n\n\n\n\nTotal\n\n\n389.56\n\n\n142\n\n\n\n\n\n\n\n\n\n\nAn example of a table summarizing results of a two-way analysis of variance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "07-anova.html#sec-twowaySPSS",
    "href": "07-anova.html#sec-twowaySPSS",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "5.6 Two-Way Analysis of Variance in SPSS",
    "text": "5.6 Two-Way Analysis of Variance in SPSS\n\n5.6.1 Instructions\nYou have learned how to execute an one-way analysis of variance, and now it is time to execute a two-way analysis of variance in SPSS. We keep using our vaccination campaign example. A two-way analysis of variance allows us to answer the question: Does the effect of campaign style on vaccine acceptance differ between people with high and those with low health literacy? This is a moderation or interaction effect. We have two categorical independent variables (or factors) and one numerical dependent variable. The first factor is the campaign style (three levels: autonomy supportive, neutral, and controlling language). The second factor is health literacy (two levels: low and high). The dependent variable is vaccine acceptance.\nIn SPSS, we use the Univariate option in the General Linear Model submenu for two-way analysis of variance. This will be described in detail in the video below. Let’s first take a closer look at the output provided by SPSS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.15: SPSS table of main and interaction effects in a two-way analysis of variance.\n\n\n\nThe significance tests on the main effects and interaction effect are reported in the Tests of Between-Subjects Effects table. Figure 5.15 offers an example. The tests on the main effects are in the red box and the green box contains the test on the interaction effect. The APA-style summary of the main effect of campaign style (variable: lang_cond) is: F (2, 137) = 8.43, p &lt; .001, eta2 = .10. Note the two degrees of freedom in between the brackets, which are marked by a blue ellipse in the figure. You get the effect size eta2 by dividing the sum of squares of an effect by the corrected total sum of squares (in purple ellipses in the figure): 37.456 / 389.566 = 0.10.\nInterpret the effects by comparing mean scores on the dependent variable among groups:\n\nIf there are two groups on a factor, for example, low and high health literacy, compare the two group means: Which group scores higher? For example, people with high health literacy score on average 5.05 on vaccine acceptance whereas the average acceptance is only 4.19 for people with low health literacy. The F test shows whether or not the difference between the two groups is statistically significant.\nIf a factor has more than two groups, for example, neutral, controlling or autonomy supportive language, use post-hoc comparisons with Bonferroni correction. The results tell you which group scores on average higher than another group and whether the difference is statistically significant if we correct for capitalization on chance.\nIf you want to interpret an interaction effect, create means plots such as Figure 5.16. Compare the differences between means across groups. In the left panel, for example, we see that the effect of health literacy on acceptance of vaccination information (the difference between the mean score of people with low health literacy and the mean score of people with high health literacy) is larger for autonomy supportive language (pink box in the middle) than for neutral language (pink box on the left), and it is smallest for controlling language (pink box on the right). Similarly, we see that the effect of seeing the autonomy supportive campagin instead of the neutral campaign is larger for people with high healthy literacy (right-hand panel, pink box on the right) than for people with low health literacy (right-hand panel, pink box on the left).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.16: SPSS means plots of the interaction effect of health literacy and campaign style on vaccine acceptance. Note that the pink arrows and dotted lines have been added manually to aid the interpretation.\n\n\n\nIn the video below we walk you through how to perform and interpret a two-way analysis of variance in SPSS. In the video we will elaborate on the dataset that we have used for our one-way analysis of variance (does the language style used in a vaccination campaign have an effect on vaccine acceptance?). The persuasive communication research team is adding a second variable, a moderator: health literacy (in the dataset called ‘health_literacy’). Participants are divided in two groups (those with high and low health literacy). When conducting a two-way analysis of variance we are able to answer the questions whether the effect of campaign style (called ‘lang_cond’) on vaccine acceptance is different for people with high and low levels of health literacy.\n\n\n\n\n\n\nVideo 5.5: Execute two-way analysis of variance in SPSS\n\n\n\nTo perform a two-way (or multi-way) analysis of variance, we select Analyze &gt; General Linear Model &gt; Univariate. The numerical dependent variable accept_post is added to Dependent Variable: and the categorical factors lang_cond and health_literacy are added to the Fixed Factor(s):. If you see the variable labels instead of the variable names, simply do a right mouse click and select show variable names. For our additional setting, we select Plots... and add the predictor (lang_cond) to the Horizontal Axis: and the moderator (health_literacy) to Seperate Lines:. As stated earlier in this chapter, we usually add the moderator as separate lines but if you have good reason to switch them (e.g., easier interpretation): you can. Do not forget to click Add and Continue. As before we also select Post Hoc..., we only need to add lang_cond since health_literacy only consist of two groups. Hopefully you remember we see t-test results in our post hoc table, meaning we are comparing two groups, this is not necessary to do when the variable only has two groups to begin with. Thus, add lang_cond to Post Hoc Tests for:, select Bonerroni and click Continue. Under options we select Descriptive statistics (for our group means) and homogeneity tests (for Levene’s F-test for our assumption check). Then, as always, click paste and run the command through the syntax.\nIn addition to the analysis we would also like to visualize the the results. We again use the chart builden in SPSS. Video 5.6 shows you how to visualize the interaction effect and how to add error bars to interpret the results.\n\n\n\n\n\n\nVideo 5.6: Visualize two-way analysis of variance in SPSS\n\n\n\nWhen we inspect the output in Video 5.7, we start with the first tables including Descriptive Statistics. In this table all the group means are provided and we can check whether we have equal subgroup sizes (meaning a balanced design, indicating statistical independence for our variables). The subgroup sizes range from 22 to 25, our largest group (25) indicates an allowed difference of 2.5. The difference of 3 participants is slightly above our allowed difference. This means we also take a look at the Levene’s F-test in the next table. The table shows a not significant results (the p-value is above .05), meaning we have met our assumption of equal variances.\n\n\n\n\n\n\nVideo 5.7: Interpreting the independent two-way analysis of variance in SPSS\n\n\n\nFor the actual test results, we take a look at the table of Tests of Between-Subjects Effects. Here we can see that both the main effects (rows: lang_cond and health_literacy) are significant and so does the interaction effect (row: lang_cond * health_literacy). If you want to know more about these effects, we can look at the Descriptive Statistics table to check the means of the variable health_literacy. Here we can see that participants with high health literacy have a higher average vaccine acceptance than participants with low health literacy. When we want to know more about the language style effect, we take a look at the Post Hoc Tests in the table Multiple Comparisons where we can find our t-tests. Like the one-way analysis of variance had already showed us: there are significant differences between the Autonomy Supportive and Neutral groups and between the Controlling and Neutral groups (the p-values are below .05 and the confidence intervals do not include the zero). To make sense of the significant interaction effect, it usually helps to take a look at the Profile Plots. Here the lines represent the two groups of our moderator, you can see that the green line is almost always above the blue line (representing those with high health literacy having a higher average vaccine acceptance than those with low health literacy). The effect of seeing a campaign with autonomy supportive language is positive for both groups of our moderator (both average vaccine acceptance scores go up), but the effect is stronger for those with high health literacy. The effect of seeing a campaign with controlling language is positive for both groups of our moderator compared to the Neutral group, but for those with high health literacy we see that autonomy supportive language has a stronger effect than controlling language and for participants with low health literacy we see that controlling language has a stronger effect than autonomy supportive language.\nIn conclusion, there is a significant main effect of campaign style on vaccine acceptance. Seeing a campaign that uses a theory-driven language style (either autonomy supportive or controlling language) compared to a campaign with neutral language has a significant positive effect on vaccine acceptance. There is also a significant main effect of a person’s health literacy. On average, those with high health literacy have a significantly higher vaccine acceptance than those with low health literacy. Additionally, there is a significant interaction effect of campaign style and health literacy on vaccine acceptance. More specifically, seeing a theory-driven campaign has a positive effect on both health literacy groups but those with high health literacy seem to have a stronger positive effect when seeing autonomy supportive language than when they see controlling language. In contrast, those with low health literacy have a stronger positive effect when seeing controlling language than when they see autonomy supportive language.\nPlease watch the video below for step-by-step instructions and for additional details.\n\nWe told you before that you had to calculate eta-squared (eta2) yourself if you are working with a SPSS version above 26. We are assuming all of you are, so in the video below we show you the calculation of eta2. Eta-squared is the measure of association, or effect size, belonging to ANOVA. We need the Test of Between-Subjects Effects table in the ANOVA output to calculate eta2. You devide the Sum of Squares for the between groups by the total Sum of Squares (SS). We calculate eta2per effect. Thus, if we want to calculate eta2 for the interaction effect we devide the SS for the interaction effect (20.602) by the total SS (389.566). Beware we use the correctal total here. The eta2 for the interaction effect is 0.05 (or 5,3% of the variance in vaccine acceptance is explained by the interaction effect).\nWatch the video below for a step-by-step instruction for calculating the eta2 of the campaign style main effect and the visual details.\n\n\n\n\n\n\nVideo 5.8: Calculating \\(\\eta^2\\) and partial \\(\\eta^2\\) from SPSS output.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "07-anova.html#take-home-points",
    "href": "07-anova.html#take-home-points",
    "title": "5  Moderation with Analysis of Variance (ANOVA)",
    "section": "5.7 Take-Home Points",
    "text": "5.7 Take-Home Points\n\nIn analysis of variance, we test the null hypothesis that all groups have the same population means. Behind the scenes, we actually test the ratio of between-groups variance to within-groups variance.\nThe overall differences in average outcome scores between groups on one factor (independent variable) are a main effect in an analysis of variance.\nThe differences in average outcome scores between subgroups, that is, groups that combine a level on one factor (predictor) and a level on another factor (moderator), represent an interaction effect. Note that we are dealing with the differences between subgroup scores that remain after the main effects have been removed.\nModeration is the phenomenon that an effect is different in different contexts. The effect can be stronger or it can have a different direction. In analysis of variance, interaction effects represent moderation.\nEta-squared measures the size of a main or interaction effect in analysis of variance. It tells us the proportion of variance in the dependent variable that is accounted for by the effect.\nA means plot is very helpful for interpreting and communicating results of an analysis of variance.\nThe F tests in analysis of variance do not tell us which groups have different average scores on the dependent variable. To this end, we use independent-samples t tests as post-hoc tests with a (Bonferroni) correction for capitalization on chance.\nTo apply analysis of variance, we need a numeric dependent variable that has equal population variance in each group of a factor or each subgroup in case of an interaction effect. However, equality of population variances is not important if all groups on a factor or all subgroups in an interaction are more or less of equal size (the largest count is at most 10% of the largest count larger than the smallest count.)\n\n\n\n\n\nDeci, Edward L., and Richard M. Ryan. 2013. Intrinsic Motivation and Self-Determination in Human Behavior. Springer Science & Business Media.\n\n\nFishbein, M., and J. N. Cappella. 2006. “The Role of Theory in Developing Effective Health Communications.” Journal of Communication 56 (s1): S1–17. http://onlinelibrary.wiley.com/doi/10.1111/j.1460-2466.2006.00280.x/full.\n\n\nFisher, R. A. 1919. “The Correlation Between Relatives on the Supposition of Mendelian Inheritance.” Transactions of the Royal Society of Edinburgh 52 (2): 399–433. https://doi.org/10.1017/S0080456800012163.\n\n\nHolbert, R. Lance, and Esul Park. 2019. “Conceptualizing, Organizing, and Positing Moderation in Communication Research.” Communication Theory, April. https://doi.org/10.1093/ct/qtz006.\n\n\nRogers, R. W. 1975. “A Protection Motivation Theory of Fear Appeals and Attitude Change1: The Journal of Psychology: Vol 91, No 1.” THe Journal of Psychology 91 (1): 93–114. https://www.tandfonline.com/doi/abs/10.1080/00223980.1975.9915803?casa_token=tnl4hHdJsB0AAAAA:T1RIciCaIhUSOxB2By3Ah9nZPbufTPANLbbdgCujfE-yZr6cugamG3pBECgsGroUDXf02-k1XlEzOw.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Moderation with Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "08-moderation-categorical.html",
    "href": "08-moderation-categorical.html",
    "title": "6  Regression Analysis And A Categorical Moderator",
    "section": "",
    "text": "Summary\nWatch this micro lecture on regression analysis with a categorical moderator for an overview of the chapter (Video 6.1).\nThe linear regression model is a powerful and very popular tool for predicting a numerical dependent variable from one or more independent variables. In this chapter, we use regression analysis to evaluate the effects of an anti-smoking campaign. We predict attitude towards smoking from exposure to the anti-smoking campaign (numerical), time spent with smokers (numerical), and the respondent’s smoking status (categorical).\nRegression coefficients, that is, the slopes of regression lines, are the effects in a regression model. They show the predicted difference in the dependent variable for a one unit difference in the independent variable (exposure, time spent with smokers) or the predicted mean difference for two categories (smokers versus non-smokers).\nBut what if the predictive effect is not the same in all contexts? For example, exposure to an anti-smoking campaign may generally generate a more negative attitude towards smoking. The effect, however, is probably different for people who smoke than for people who do not smoke. In this case, the effect of campaign exposure on attitude towards smoking is moderated by context: Whether or not the person exposed to the campaign is a smoker.\nDifferent effect sizes for different contexts are different regression coefficients for different contexts. We need different regression lines for different groups of people. We can use an interaction variable as an independent variable in a regression model to accommodate for moderation as different effects. An interaction variable is just the product of the predictor variable and the moderator variable.\nAs an independent variable in the model, the regression coefficient of an interaction variable (interaction effect for short) has a confidence interval and a p value. The confidence interval tells us the plausible values for the size of the interaction effect in the population. The p value tests the null hypothesis that there is no interaction effect at all in the population.\nTo interpret the interaction effect, we must determine the size of the effect of the predictor on the dependent variable for each group of the moderator. For example, the effect of campaign exposure on smoking attitude for smokers and the effect for non-smokers.\nAn interaction effect in a regression model closely resembles an interaction effect in analysis of variance. The effect of a single predictor that is involved in an interaction effect in a regression model, however, is not a main effect as in analysis of variance. It is a conditional effect, namely the effect for one particular value of the moderator, that is, the effect within one particular context. To understand this, we must pay close attention to the regression equation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Analysis And A Categorical Moderator</span>"
    ]
  },
  {
    "objectID": "08-moderation-categorical.html#sec-regression-equation",
    "href": "08-moderation-categorical.html#sec-regression-equation",
    "title": "6  Regression Analysis And A Categorical Moderator",
    "section": "6.1 Regression Analysis",
    "text": "6.1 Regression Analysis\nIn the social sciences, we usually expect that a particular outcome has several causes. Investigating the effects of an anti-smoking campaign, for instance, we would not assume that a person’s attitude towards smoking depends only on exposure to a particular anti-smoking campaign. It is easy to think of other and perhaps more influential causes such as personal smoking status, contact with people who do or do not smoke, susceptibility to addiction, and so on. Regression analysis with multiple predictor variables is called a multiple regression analysis.\n\n\n\n\n\n\n\n\nFigure 6.1: A conceptual model with some hypothesized causes of attitude towards smoking.\n\n\n\n\n\nFigure 6.1 summarizes some hypothesized causes of the attitude towards smoking. Attitude towards smoking is measured as a scale, so it is a numerical variable. In linear regression, the dependent variable (\\(y\\)) must be numerical and in principle continuous. There are regression models for other types of dependent variables, for instance, logistic regression for a dichotomous (0/1) dependent variable and Poisson regression for a count dependent variable, but we will not discuss these models.\n\n6.1.1 The regression equation\nA regression model translates this conceptual diagram into a statistical model. The statistical regression model is a mathematical function with the dependent variable (also known as the outcome variable, usually referred to with the letter \\(y\\)) as the sum of a constant, the effects (\\(b\\)) of independent variables or predictors (\\(x\\)), which are predictive effects, and an error term (\\(e\\)), which is also called the residuals, see Equation Equation 6.1.\n\\[\n\\small\n  y = constant + b_1*x_1 + b_2*x_2 + b_3*x_3 + e\n\\normalsize\n\\tag{6.1}\\]\nIf we want to predict the dependent variable (\\(y\\)), we ignore the error term (\\(e\\)) in the equation, and we indicate the prediction by adding a hat to outcome variable \\(\\hat{y}\\). The equation without the error term Eq. 6.2 represents the regression line that we visualize and interpret in the following subsections. We use the error term only when we discuss the assumptions for statistical inference on a regression model in Section 6.1.5.\n\\[\n\\small\n  \\hat{y} = constant + b_1*x_1 + b_2*x_2 + b_3*x_3\n\\normalsize\n\\tag{6.2}\\]\nThough it is conceptually relevant to understand that the error term represents the difference between what the model predicts and what we actually observe.\n\\[\n\\small\n  \\epsilon = y - \\hat{y}\n\\normalsize\n\\tag{6.3}\\]\nFrom Equation 6.3 we can see that the error, also called the residual, is the difference between the observed value of the dependent variable (\\(y\\)) and the predicted value (\\(\\hat{y}\\)). Thus, the better our model predicts the dependent outcome variable, the smaller the residuals.\nThe following sections will explain the regression equation in more detail. We will first discuss the regression equation with a single numerical predictor, then we will discuss the regression equation with a single dichotomous predictor, and finally we will discuss the regression equation with a categorical predictor. The latter requires some tricks to be able to include it in a regression model.\n\n\n6.1.2 A numerical predictor\nLet us first have a close look at a simple regression equation, that is, a regression equation with just one predictor (\\(x\\)). Let us try to predict attitude towards smoking from exposure to an anti-smoking campaign.\n\n\n\n\n\n\n\nFigure 6.2: Predicting attitude towards smoking from exposure to an anti-smoking campaign. The orange dot represents the predicted attitude for the selected value of exposure.\n\n\n\nGood understanding of the regression equation is necessary for understanding moderation in regression models. So let us have a close look at an example equation Eq. 6.4. In this example, the dependent variable attitude towards smoking is predicted from a constant and one independent variable, namely exposure to an anti-smoking campaign.\n\\[\n\\small\n  attitude = constant + b*exposure\n\\normalsize\n\\tag{6.4}\\]\nThe constant is the predicted attitude if a person scores zero on all independent variables. To see this, plug in (replace) zero for the predictor in the equation (Eq. Equation 6.5) and remember that zero times something yields zero. This reduces the equation to the constant.\n\\[\n\\small\n\\begin{split}\n  attitude &= constant + b*0 \\\\\n  attitude &= constant + 0 \\\\\n  attitude &= constant\n\\end{split}\n\\normalsize\n\\tag{6.5}\\]\nFor all persons scoring zero on exposure, the predicted attitude equals the value of the regression constant. This interpretation only makes sense if the predictor can be zero. If, for example, exposure had been measured on a scale ranging from one to seven, nobody can have zero exposure, so the constant has no straightforward meaning.\nThe unstandardized regression coefficient \\(b\\) represents the predicted difference in the dependent variable for a difference of one unit in the independent variable. For example, plug in the values 1 and 0 for the exposure variable in the equation. If we take the difference of the two equations, we are left with \\(b\\). Other terms in the two equations cancel out.\n\\[\n\\small\n\\begin{split}\n  attitude = constant + b*1 \\\\\n  \\underline{- \\mspace{20mu} attitude = constant + b*0} \\\\\n  attitude \\mspace{4mu} difference = b*1 - b*0 = b - 0 = b\n\\end{split}\n\\normalsize\n\\tag{6.6}\\]\n\n\n\n\n\n\nThe unstandardized regression coefficient \\(b\\) represents the predicted difference in the dependent variable for a difference of one unit in the independent variable.\nIt is the slope of the regression line.\n\n\n\nWhether this predicted difference is small or large depends on the practical context. Is the predicted decrease in attitude towards smoking worth the effort of the campaign? In the example shown in Figure 6.2, one additional unit of exposure decreases the predicted attitude by 0.6. This seems to be quite a substantial change on a scale from -5 to 5.\nIn the data, the smallest exposure score is (about) zero, predicting a positive attitude of 1.6. The largest observed exposure score is around eight, predicting a negative attitude of -3.2. If exposure causes the predicted differences in attitude, the campaign would have interesting effects. It may change a positive attitude into a rather strong negative attitude.\nIf we want to apply a rule of thumb for the strength of the effect, we usually look at the standardized regression coefficient (\\(b^*\\) according to APA, Beta in SPSS output). See Section 4.2.8.2 for some rules of thumb for effect size interpretation.\nNote that the regression coefficient is calculated for predictor values that occur within the data set. For example, if the observed exposure scores are within the range zero to eight, these values are used to predict attitude towards smoking.\nWe cannot see this in the regression equation, which allows us to plug in -10, 10, or 100 as exposure values. But the values for attitude that we predict from these exposure values are probably nonsensical (if possible at all: -10 exposure?) Our data do not tell us anything about the relation between exposure and anti-smoking attitude for predictor values outside the observed zero to eight range. We should not pretend to know the effects of exposure levels outside this range. It is good practice to check the actual range of predictor values.\n\n\n6.1.3 Dichotomous predictor\nInstead of a numerical independent variable, we can use a dichotomy as an independent variable in a regression model. The dichotomy is preferably coded as 1 versus 0, for example, 1 for smokers and 0 for non-smokers among our respondents.\n\n\n\n\n\n\n\nFigure 6.3: What is the difference in attitude between non-smokers and smokers?\n\n\n\nThe interpretation of the effect of a dichotomous independent variable in a regression model is quite different from the interpretation of a numerical independent variable’s effect.\nIt does not make sense to interpret the unstandardized regression coefficient of, for example, smoking status as predicted difference in attitude for a difference of one ‘more’ smoking status. After all, the 0 and 1 scores do not mean that there is one unit ‘more’ smoking. Instead, the coefficient indicates that we are dealing with different groups: smokers versus non-smokers.\nIf smoking status is coded as smoker (1) versus non-smoker (0), we effectively have two versions of the regression equation. The first equation Equation 6.7 represents all smokers, so their smoking status score is 1. The smoking status of this group has a fixed contribution to the predicted average attitude, namely \\(b\\).\n\\[\n\\small\n\\begin{split}\n  attitude &= constant + b*status \\\\\n  attitude_{smokers} &= constant + b*1 \\\\\n  attitude_{smokers} &= constant + b\n\\end{split}\n\\normalsize\n\\tag{6.7}\\]\nRegression equation Equation 6.8 represents all non-smokers. Their smoking status score is 0, so the smoking status effect drops from the model.\n\\[\n\\small\n\\begin{split}\n  attitude &= constant + b*status \\\\\n  attitude_{non-smokers} &= constant + b*0 \\\\\n  attitude_{non-smokers} &= constant + 0\n\\end{split}\n\\normalsize\n\\tag{6.8}\\]\nIf you compare the final equations for smokers Eq. 6.7 and non-smokers Eq. 6.8, the only difference is \\(b\\), which is present for smokers but absent for non-smokers. It is the difference between the average score on the dependent variable (attitude) for smokers and the average score for non-smokers. We are testing a mean difference. Actually, this is exactly the same as an independent-samples t test!\n\n\n\n\n\n\nThe unstandardized regression coefficient for a dummy (0/1) variable represents the difference between the average outcome score of the group coded as ‘1’ and the average outcome score of the group coded as ‘0’.\n\n\n\nImagine that \\(b\\) equals 1.6. This indicates that the average attitude towards smoking among smokers (coded ‘1’) is 1.6 units above the average attitude among non-smokers (coded ‘0’). Is this a small or large effect? In the case of a dichotomous independent variable, we should not use the standardized regression coefficient to evaluate effect size. The standardized coefficient depends on the distribution of 1s and 0s, that is, which part of the respondents are smokers. But this should be irrelevant to the size of the effect.\nTherefore, it is recommended to interpret only the unstandardized regression coefficient for a dichotomous independent variable. Interpret it as the difference in average scores for two groups.\n\n\n6.1.4 A categorical independent variable and dummy variables\nHow about a categorical variable containing three or more groups, for example, the distinction between respondents who smoke (smokers), stopped smoking (former smokers), and respondents who never smoked (non-smokers)? Can we include a categorical variable as an independent variable in a regression model? Yes, we can but we need a trick.\n\n\n\n\n\n\n\nFigure 6.4: What are the predictive effects of smoking status?\n\n\n\nIn this example, smoking status is measured with three categories: (1) non-smokers, (2) former smokers, and (3) smokers. Let us use the term categorical variable only for variables containing three or more categories or groups. This makes it easy to distinguish them from dichotomous variables. This distinction is important because we can include a dichotomous variable straight away as a predictor in a regression model but we cannot do so for a variable with more than two categories. We can only include such a categorical independent variable if we change it into a set of dichotomies.\nWe can create a new dichotomous variable for each group, indicating whether (score 1) or not (score 0) the respondent belongs to this group. In the example, we could create the variables neversmoked, smokesnomore, and smoking. Every respondent would score 1 on one of the three variables and 0 on the other two variables (Table 6.1). These variables are called dummy variables or indicator variables.\n\n\n\n\nTable 6.1\n\n\n\n\n\n\n\nOriginal categorical variable:\n\n\nneversmoked\n\n\nsmokesnomore\n\n\nsmoking\n\n\n\n\n\n\n1 - Non-smoker\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n2 - Former smoker\n\n\n0\n\n\n1\n\n\n0\n\n\n\n\n3 - Smoker\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\nDummy variables for a categorical independent variable: One dummy variable is superfluous.\n\n\n\n\n\nIf we want to include a categorical independent variable in a regression model, we must use all dummy variables as independent variables except one. In the example, we must include two out of the three dummy variables. Equation Equation 6.9 includes dummy variables for former smokers (\\(smokesnomore\\)) and smokers (\\(smoking\\)).\n\n\n\n\n\n\nInclude dummy variables as independent variables for all except one categories of a categorical variable.\nThe category without dummy variable is the reference group.\n\n\n\n\\[\n\\small\n\\begin{split}\n  attitude &= constant + b_1*smokesnomore + b_2*smoking\n\\end{split}\n\\normalsize\n\\tag{6.9}\\]\nThe two dummy variables give us three different regression equations: one for each smoking status category. Just plug in the correct 0 or 1 values for respondents with a particular smoking status.\nLet us first create the equation for non-smokers. To this end, we replace both \\(smokesnomore\\) and \\(smoking\\) by 0. As a result, both dummy variables drop from the equation Eq. 6.10, so the constant is the predicted attitude for non-smokers. The non-smokers are our reference group because they are not represented by a dummy variable in the equation.\n\\[\n\\small\n\\begin{split}\n  attitude &= constant + b_1*smokesnomore + b_2*smoking \\\\\n  attitude_{non-smokers} &= constant + b_1*0 + b_2*0 \\\\\n  attitude_{non-smokers} &= constant\n\\end{split}\n\\normalsize\n\\tag{6.10}\\]\nFor former smokers, we plug in 1 for \\(smokesnomore\\) and 0 for \\(smoking\\). The predicted attitude for former smokers equals the constant plus the unstandardized regression coefficient for the \\(smokesnomore\\) dummy variable (\\(b_1\\)), see Equation Equation 6.11. Remember that the constant represents the non-smokers (reference group), so the unstandardized regression coefficient \\(b_1\\) for the \\(smokesnomore\\) dummy variable shows us the difference between former smokers and non-smokers: How much more positive or more negative the average attitude towards smoking is among former smokers than among non-smokers.\n\\[\n\\small\n\\begin{split}\n  attitude &= constant + b_1*smokesnomore + b_2*smoking \\\\\n  attitude_{former smokers} &= constant + b_1*1 + b_2*0 \\\\\n  attitude_{former smokers} &= constant + b_1\n\\end{split}\n\\normalsize\n\\tag{6.11}\\]\nFinally, for smokers, we plug in 0 for \\(smokesnomore\\) and 1 for \\(smoking\\) Eq. 6.12. The predicted attitude for smokers equals the constant plus the unstandardized regression coefficient for the \\(smoking\\) dummy variable (\\(b_2\\)). This regression coefficient, then, represents the difference in average attitude between smokers and non-smokers (reference group).\n\\[\n\\small\n\\begin{split}\n  attitude &= constant + b_1*smokesnomore + b_2*smoking \\\\\n  attitude_{smokers} &= constant + b_1*0 + b_2*1 \\\\\n  attitude_{smokers} &= constant + b_2\n\\end{split}\n\\normalsize\n\\tag{6.12}\\]\nThe interpretation of the effects (regression coefficients) for the included dummies is similar to the interpretation for a single dichotomous independent variable such as smoker versus non-smoker. It is the difference between the average score of the group coded 1 on the dummy variable and the average score of the reference group on the dependent variable. The reference group is the group scoring 0 on all dummy variables that represent the categorical independent variable.\nIf we exclude the dummy variable for the respondents who never smoked, as in the above example, the regression weight of the dummy variable \\(smokesnomore\\) gives the average difference between former smokers and non-smokers. If the regression weight is negative, for instance -0.8, former smokers have on average a more negative attitude towards smoking than non-smokers. If the difference is positive, former smokers have on average a more positive attitude towards smoking.\nWhich group should we use as reference category, that is, which group should not be represented by a dummy variable in the regression model? This is hard to say in general. If one group is of greatest interest to us, we could use this as the reference group, so all dummy variable effects express differences with this group. Alternatively, if we expect a particular ranking of the average scores, we may pick the group at the highest, lowest or middle rank as the reference group. If you can’t decide, run the regression model several times with a different reference group.\nFinally, note that we should not include all three dummy variables in the regression model Eq. 6.9. We can already identify the non-smokers, because they score 0 on both the \\(smokesnomore\\) and \\(smoking\\) dummy variables. Adding the \\(neversmoked\\) dummy variable to the regression model is like including the same independent variable twice. How can the estimation process decide which of the two identical independent variable is responsible for the effect? It can’t decide, so the estimation process fails or it drops one of the dummy variables. If this happens, the independent variables are said to be perfectly multicollinear.\n\n\n6.1.5 Assumptions for regression analysis\nIf we are working with a random sample or we have other reasons to believe that our data could have been different due to chance (Section 4.7.4), we should not just interpret the results for the data set that we collected. We should apply statistical inference—confidence intervals and significance tests—to our results. The confidence interval gives us bounds for plausible population values of the unstandardized regression coefficient. The p value is used to test the null hypothesis that the unstandardized regression coefficient is zero in the population.\nEach regression coefficient as well as the constant may vary from sample to sample drawn from the same population, so we should devise a sampling distribution for each of them. These sampling distributions happen to have a t distribution under particular assumptions.\nChapter 3 and Chapter 4 have extensively discussed how confidence intervals and p values are constructed and how they must be interpreted. So we focus now on the assumptions under which the t distribution is a good approximation of the sampling distribution of a regression coefficient.\n\n6.1.5.1 Independent observations\nThe two most important assumptions require that the observations are independent and identically distributed. These requirements arise from probability theory. If they are violated, the statistical results should not be trusted.\nEach observation, for instance, a measurement on a respondent, must be independent of all other observations. A respondent’s dependent variable score is not allowed to depend on scores of other respondents.\nIt is hardly possible to check that our observations are independent. We usually have to assume that this is the case. But there are situations in which we should not make this assumption. In time series data, for example, the daily amount of political news, we usually have trends, cyclic movements, or issues that affect the amount of news over a period of time. As a consequence, the amount and contents of political news on one day may depend on the amount and contents of political news on the preceding days.\nClustered data should also not be considered as independent observations. Think, for instance, of student evaluations of statistics tutorials. Students in the same tutorial group are likely to give similar evaluations because they had the same tutor and because of group processes: Both enthusiasm and dissatisfaction can be contagious.\n\n\n6.1.5.2 Identically distributed observations\nTo check the assumption of identically distributed observations, we inspect the residuals. Remember, the residuals are represented by the error term (\\(e\\)) in the regression equation. They are the difference between the scores that we observe for our respondents and the scores that we predict for them with our regression model (\\(y - \\hat{y}\\)).\n\n\n\n\n\n\n\nFigure 6.5: What are the residuals and how are they distributed?\n\n\n\nIf we sample from a population where attitude towards smoking depends on exposure, smoking status, and contact with smokers, we will be able to predict attitude from the independent variables in our sample. Our predictions will not be perfect, sometimes too high and sometimes too low. The differences between predicted and observed attitude scores are the residuals.\nIf our sample is truly a random sample with independent and identically distributed observations, the sizes of our errors (residuals) should be normally distributed for each value of the dependent variable, that is, attitude in our example. The residuals should result from chance for the relation between chance and a normal distribution).\nSo for all possible values of the dependent variable, we must collect the residuals for the observations that have this score on the dependent variable. For example, we should select all respondents who score 4.5 on the attitude towards smoking scale. Then, we select the residuals for these respondents and see whether they are approximately normally distributed.\nUsually, we do not have more than one observation (if any) for a single dependent variable score, so we cannot apply this check. Instead, we use a simple and coarse approach: Are all residuals normally distributed?\nA histogram with an added normal curve (like the right-hand plot in Figure 6.5) helps us to evaluate the distribution of the residuals. If the curve more or less follows the histogram, we conclude that the assumption of identically distributed observations is plausible. If not, we conclude that the assumption is not plausible and we warn the reader that the results can be biased.\n\n\n6.1.5.3 Linearity and prediction errors\nThe other two assumptions that we use tell us about problems in our model rather than problems in our statistical inferences. Our regression model assumes a linear effect of the independent variables on the dependent variable (linearity) and it assumes that we can predict the dependent variable equally well or equally badly for all levels of the dependent variable (homoscedasticity, next section).\nThe regression models that we estimate assume a linear model. This means that an additional unit of the independent variable always increases or decreases the predicted value by the same amount. If our regression coefficient for the effect of exposure on attitude is -0.25, an exposure score of one predicts a 0.25 more negative attitude towards smoking than zero exposure. Exposure score five predicts the same difference in comparison to score four as exposure score ten in comparison to exposure score nine, and so on. Because of the linearity assumption, we can draw a regression model as a straight line. Residuals of the regression model help us to see whether the assumption of a linear effect is plausible.\n\n\n\n\n\n\n\nFigure 6.6: How do residuals tell us whether the relation is linear?\n\n\n\nThe relation between an independent and dependent variable, for example, exposure and attitude towards smoking, does not have to be linear. It can be curved or have some other fancy shape. Then, the linearity assumption is not met. A straight regression line does not nicely fit such data.\nWe can see this in a graph showing the (standardized) residuals (vertical axis) against the (standardized) predicted values of the dependent variable (on the horizontal axis), as exemplified by the lower plot in Figure 6.6. Note that the residuals represent prediction errors. If our regression predictions are systematically too low at some levels of the dependent variable and too high at other levels, the residuals are not nicely distributed around zero for all predicted levels of the dependent variable. This is what you see if the association is curved or U-shaped.\nThis indicates that our linear model does not fit the data. If it would fit, the average prediction error is zero for all predicted levels of the dependent variable. Graphically speaking, our linear model matches the data if positive prediction errors (residuals) are more or less balanced by negative prediction errors everywhere along the regression line.\n\n\n6.1.5.4 Homoscedasticity and prediction errors\nThe plot of residuals by predicted values of the dependent variable tells us more than whether a linear model fits the data.\n\n\n\n\n\n\n\nFigure 6.7: How do residuals tell us that we predict all values equally well?\n\n\n\nThe other assumption states that we can predict the dependent variable equally well at all dependent variable levels. In other words, the prediction errors (residuals) are more or less the same at all levels of the dependent variable. This is called homoscedasticity. If we have large prediction errors at some levels of the dependent variable, we should also have large prediction errors at other levels. As a result, the vertical width of the residuals by predictions scatter plot should be more or less the same from left to right. The dots representing residuals resemble a more or less rectangular band.\nIf the prediction errors are not more or less equal for all levels of the predicted scores, our model is better at predicting some values than other values. For example, low values can be predicted better than high values of the dependent variable. The dots representing residuals resemble a cone. This may signal, among other things, that we need to include moderation in the model.\n\n\n6.1.5.5 Checking regression assumptions with SPSS\nVideo 6.2 shows how to check the assumptions of regression analysis in SPSS. The example is based on the smokers.sav data set, which contains data on smoking status and attitude towards smoking. The video shows how to inspect the residuals and the predicted values of the dependent variable.\n\n\n\n\n\n\nVideo 6.2: Inspecting residuals.\n\n\n\n\n\n\n6.1.6 Regression Analysis in SPSS\nIn SPSS, we use the Linear option in the Regression submenu for regression analysis. For a moderation model, we first use the Compute Variable option in the Transform menu to calculate an interaction variable: we multiply (using *) the predictor variable by the moderator variable. The interaction variable is included in the regression model as an independent variable, just like the predictor, moderator, and any other independent variables (covariates).\nA categorical predictor variable such as a participant’s residential area (urban, suburban, rural) is included in the regression model as dummy variables, which have the values 0 or 1, for example: dummy variables suburban and rural, each with values yes (1) and no (0). The category without dummy variable is the reference group.\nThe regression coefficient of a dummy variable gives us the difference between the average score on the dependent variable of the group scoring 1 on the dummy variable and the reference group. In the example presented in Figure 6.8, the attitude towards smoking for participants living in a suburban environment (red box) is on average 0.33 more positive than among participants in an urban environment (the reference group).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.8: SPSS table of regression effects for a model in which the effect of exposure is moderated by participant’s smoking status (reference group: people who never smoked).\n\n\n\nThe predictor variable Exposure is included in the interaction effect. As a consequence, the regression coefficient for this variable (green box in Figure 6.8) expresses the effect of exposure on attitude towards smoking for the reference group on the other variable included in the interaction effect, namely, people who never smoked. A one unit increase in exposure predicts a 0.20 more negative (-0.197) attitude towards smoking for people who never smoked.\nIf we would like to know the effect of exposure on attitude towards smoking for former smokers, we must add the regression coefficient for the interaction of exposure with former smokers (blue box) to the regression coefficient of exposure (green box): A one unit increase in exposure predicts a 0.47 more negative (-0.465 = -0.197 + -0.268) attitude towards smoking for former smokers.\nAn interaction effect such as -0.27 for Former smoker * exposure tells us the difference between the exposure effect for former smokers and the exposure effect for people who never smoked (reference group). A plot of the regression lines shows the different exposure effects (Figure 6.9). The red line (effect of exposure for former smokers) has a stronger downward tendency than the blue line (exposure effect for people who never smoked).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.9: Simple regression lines showing the effect of exposure on attitude towards smoking for former smokers and people who never smoked (in an urban environment).\n\n\n\n\n6.1.6.1 SPSS Simple regression with a numeric predictor\nVideo 6.3 shows how to run and interpret a simple regression analysis with one numeric predictor in SPSS. The video shows how to run a regression analysis with confidence intervals for the regression coefficients. The example is based on the consumers.sav data set, which contains data on brand awareness by advertisement exposure.\n\n\n\n\n\n\nVideo 6.3: Asymmetric association as prediction\n\n\n\n\n\n6.1.6.2 SPSS Simple regression with a categorical predictor\nVideo 6.4 shows how to create dummy variables in SPSS. The example is based on the smokers.sav data set, which contains data on smoking status and attitude towards smoking. The video shows two ways of creating dummy variables: the Transform &gt; Create Dummy Variables option and the Transform &gt; Recode into Different Variables option. The first option is preferred because it is easier to use.\n\n\n\n\n\n\nVideo 6.4: Creating dummy variables in SPSS.\n\n\n\nVideo 6.5 shows how to use dummy variables in a regression model in SPSS. The example is based on the smokers.sav data set, which contains data on smoking status and attitude towards smoking. The video shows how to run a regression analysis with dummy variables.\n\n\n\n\n\n\nVideo 6.5: Using dummy variables in a regression model in SPSS.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Analysis And A Categorical Moderator</span>"
    ]
  },
  {
    "objectID": "08-moderation-categorical.html#sec-categoricalmoderator",
    "href": "08-moderation-categorical.html#sec-categoricalmoderator",
    "title": "6  Regression Analysis And A Categorical Moderator",
    "section": "6.2 Moderation with categorical and continuous predictors",
    "text": "6.2 Moderation with categorical and continuous predictors\nWhat if the effect of campaign exposure on attitude towards smoking may be different in different contexts, e.g., for people who smoke themselves and people who do not smoke? Perhaps, the campaign is more effective among smokers than among non-smokers or the other way around. If so, the effect of campaign exposure is moderated by the smoking status of the participants.\nIn a conceptual diagram (Figure 6.10), moderation is represented by an arrow pointing at another arrow. The moderator (smoking status) changes the relation between the predictor (campaign exposure) and the dependent variable (attitude towards smoking).\n\n\n\n\n\n\n\n\nFigure 6.10: Conceptual diagram of moderation.\n\n\n\n\n\nWe used a similar diagram to express moderation in two-way analysis of variance (Section 5.4). But now at least one of our independent variables is numeric, for example, the number of times the respondent has been exposed to the campaign.\nAnalysis of variance (ANOVA) investigates the effects of categorical variables on a numerical dependent variable. It cannot handle numerical independent variables. Although there are ways to include numerical independent variables in analysis of variance, for example, analysis of covariance (ANCOVA), we use regression analysis if we have a numerical dependent variable and at least one numerical independent variable.\n\n\n\n\n\n\nUse regression analysis if you have a numerical dependent variable and at least one numerical independent variable.\n\n\n\nIn the current section, we discuss regression models with a numerical predictor and a categorical moderator. The next chapter (Chapter 7) presents regression models in which both the predictor and the moderator are numerical.\n\n6.2.1 A dichotomous moderator\n\n\n\n\n\n\n\nFigure 6.11: Is the effect of exposure on attitude moderated by smoking status?\n\n\n\nIn Section 6.1, we have analyzed the predictive effects of exposure to an anti-smoking campaign and smoking status on a person’s attitude towards smoking. We have found a negative effect for exposure and a positive effect for smoking. More exposure predicts a more negative attitude whereas smokers have on average a more positive attitude towards smoking than non-smokers.\nOur current question is: Does exposure to the campaign have the same effect for smokers and non-smokers? We want to compare an effect (exposure on attitude) for different contexts (smokers versus non-smokers), so our current question involves moderation. Is the effect of exposure on attitude moderated by smoking status?\nOur moderator (smoker vs. non-smoker) is a dichotomous variable but our predictor (exposure) is numerical, so we cannot use analysis of variance. Instead, we use regression analysis, which allows numerical predictors.\nIn the context of a regression model, moderation means different slopes for different groups. The slope of the regression line is the regression coefficient, which expresses the effect of the predictor on the dependent variable. If we have different effects in different contexts (moderation), we must have different regression coefficients for different groups.\n\n6.2.1.1 Interaction variable\nHow do we obtain different regression coefficients and lines for smokers and non-smokers? The statistical trick is quite easy: Include an additional predictor in the model that is the product (multiplication) of the predictor (exposure) and the moderator (smoking status). This new predictor is the interaction variable. The regression coefficient of the interaction variable is called the interaction effect.\n\n\n\n\n\n\n\nFigure 6.12: How does an interaction variable create different regression lines for different groups?\n\n\n\nThe interaction variable must be included together with the original predictor and moderator variables, see Equation 6.13. This is also visible in the statistical diagram (Figure 6.13) for moderation in a regression model.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker\n\\end{split}\n\\normalsize\n\\tag{6.13}\\]\n\n\n\n\n\n\n\n\nFigure 6.13: Statistical diagram of moderation.\n\n\n\n\n\nThe smoking status variable is coded 1 for smokers and 0 for non-smokers. For clarity, we name this variable smoker with score 1 for Yes and score 0 for No. We have two different regression equations, one for each group on the dichotomous predictor smoker. Just plug in the two possible values (1 and 0) for this variable. For non-smokers Equation 6.14, the interaction variable drops from the model because multiplying by zero yields zero. For non-smokers, our reference group, \\(b_1\\) represents the effect of exposure on attitude. It is called the simple slope of exposure for non-smokers.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker \\\\\n  attitude_{non-smokers} = &\\ constant + b_1*exposure + b_2*0 + b_3*exposure*0 \\\\\n  attitude_{non-smokers} = &\\ constant + b_1*exposure\n\\end{split}\n\\normalsize\n\\tag{6.14}\\]\nIn contrast, the interaction variable remains in the model for smokers Equation 6.15, who score 1 on smoking status. Note what happens with the coefficient of the exposure effect if we rearrange the terms a little: The exposure effect equals the effect for the reference group of non-smokers (\\(b_1\\)) plus the effect of the interaction variable (\\(b_3\\)). The simple slope for smokers, then, is (\\(b_1 + b_3\\)).\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker \\\\\n  attitude_{smokers} = &\\ constant + b_1*exposure + b_2*1 + b_3*exposure*1 \\\\\n  attitude_{smokers} = &\\ constant + b_1*exposure + b_3*exposure + b_2 \\\\\n  attitude_{smokers} = &\\ constant + (b_1 + b_3)*exposure + b_2\n\\end{split}\n\\normalsize\n\\tag{6.15}\\]\nThe interaction variable changes the slope of the effect of exposure on attitude. More specifically, the regression coefficient of the interaction variable (\\(b_3\\)) shows the difference between the simple slope of the exposure effect for smokers (\\(b_1+b_3\\)) and the simple slope for non-smokers (\\(b_1\\)).\nLet us assume that the unstandardized regression coefficient of the interaction effect is -0.3. This means that the effect of exposure on attitude is more strongly negative (or less positive) for smokers than for non-smokers. One additional unit of exposure decreases the predicted attitude for smokers by 0.3 more than for non-smokers.\n\n\n6.2.1.2 Conditional effects, not main effects\nIt is very important to note that the effects of exposure and smoking status in a model with exposure by smoking status interaction are not main effects as in analysis of variance. As we have seen in the preceding section Equation 6.14, the regression coefficient \\(b_1\\) for exposure expresses the effect of exposure for non-smokers. It is a conditional effect, namely the effect for non-smokers only. Non-smokers are the reference group because they score zero on the moderator (smoker). This is quite different from a main effect in analysis of variance, which is an average effect over all groups.\nIn a similar way, the regression coefficient \\(b_2\\) for smoking status expresses the effect for persons who score zero on the exposure predictor. Simply plug in the value 0 for exposure in the regression equation Eq. 6.16.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ constant + b_1*exposure + b_2*smoker + b_3*exposure*smoker\\\\\n  attitude_{no-expo} = &\\ constant + b_1*0 + b_2*smoker + b_3*0*smoker \\\\\n  attitude_{no-expo} = &\\ constant + b_2*smoker\n\\end{split}\n\\normalsize\n\\tag{6.16}\\]\nSmoking status is a dichotomy, so its regression coefficient (\\(b_2\\)) tells us the average difference in attitude between smokers and non-smokers. Due to the inclusion of the interaction variable, it now tells us the difference in average attitude between smokers and non-smokers who have zero exposure to the anti-smoking campaign. Note again that this is a conditional effect, not a main effect.\n\n\n6.2.1.3 Interpretation and statistical inference\n\n\n\n\nTable 6.2\n\n\n\n\n\n\n\n\n\nB\n\n\nStd. Error\n\n\nBeta\n\n\nt\n\n\nSig.\n\n\nLower Bound\n\n\nUpper Bound\n\n\n\n\n\n\n(Constant)\n\n\n0.900\n\n\n0.357\n\n\n\n\n2.521\n\n\n0.014\n\n\n0.190\n\n\n1.610\n\n\n\n\nExposure\n\n\n-0.162\n\n\n0.061\n\n\n-0.410\n\n\n-2.651\n\n\n0.010\n\n\n-0.284\n\n\n-0.040\n\n\n\n\nStatus (smoker = 1, non-smoker = 0)\n\n\n1.980\n\n\n0.738\n\n\n0.096\n\n\n2.683\n\n\n0.009\n\n\n0.512\n\n\n3.448\n\n\n\n\nExposure*Status (smoker)\n\n\n-0.327\n\n\n0.142\n\n\n-0.238\n\n\n-2.311\n\n\n0.023\n\n\n-0.609\n\n\n-0.045\n\n\n\n\nPredicting attitude towards smoking: regression analysis results.\n\n\n\n\n\nIn Table 6.2, the effect of exposure on attitude depends on the value of smoking status because the model includes an interaction effect of exposure with smoking status (red). Non-smokers are the reference group on the smoking status dummy variable because they are coded 0. Therefore, the regression coefficient for exposure (blue) gives us the effect of exposure on smoking attitude for non-smokers. If you want to check this, plug in 0 for the smoking status variable in the regression equation that we may construct from Table 6.2.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ \\color{#FDAE61}{constant} + \\color{#2B83BA}{b_1*exposure} + \\color{#ABDDA4}{b_2*status} + \\color{#D7191C}{b_3*exposure*status}\\\\\n  attitude_{non-smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#ABDDA4}{1.980*0} + \\color{#D7191C}{-0.327*exposure*0}\\\\\n  attitude_{non-smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#ABDDA4}{0} + \\color{#D7191C}{0}\\\\\n  attitude_{non-smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure}\n\\end{split}\n\\normalsize\n\\tag{6.17}\\]\nMultiplying by zero yields zero, so for non-smokers, the resulting effect of exposure on attitude is -0.16. An additional unit of exposure predicts a smoking attitude among non-smokers that is 0.16 points more negative. More exposure to the campaign goes together with a more negative attitude towards smoking for non-smokers. The p value for this effect tests the null hypothesis that the effect is zero in the population. If the exposure effect is statistically significant, we reject this null hypothesis.\nSmokers are coded 1 on the (smoking) status variable and non-smokers are coded 0, so the regression coefficient for the interaction variable tells us that the slope of the exposure effect is 0.33 lower for smokers than for non-smokers. The estimated slope of the exposure effect is -0.16 for non-smokers. We can add the regression coefficient of the interaction variable to obtain the estimated slope for smokers, which is -0.49.\nIf you want to check this, plug in 1 for the smoking status variable in the regression equation. Add the two effects of exposure in the equation to obtain the effect of exposure on attitude for smokers.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ \\color{#FDAE61}{constant} + \\color{#2B83BA}{b_1*exposure} + \\color{#ABDDA4}{b_2*status} + \\color{#D7191C}{b_3*exposure*status}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#ABDDA4}{1.980*1} + \\color{#D7191C}{-0.327*exposure*1}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#ABDDA4}{1.980} + \\color{#D7191C}{-0.327*exposure}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#D7191C}{-0.327*exposure} + \\color{#ABDDA4}{1.980}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + (\\color{#2B83BA}{-0.162} + \\color{#D7191C}{-0.327})*exposure + \\color{#ABDDA4}{1.980}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + -0.489*exposure + \\color{#ABDDA4}{1.980}\n\\end{split}\n\\normalsize\n\\tag{6.18}\\]\nNow we can compare the slopes (regression coefficients) for the two groups, which gives good insight into the nature of moderation in this example. The effect of exposure on attitude is more strongly negative for smokers (-0.49) than for non-smokers (-0.16).\nThe interaction variable is treated as an ordinary predictor in the estimation process, so it receives a confidence interval and a p value. The null hypothesis for the p value is that the interaction effect is zero in the population. In other words, the effect of exposure on attitude is hypothesized to be the same for smokers and non-smokers in the population; no moderation is expected in the population.\nWe know the confidence intervals and p values of the exposure effect for non-smokers (the regression coefficient for exposure) and for the difference between their exposure effect and the exposure effect for smokers (the regression coefficient for the interaction effect). We do not know, however, the confidence interval and statistical significance of the exposure effect for smokers. We cannot add confidence intervals or p values, so we do not know if the effect of exposure for smokers is significantly different from zero in the population.\nIf you want to know the confidence interval or p value of the exposure effect for smokers, you have to rerun the regression analysis using a different dummy variable for the moderator. You should create a dichotomous variable that assigns 0 to smokers and 1 to non-smokers, and an interaction variable created with this dichotomy. The regression coefficient of the exposure effect now expresses the effect for smokers because smokers are the reference group on the new dummy variable. The associated p value and confidence interval apply to the exposure effect for smokers.\nInteraction variables are used just like ordinary predictors, so the general assumptions of regression analysis apply. See Section 6.1.5 for a description of the assumptions and checks.\nIn a similar way, the effect of smoking status on attitude is conditional on exposure because smoking status and exposure are included in the interaction variable. The regression coefficient for status tells us the difference between smokers and non-smokers who have 0 exposure. So, without exposure to the campaign, smokers are on average 1.98 more positive towards smoking than non-smokers. The p value tests the null hypothesis that the difference is zero for people without exposure (exposure = 0) to the anti-smoking campaign.\nLet us conclude the interpretation with a warning. The standardized regression coefficients that SPSS reports for interaction effects or effects of predictors that are involved in interaction effects must not be used. They are calculated in the wrong way if the regression model includes an interaction variable. As a result, they are misleading.\n\n\n6.2.1.4 PROCESS with 2 level moderator (Dichotomous)\nAs SPSS cannot apply statistical inference to indirect effects, we need to use the PROCESS macro developed for this purpose (Hayes 2013). Instructions on how to add PROCESS to your SPSS installation can be found in Appendix 2. To run the analysis below, PWOCESS needs to be installed in SPSS.\nIn Video 6.6, we show how to use PROCESS to estimate a moderation model with a categorical moderator with two levels. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable status2).\n\n\n\n\n\n\nInstructions on how to add PROCESS to your SPSS installation can be found in Appendix 2.\n\n\n\nIn SPSS we go to Analyze &gt; Regression &gt; PROCESS.... In the dialog box, we select the dependent variable (attitude) as Y variable, the predictor (exposure) as X variable, and the moderator (status2) as Moderator variable W. We select Model 1 for a moderation model with one moderator. Under Options, we select generate code to visualize interaction, and we click on OK to run the analysis.\nWhen PROCESS is finished, we can find the output in the SPSS output window. By double clicking on the output, we can select the generated code for visualizing the interaction. Copy the code, open a new syntax window (File &gt; New &gt; Syntax), and paste the code into the new syntax window. We can run this code to create a plot of the interaction.\nThe plot shows the scatter plot for the relation between exposure and attitude for the two groups of the moderator (non-smokers and smokers). To add the regression lines, we can use the Add Fit Line to subgroups in the Chart Editor. The Chart Editor opens when we double click on the plot.\n\n\n\n\n\n\nVideo 6.6: Estimating categorical by numerical moderation with regression in SPSS using PROCESS.\n\n\n\nVideo 6.7 shows how to interpret the output of PROCESS for a two category moderator. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable status2).\n\n\n\n\n\n\nVideo 6.7: Interpreting the output of PROCESS for a two category moderator.\n\n\n\nThe Model Summary of the PROCESS Output 6.1, contains the \\(R\\), \\(R^2\\), indicating a correlation between the observed and predicted values of the dependent variable (attitude), and the predicted values of the dependent variable (attitude), which explains 23.58% of the variance. The \\(F\\) test indicates that the model is statistically significant.\n\n\n\n\nModel Summary \n          R       R-sq        MSE          F        df1        df2          p \n      .4856      .2358     2.2022     8.3301     3.0000    81.0000      .0001\n\n\n\nOutput 6.1: PROCESS Model Summary output.\n\n\n\nThe ‘Model’ Output 6.2 contains the regression coefficients for the predictor (exposure), the moderator (status2), and the interaction variable Int_1 (exposure*status2). The collumn coeff contains the unstandardized regression coefficients, the column se contains the standard errors of the coefficients, the column t contains the t-values, and the column p contains the p-values. The columns LLCI and ULCI contain the lower and upper limits of the confidence intervals for the coefficients. The Product terms key shows the interaction variable Int_1 and the variables that are multiplied to create the interaction variable. As status2 is a dichotomous variable with values 0 or 1, the interaction term is either the exposure \\(\\times\\) 1 or is cancelled out as exposure is multiplied by 0.\n\n\n\n\nModel \n              coeff         se          t          p       LLCI       ULCI \nconstant      .8998      .3568     2.5215      .0136      .1898     1.6098 \nexposure     -.1620      .0611    -2.6506      .0097     -.2835     -.0404 \nstatus2      1.9798      .7379     2.6830      .0088      .5116     3.4479 \nInt_1        -.3271      .1415    -2.3109      .0234     -.6087     -.0455 \n \nProduct terms key: \n Int_1    :        exposure x        status2\n\n\n\nOutput 6.2: PROCESS Model beta coefficients output.\n\n\n\nThe benefit of using PROCESS is that it provides the conditional effects of the predictor (exposure) at different values of the moderator (status2). With a dichotomous moderator, we have two values: 0 (non-smokers) and 1 (smokers). The conditional effects are shown in the Conditional effects of the focal predictor at values of the moderator(s) table Output 6.3. The column Effect contains the conditional effect of exposure on attitude for non-smokers and smokers.\n\n\n\n\nConditional effects of the focal predictor at values of the moderator(s): \n \n    status2     Effect         se          t          p       LLCI       ULCI \n      .0000     -.1620      .0611    -2.6506      .0097     -.2835     -.0404 \n     1.0000     -.4890      .1277    -3.8307      .0003     -.7430     -.2350 \n\n\n\nOutput 6.3: PROCESS Model conditional effects output.\n\n\n\nWith conditional effects, we mean the effect of exposure on attitude for non-smokers and smokers, the regression slopes at level 0 and level 1 of the moderator. These beta coefficients in the Effect column are the simplified regression slopes for the two groups.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ \\color{#FDAE61}{constant} + \\color{#2B83BA}{b_1*exposure} + \\color{#ABDDA4}{b_2*status} + \\color{#D7191C}{b_3*exposure*status}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#ABDDA4}{1.980*1} + \\color{#D7191C}{-0.327*exposure*1}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#ABDDA4}{1.980} + \\color{#D7191C}{-0.327*exposure}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#2B83BA}{-0.162*exposure} + \\color{#D7191C}{-0.327*exposure} + \\color{#ABDDA4}{1.980}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + (\\color{#2B83BA}{-0.162} + \\color{#D7191C}{-0.327})*exposure + \\color{#ABDDA4}{1.980}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#000000}{-0.489*exposure} + \\color{#ABDDA4}{1.980}\\\\\n  attitude_{smoker} = &\\ \\color{#FDAE61}{0.900} + \\color{#ABDDA4}{1.980} \\color{#000000}{-0.489*exposure}\\\\\n  attitude_{smoker} = &\\ \\color{#000000}{2.880} + \\color{#000000}{-0.489*exposure}\\\\\n  attitude_{smoker} = &\\ \\color{#000000}{2.880} - \\color{#000000}{0.489*exposure}  \n\\end{split}\n\\normalsize\n\\tag{6.19}\\]\nAs we can see from Output 6.3, the slope (effect) for smokers is -0.490, which is the sum of the slope for non-smokers (-0.162) and the interaction effect (-0.327). Plugging in the values in the regression Equation 6.19, and simplifying, we can see on the bottom line, that the slope for exposure is, given some rounding, indeed -0.489.\nIn addition, PROCESS also provides the upper and lower limits of the confidence intervals, and the \\(p\\)-value for the conditional effects. With this we can statistically test the null hypothesis that the conditional effect is zero. And we can see that the confidence intervals of the conditional effects do not include zero.\n\n\n\n6.2.2 A categorical moderator\nWhat if we have three or more groups on our moderator? For example, smoking status measured with three categories: (1) never smoked, (2) formerly smoked, (3) currently smoking? Does the effect of exposure on attitude vary between non-smokers, participants who stopped smoking, and those who are still smoking?\n\n\n\n\n\n\n\nFigure 6.14: When do we have moderation with a categorical moderator?\n\n\n\nIn Section 6.1.4, we learned that we must create dummy variables for all but one groups of a categorical predictor in a regression model. This is what we have to do also for a categorical moderator. If the effect of a predictor, such as exposure, is moderated by a categorical variable, we have to create an interaction variable for each dummy variable in the equation. To create the interaction variables, we multiply the predictor by each of the dummy variables.\n\n\n\n\n\n\n\n\nFigure 6.15: Statistical diagram with a moderator consisting of three groups. Non-smokers are the reference group\n\n\n\n\n\nIn the end, we have an interaction variable for all groups but one on the categorical moderator. Figure 6.15 shows the statistical diagram. Estimation of the model yields point estimates (regression coefficients), confidence intervals, and p values for all independent variables (Table 6.3).\n\n\n\n\nTable 6.3\n\n\n\n\n\n\n\n\n\nB\n\n\nStd. Error\n\n\nt\n\n\nSig.\n\n\nLower Bound\n\n\nUpper Bound\n\n\n\n\n\n\n(Constant)\n\n\n1.644\n\n\n0.288\n\n\n5.717\n\n\n0.000\n\n\n1.072\n\n\n2.216\n\n\n\n\nExposure\n\n\n-0.185\n\n\n0.046\n\n\n-3.987\n\n\n0.000\n\n\n-0.277\n\n\n-0.093\n\n\n\n\nFormer smoker\n\n\n-1.095\n\n\n0.544\n\n\n-2.013\n\n\n0.048\n\n\n-2.178\n\n\n-0.012\n\n\n\n\nSmoker\n\n\n1.235\n\n\n0.521\n\n\n2.372\n\n\n0.020\n\n\n0.199\n\n\n2.272\n\n\n\n\nExposure*Former smoker\n\n\n-0.405\n\n\n0.112\n\n\n-3.604\n\n\n0.001\n\n\n-0.629\n\n\n-0.181\n\n\n\n\nExposure*Smoker\n\n\n-0.304\n\n\n0.098\n\n\n-3.116\n\n\n0.003\n\n\n-0.498\n\n\n-0.110\n\n\n\n\nPredicting attitude towards smoking for three smoking status groups: regression analysis results.\n\n\n\n\n\nRemember that the effects of predictors that are included in interactions are conditional effects: effects for the reference group or reference value on the other variable involved in the interaction. Non-smokers are the reference group for participant’s smoking status. The p value for the exposure predictor tests the hypothesis that the exposure effect for non-smokers is zero in the population.\nFor the two dummy variables Former smoker and Smoker, the null hypothesis is tested that they have the same average attitude in the population as the non-smokers (reference group) if they are not exposed to the anti-smoking campaign. Participants who are not exposed to the campaign (zero exposure) are the reference group here.\nInteraction predictors show effect differences. In Table 6.3, the interaction predictors test the null hypotheses that the effect of exposure on attitude is equal for former smokers and non-smokers (Exposure*Former smoker) or for smokers and non-smokers (Exposure*Smoker) in the population.\nIf we would like to know whether the exposure effect for former smokers is significantly different from zero, we have to rerun the regression model using former smokers as reference group. This new model would also tell us whether the exposure effect for former smokers is significantly different from the exposure effect for people who are still smoking.\n\n6.2.2.1 SPSS dummy variables for interaction\nAs described above, regression analysis with categorical moderators requires dummy and interaction variables. It is always possible to manually create dummy variables for categorical moderators as shown in Video 6.8. However, the PROCESS macro for SPSS is a very useful tool for estimating moderation models with categorical moderators. It is easy to use and it automatically creates interaction terms and dummy variables for you. The add on is free and can be downloaded from Andrew Hayes’ website.\n\n\n\n\n\n\nVideo 6.8: Creating dummy variables for categorical by numerical interaction predictors (2 methods).\n\n\n\n\n\n6.2.2.2 PROCESS with more than 2 levels in moderator (Categorical)\nIf we have more than two levels in our moderator, we can use the same procedure as with the dichotomous analysis. Video 6.9 shows how to use PROCESS to estimate a moderation model with a categorical moderator with more than two levels. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable status3).\nIn the dialog box of PROCESS, we select the dependent variable (attitude) as Y variable, the predictor (exposure) as X variable, and the moderator (status3) as Moderator variable W. We select Model 1 for a moderation model with one moderator. Under Options, we select generate code to visualize interaction.\n\n\n\n\n\n\nInstructions on how to add PROCESS to your SPSS installation can be found in Appendix 2.\n\n\n\nIn addition we have to let PROCESS know that we have a categorical moderator with more than two levels. We do this by clicking the Multicategorical button and indicating that the Moderator variable W is multicatigorical. Click continue and click on OK to run the analysis. Video 6.9 shows how to use PROCESS to estimate a moderation model with a categorical moderator with more than two levels.\n\n\n\n\n\n\nVideo 6.9: Running a moderation analysis in SPSS using PROCESS with a categorical moderator with more than 2 levels and a continuous predictor.\n\n\n\nWhen the analysis is finished, we can find the output in the SPSS output window. By double clicking on the output, we can select the generated code for visualizing the interaction. Copy the code, open a new syntax window (File &gt; New &gt; Syntax), and paste the code into the new syntax window. We can run this code to create a plot of the interaction.\nVideo 6.10 shows how to interpret the output of PROCESS for a categorical moderator with more than two levels. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable status3).\n\n\n\n\n\n\nVideo 6.10: Interpretation of SPSS PROCESS output with a categorical moderator with more than 2 levels and a continuous predictor.\n\n\n\n\n\n6.2.2.3 Common support\nIn a regression model with moderation, we have to interpret the effect of a predictor involved in an interaction at a particular value of the moderator (Section 6.2.1.2). The estimated effect at a particular value of the moderator can only be trusted if there are quite some observations at or near this value of the moderator. In addition, these observations should cover the full range of values on the predictor. After all, the effect that we estimate must tell us whether high values on the predictor go together with higher (or lower) values on the dependent variable than low values on the predictor.\nFor example, we need quite some observations for smokers to estimate the conditional effect of exposure on attitude for smokers. If there are hardly any smokers in our sample, we cannot estimate the effect of exposure on attitude for them in a reliable way. Even if we have quite some observations for smokers but all smokers have low exposure, we cannot say much about the effect of exposure on attitude for them. If we cannot say much about the effect within this group, we cannot say much about the difference between this effect and effects for other groups. In short, the moderation model is problematic in this situation.\n\n\n\n\n\n\n\nFigure 6.16: How well do the observations cover the predictor within each category of smoking status?\n\n\n\nThe variation of predictor scores for a particular value of the moderator is called common support (Hainmueller, Mummolo, and Xu 2016). If common support for predictors involved in moderation is poor, we should hesitate to draw conclusions from the estimated effects. Guidelines for good common support are hard to give. Common support is usually acceptable if there are observations over the entire range of the predictor.\nIt is recommended to check the number of observations per value of the moderator. For a categorical moderator, such as smoking status, a scatter plot of the dependent variable (vertical axis) by predictor (horizontal axis) with dots coloured according to the moderator category may do the job. The left panel of Figure 6.16 shows an example. Check that there are observations for more or less all values of the predictor in each color. If the scatter plot is hard to read, create a histogram of predictor values grouped by moderator categories, as in the right panel of Figure 6.16.\n\nSPSS check common support\nVideo 6.11 shows how to check common support for a predictor at different moderator values in SPSS. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable status2), using contact with smokers as a covariate.\n\n\n\n\n\n\nVideo 6.11: Check common support for a predictor at different moderator values in SPSS.\n\n\n\n\n\n\n6.2.2.4 Visualizing moderation and covariates\nA plot with different regression lines for different categories of the moderator is a very useful way of presenting your results. We can, however, only plot a regression line if we have a single independent variable. After all, we only have one horizontal (X) axis in a plot to display a predictor.\nIn a moderation model, we have at least two independent variables: the predictor and the moderator. For example, exposure is our predictor because our interest focuses on the effect of campaign exposure on attitude. Participant’s smoking status is the moderator because we expect different exposure effects for participants with different smoking statuses. We may even have additional independent variables for which we want to control (more on this in Chapter 8), for example, a participant’s contacts with smokers. Let us call these additional independent variables covariates.\n\n\n\n\n\n\nA predictor is the independent variable that is currently central to our analysis.\nA moderator is an independent variable for which we expect different effects of the predictor.\nA covariate is an independent variable that is currently not central to our analysis.\n\n\n\nNote that the distinction between predictor, moderator, and covariate is temporary. As soon as we focus on another variable, that variable becomes the predictor and the other predictors become moderators or covariates. The distinction between predictor, moderator, and covariate is just terminology to show on which variable we focus.\n\n\n\n\n\n\n\n\nFigure 6.17: Statistical diagram of moderation with contact as covariate.\n\n\n\n\n\nFigure 6.17 shows the statistical diagram of a moderation model with contact as covariate and Table 6.4 summarizes the estimated effects. How can we get rid of the moderator and covariate(s), so exposure is left as the only independent variable and we can plot regression lines for exposure effects?\n\n\n\n\nTable 6.4\n\n\n\n\n\n\n\n\n\nB\n\n\nStd. Error\n\n\nBeta\n\n\nt\n\n\nSig.\n\n\nLower Bound\n\n\nUpper Bound\n\n\n\n\n\n\n(Constant)\n\n\n-0.087\n\n\n0.684\n\n\n\n\n-0.127\n\n\n0.899\n\n\n-1.448\n\n\n1.275\n\n\n\n\nExposure\n\n\n-0.118\n\n\n0.066\n\n\n-0.332\n\n\n-1.786\n\n\n0.078\n\n\n-0.249\n\n\n0.013\n\n\n\n\nSmoker (0 = no, 1 = yes))\n\n\n1.982\n\n\n0.730\n\n\n0.095\n\n\n2.717\n\n\n0.008\n\n\n0.530\n\n\n3.434\n\n\n\n\nExposure*Smoker\n\n\n-0.329\n\n\n0.140\n\n\n-0.239\n\n\n-2.348\n\n\n0.021\n\n\n-0.607\n\n\n-0.050\n\n\n\n\nContact\n\n\n0.152\n\n\n0.090\n\n\n0.180\n\n\n1.683\n\n\n0.096\n\n\n-0.028\n\n\n0.331\n\n\n\n\nPredicting attitude towards smoking: regression analysis results with contact as covariate.\n\n\n\n\n\nAs a first step, use the estimated values of the regression coefficients in the SPSS output (Table 6.4) to create a regression equation (Equation 6.20). Just start at the top of the table and write down the regression coefficients (B) and the independent variable names.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ \\color{#FDAE61}{-0.087} + \\color{#2B83BA}{-0.118*exposure} + \\color{#ABDDA4}{1.982*smoker} + \\color{#D7191C}{-0.329*exposure*smoker}\\\\\n  &+ \\color{brown}{0.152*contact}\n\\end{split}\n\\normalsize\n\\tag{6.20}\\]\nAs a second step, choose an interesting value for every independent variable in the equation except the predictor. If we want to have the regression line for smokers, choose 1 for the smoker variable. For a numerical covariate such as contact, it is recommended to choose the mean. Average contact with smokers happens to be 5.091 in our example. Now replace the independent variables in the equation by the selected values and simplify the equation (Equation 6.21).\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ \\color{#FDAE61}{-0.087} + \\color{#2B83BA}{-0.118*exposure} + \\color{#ABDDA4}{1.982*smoker} + \\color{#D7191C}{-0.329*exposure*smoker}\\\\\n  &+ \\color{brown}{0.152*contact}\\\\\n  attitude = &\\ \\color{#FDAE61}{-0.087} + \\color{#2B83BA}{-0.118*exposure} + \\color{#ABDDA4}{1.982*1} + \\color{#D7191C}{-0.329*exposure*1} + \\color{brown}{0.152*5.091}\\\\\n  attitude = &\\ \\color{#FDAE61}{-0.087} + \\color{#2B83BA}{-0.118*exposure} + \\color{#ABDDA4}{1.982} + \\color{#D7191C}{-0.329*exposure} + \\color{brown}{0.772}\\\\\n  attitude = &\\ (\\color{#FDAE61}{-0.087} + \\color{#ABDDA4}{1.982}+ \\color{brown}{0.772}) + (\\color{#2B83BA}{-0.118*exposure} + \\color{#D7191C}{-0.329*exposure})\\\\\n  attitude = &\\ 2.669 + -0.447*exposure\n\\end{split}\n\\normalsize\n\\tag{6.21}\\]\nThe terms with smoker and contact disappear from the equation, so exposure is the only independent variable that remains in the equation. Now, we can draw the simple regression line predicting attitude from exposure for smokers using this equation. Note that this is the regression line for people with average contact with smokers.\nRepeat these steps but plug in the score 0 for the smoker predictor to obtain the simple regression line for non-smokers. Figure 6.18 shows the two regression lines and their equations. The effect of exposure on attitude is more strongly negative for smokers than for non-smokers.\n\n\n\n\n\n\n\n\nFigure 6.18: The effects of exposure on attitude for non-smokers and smokers. Both smokers and non-smokers are assumed to have average contact with smokers.\n\n\n\n\n\n\nSPSS visualizing interaction\nThough PROCESS can visualize the interaction between a continuous predictor and a categorical moderator, you sometimes want to have some more control over the plot. In this case, you can use the Graphs &gt; Legacy Dialogs &gt; Scatter/Dot menu in SPSS to create a scatterplot with regression lines for different groups of the moderator.\nVideo 6.12 shows how to visualize the interaction between a continuous predictor and a categorical moderator in SPSS. The example uses the smokers.sav data set, predicting the attitude towards smoking from exposure moderated by smoking status (variable status3), using contact with smokers as a covariate.\n\n\n\n\n\n\nVideo 6.12: Visualizing moderation by regression lines in a scatterplot in SPSS.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Analysis And A Categorical Moderator</span>"
    ]
  },
  {
    "objectID": "08-moderation-categorical.html#take-home-points",
    "href": "08-moderation-categorical.html#take-home-points",
    "title": "6  Regression Analysis And A Categorical Moderator",
    "section": "6.3 Take-Home Points",
    "text": "6.3 Take-Home Points\n\nWe use regression anaclysis if our dependent variable is numeric and we have at least one numeric independent (predictor) variable.\nWe use dummy variables to include a categorical variable as a predictor in a regression model. We need a dummy (1/0) variable for each category on the categorical variable except for one category, which represents the reference group.\nWe use an F test to test the null hypothesis that the regression model does not help to predict the dependent variable in the population. We use a t test to test the null hypothesis that a regression coefficient is zero in the population.\nIn a regression model, moderation means that there are different slopes (effects of the predictor) for different groups or contexts (moderator).\nInteraction variables represent moderation in a regression model.\nAn interaction variable is the product of the predictor and moderator. If a categorical moderator is represented by one or more dummy variables, we need an interaction variable for each of the moderator’s dummy variables.\nStatistical inference for an interaction variable is exactly the same as for “ordinary” regression predictors.\nThe effect of the predictor in a model with an interaction variable does not represent a main or average effect. It is a conditional effect: The effect for cases that score zero on the moderator.\nTo interpret moderation, describe the effects (slopes, unstandardized regression coefficients) and visualize the regression lines for different groups.\nWarn the reader if the predictor scores are not nicely distributed for all groups or levels (no common support).\nDon’t use the standardized regression coefficients (Beta) for interaction variables, variables included in interactions, or for dummy variables in SPSS.\n\n\n\n\n\nHainmueller, Jens, Jonathan Mummolo, and Yiqing Xu. 2016. “How Much Should We Trust Estimates from Multiplicative Interaction Models? Simple Tools to Improve Empirical Practice.” https://doi.org/10.2139/ssrn.2739221.\n\n\nHayes, Andrew F. 2013. Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach. Guilford Press.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Regression Analysis And A Categorical Moderator</span>"
    ]
  },
  {
    "objectID": "09-moderation-continuous.html",
    "href": "09-moderation-continuous.html",
    "title": "7  Regression Analysis With A Numerical Moderator",
    "section": "",
    "text": "Summary\nWatch this micro lecture (Video 7.1) on regression models with a numerical moderator for an overview of the chapter.\nChapter 6 shows us how we can include dichotomous and categorical variables as predictors and moderators in a regression model. Using dummy variables, we can analyze mean differences between groups and we can construct different regression lines for different groups (moderation). A graph showing the different regression models for different moderator groups communicates the results of a moderation model in an attractive way.\nWhat if our moderator is not dichotomous or categorical but numerical? For example, the effect of exposure to an anti-smoking campaign on attitude towards smoking can be different for people of different age or for people who spend more time with smokers.\nWe can include a numerical moderator in a regression model just like a dichotomous moderator. Add the predictor, the moderator, and an interaction variable, which is the product of the moderator and the predictor. If both the predictor and moderator are numerical, the interaction variable is numerical. It gives us numbers, not groups.\nThe interpretation of an interaction effect is different if the moderator is numerical instead of dichotomous or categorical. In general, the regression coefficient of a numerical variable expresses the effect of a one unit change. For a numerical predictor, this is the predicted change in the dependent variable. For a numerical moderator, however, it is the predicted change in the effect of the predictor. The unstandardized regression coefficient for a numerical moderator, then, tells us the predicted change in the effect of the predictor for a one unit increase in the moderator.\nThis interpretation is quite abstract and not easy to understand. It is better to visualize the regression lines for different values of the moderator. We usually draw regression lines for three interesting moderator values. The mean value of the moderator shows us the effect at a medium level of the moderator. One standard deviation below or above the mean of the moderator represent attractive low and high moderator values.\nJust like a model with a dichotomous or categorical moderator, the effect of a predictor that is involved in moderation is a conditional effect. In other words, it is the effect of that predictor conditioned under one particular value of the moderator, namely the value zero. Unfortunately, zero is not always a meaningful value for the moderator. If it does not exist or appears rarely on the moderator, it is better to mean-center the moderator. Mean-centering a variable changes the scores such that the mean of the original variable becomes zero on the mean-centered variable. The value zero is always meaningful for a mean-centered variable because it represents the mean score on the original variable. With a mean-centered moderator, the regression coefficient of the predictor always makes sense.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression Analysis With A Numerical Moderator</span>"
    ]
  },
  {
    "objectID": "09-moderation-continuous.html#sec-cont-moderator-regression",
    "href": "09-moderation-continuous.html#sec-cont-moderator-regression",
    "title": "7  Regression Analysis With A Numerical Moderator",
    "section": "7.1 Numerical Moderator",
    "text": "7.1 Numerical Moderator\nWith a categorical moderator, it is quite obvious for which values of the moderator we are going to calculate and depict the effect of the predictor on the dependent variable. If smoking status moderates the effect of exposure on attitude towards smoking, we will inspect a regression line for each smoking status category: smokers, former smokers, and non-smokers. But what if the moderator is a numerical variable, for example, the intensity of contact with smokers?\n\n\n\n\n\n\n\nFigure 7.1: How do contact values affect the conditional effect of exposure on attitude?\n\n\n\nPeople hanging around a lot with smokers may have a more positive attitude towards smoking than people who have little contact with smokers. If people whose company you value are smokers, you are less likely to condemn smoking. This is an overall effect of contact with smokers on attitude towards smoking.\nIn addition, the anti-smoking campaign may be less effective for people who spend a lot of time with smokers. The attitude towards smoking may be stronger among people who spend more time with smokers, so it is more difficult to change the attitude. In this situation, contact with smokers decreases the effect of campaign exposure on attitude. The effect of exposure is moderated by contact with smokers.\nOur moderator, contact with smokers, is numerical. As a consequence, we can have an endless number of contact levels as groups for which the slope may change. This is the only difference with a categorical moderator. Other than that, we will analyze a numerical moderator in the same way as we analyzed a categorical moderator.\n\n7.1.1 Interaction variable\nWe need one interaction variable to include a numerical moderator in a regression model. As before, the interaction variable is the product of the predictor and the moderator. Multiply the predictor by the moderator to obtain the interaction variable.\nAlthough we have an endless number of different moderator values or “groups”, we only need one interaction variable. It represents the gradual (linear) change of the effect of the predictor for higher values of the moderator.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ constant + b_1*exposure + b_2*contact + b_3*exposure*contact \\\\\n  attitude = &\\ constant + (b_1 + b_3*contact)*exposure + b_2*contact\n\\end{split}\n\\normalsize\n\\tag{7.1}\\]\nTo see this, it is helpful to inspect the regression equation with rearranged terms Equation 7.1. Every additional contact with smokers adds \\(b_3\\) to the slope \\((b_1 + b_3*contact)\\) of the exposure effect. The addition is gradual—a little bit of additional contact with smokers changes the exposure effect a little bit—and it is linear: A unit increase in contact adds the same amount to the effect whether the effect is at a low or a high level.\nWe can interpret the regression coefficient of the interaction effect (\\(b_3\\)) here as the predicted change in the exposure effect (slope) for a one unit difference in contact (the moderator). A positive coefficient indicates that the exposure effect is more positive (or less negative) for higher levels of contact with smokers. A negative coefficient indicates that the effect is more negative (or less positive) for people with more contacts with smokers.\nNote that positive and negative are used here in their mathematical meaning, not in an appreciative way. A positive effect of exposure implies a more positive attitude towards smoking. Anti-smoking campaigners probably evaluate this as a negative result.\n\n\n7.1.2 Conditional effect\nIn the presence of an interaction effect of exposure and contact, the regression coefficients for exposure and contact represent conditional effects (see Section 6.2.1.2), namely, the effects for cases that score zero on the other variable. Plug in zero for the moderator and you will see that all terms with a moderator drop from the equation and only \\(b_1\\) is left as the effect of exposure.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ constant + (b_1 + b_3*contact)*exposure + b_2*contact \\\\\n  attitude = &\\ constant + (b_1 + b_3*0)*exposure + b_2*0 \\\\\n  attitude = &\\ constant + b_1*exposure\n\\end{split}\n\\normalsize\n\\tag{7.2}\\]\nThe zero score on the moderator is the reference value for the conditional effect of the predictor. Cases that score zero on the moderator are the reference group just like cases scoring zero on all dummy variables are the reference group in a model with a categorical moderator (Section 6.1.3).\n\n\n7.1.3 Mean-centering\nBecause the effect of a predictor involved in an interaction is a conditional effect, a zero score on the moderator has a special role. It is the reference value for the effect of the predictor. For example, the effect of exposure on attitude applies to respondents with zero contacts with smokers if the regression model includes an exposure by contact interaction. If zero on the moderator is so important as a reference value, we may want to manipulate this value to ensure that it is meaningful.\n\n\n\n\n\n\n\nFigure 7.2: What happens if you mean-center the moderator variable?\n\n\n\nWhat if there are no people with zero contact? Then, the interpretation of the regression coefficient \\(b_1\\) for exposure does not make sense. In this situation, it is better to mean-center the moderator (contact) before you add it to the regression equation and before you calculate the interaction variable.\nTo mean-center a variable, you subtract the variable’s mean from all scores on the variable. As a result, a mean score on the original variable becomes a zero score on the mean-centered variable.\n\\[\n\\small\n  contactcentered = contact - mean(contact)\n\\normalsize\n\\]\nMean-centering shifts the values of a variable such that the mean of the new variable becomes zero (Figure 7.3). Below-average values on the original variable are negative on a mean-centered variable and above-average values are positive. The shape of the distribution remains the same.\n\n\n\n\n\n\n\n\nFigure 7.3: Histograms of the original contacts with smokers variable and the mean-centered variable. The red lines represent the means.\n\n\n\n\n\nWith mean-centered numerical moderators, a conditional effect in the presence of interaction always makes sense. It is the effect of the predictor for respondents who have an average score on the moderator because they score zero on the mean-centered variable. An average score always falls within the range of scores that actually occur. If we mean-center the moderator variable contact with smokers, the regression coefficient \\(b_1\\) for exposure expresses the effect of exposure on attitude for people with average contacts with smokers. This makes sense.\nRemember that the interaction variable is the product of the predictor and moderator (Section 6.2.1.1). If any or both of these are mean-centered, you should multiply the mean-centered variable(s) to create the interaction variable.\nThe regression coefficient of the predictor tells us how much the predicted value of the dependent variable changes for a one unit increase in the predictor score for cases that score zero on the moderator. For example, one additional unit of exposure to the campaign decreases the attitude towards smoking by (-)0.53 for people with zero contacts with smokers (Figure 7.4, red box). The cases that score zero on the moderator (here: people with zero contacts with smokers) are the reference group; zero is the reference value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: SPSS table of regression effects for a model in which the effect of exposure is moderated by contact with smokers: with the original variables (left) and with the moderator mean-centered (right).\n\n\n\nThe effect of exposure on attitude changes if we mean-center the moderator variable contact (Figure 7.4, green box). People with an average number of contacts with smokers score zero on the mean-centered variable, so they are the new reference group. Among people with an average number of contacts with smokers, one additional unit of exposure decreases the predicted attitude by (-)0.31.\nThe regression coefficient of the interaction effect (Figure 7.4, blue box) tells us how much the effect of the predictor changes if the moderator increases by one unit. One additional contact with a smoker increases the effect of exposure on attitude by 0.04, making it 0.04 less strongly negative or more strongly positive.\n\n7.1.3.1 SPSS manually mean center a variable\nVideo 7.2 shows how to mean-center a variable in SPSS. The example data set is smokers.sav, which contains the effect of campaign exposure on attitude towards smoking moderated by contacts that people have with smokers.\nTo mean-center a variable, we first obtain the value of the variable’s mean with the Statistics option in the Frequencies submenu of Descriptive Statistics. Next, we use the Compute Variable option in the Transform menu to subtract this mean from the original variable.\n\n\n\n\n\n\nVideo 7.2: Mean-centering numerical variables.\n\n\n\n\n\n\n7.1.4 Symmetry of predictor and moderator\nIf we want to interpret the conditional effect of contact on attitude (\\(b_2\\)), we must realize that this is the effect for people who score zero on the exposure variable if the exposure by contact interaction is included in the regression model. This is clear if we rearrange the regression equation as in Equation 7.3.\n\\[\n\\small\n\\begin{split}\n  attitude = &\\ constant + b_1*exposure + b_2*contact + b_3*exposure*contact \\\\\n  attitude = &\\ constant + b_1*exposure + (b_2 + b_3*exposure)*contact \\\\\n  attitude = &\\ constant + b_1*0 + (b_2 + b_3*0)*contact \\\\\n  attitude = &\\ constant + b_2*contact\n  \\end{split}\n\\normalsize\n\\tag{7.3}\\]\nBut wait a minute, this is what we would do if contact was the predictor and exposure the moderator. That is a completely different situation, is it not? No, technically it does not make a difference which variable is the predictor and which is the moderator (Figure 7.5). The predictor and moderator are symmetric. The difference is only in our theoretical expectations and in our interpretation.\n\n\n\n\n\n\n\n\nFigure 7.5: Two conceptual diagrams of moderation for the same interaction effect.\n\n\n\n\n\nFor example, let us assume that the regression coefficient of the interaction effect of exposure and contact is 0.2. We can interpret this regression coefficient with contact as moderator and exposure as predictor: An additional unit of contact with smokers increases the effect of exposure on attitude by 0.2. But we can also interpret it with exposure as moderator and contact as predictor: An additional unit of exposure increases the effect of contact with smokers on attitude by 0.2.\nThe conditional effect of the moderator, as stated above, is the effect of the moderator if the predictor is zero. This interpretation makes sense only if there are cases with zero scores on the predictor. In the current example, the scores on exposure range from 0 to 10, so zero exposure is meaningful. But it represents a borderline score with perhaps a very atypical effect of contact on attitude or few observations. For these reasons, it is recommended to mean-center both the predictor and moderator if they are numerical. In case of a dichotomous or categorical moderator (Section 6.2), the predictor can also be mean-centered.\n\n\n7.1.5 Statistical inference on conditional effects\nConditional effect refers to the effect (beta coefficients) of a predictor on the dependent variable for a particular value of the moderator. If we want to test whether the conditional effect is significantly different from zero, we can use the same procedure as for testing the significance of a regression coefficient. We can use a t-test to test whether the regression coefficient is significantly different from zero and look at the upper and lower bounds of the confidence interval for the regression coefficient.\n\n\n\n\nTable 7.1\n\n\n\n\n\n\n\n\n\nB\n\n\nStd. Error\n\n\nBeta\n\n\nt\n\n\nSig.\n\n\nLower Bound\n\n\nUpper Bound\n\n\n\n\n\n\n(Constant)\n\n\n0.283\n\n\n0.186\n\n\n\n\n1.528\n\n\n0.130\n\n\n-0.086\n\n\n0.653\n\n\n\n\nExposure (mean-centered)\n\n\n-0.181\n\n\n0.063\n\n\n-0.321\n\n\n-2.858\n\n\n0.005\n\n\n-0.307\n\n\n-0.055\n\n\n\n\nContact (mean-centered)\n\n\n0.160\n\n\n0.095\n\n\n0.190\n\n\n1.690\n\n\n0.095\n\n\n-0.028\n\n\n0.349\n\n\n\n\nExposure*Contact (mean-centered)\n\n\n0.019\n\n\n0.034\n\n\n0.066\n\n\n0.555\n\n\n0.581\n\n\n-0.049\n\n\n0.086\n\n\n\n\nPredicting attitude towards smoking: regression analysis results with exposure and contact mean-centered.\n\n\n\n\n\nThe regression model yields a p value and confidence interval for the predictor at the reference value of the moderator. In the model estimated in Table 7.1, for instance, we obtain a p value of 0.005 and a 95% confidence interval of [-0.31, -0.05] for the effect of exposure on attitude. This is the conditional effect of exposure on attitude for cases that score zero on the moderator variable (contact with smokers).\nIf the moderator variable contact is mean-centered, the p value tests the null hypothesis that the effect of exposure is zero for people who have average contact with smokers. The confidence interval tells us that the effect of exposure on attitude for people with average contacts with smokers in the population ranges between -0.31 and -0.05 with 95% confidence. If the moderator is not mean-centered, the results apply to people who have no contact with smokers.\nMean-centering of the moderator changes, so to speak, the regression line that we test. Instead of testing the effect of exposure for people with no smoker contact, we test the effect for people with average contact with smokers if the moderator is mean-centered. We might also be interested in the effect of exposure for people with more or less contact with smokers. In that case, we can calculate the conditional effect of exposure at different levels of contact with smokers. This is explained in the next section.\nLooking at Table 7.1, we see that the regression coefficient for the interaction effect of exposure and contact is not significantly different from zero. This means that the effect of exposure on attitude is not moderated by contact with smokers. The effect of exposure on attitude is the same for people who have more contact with smokers.\n\n\n7.1.6 Effect at different moderator values\nIt can be quite tricky to interpret regression coefficients in a regression model that contains interaction effects. The safest strategy is to draw regression lines for different values of the moderator. But what are interesting values if the moderator is numerical?\n\n\n\n\n\n\n\nFigure 7.6: Which moderator values are helpful for visualizing moderation?\n\n\n\nAs we have seen in Section 7.1.1, the regression coefficient of an interaction effect with a numerical moderator can be directly interpreted. It represents the predicted difference in the unstandardized effect size for a one unit increase in the moderator. For example, one more contact with a smoker increases the exposure effect by 0.04.\nThe size of the interaction effect tells us the moderation trend, for instance, people who are more around smokers tend to be less opposed to smoking if they are exposed to the anti-smoking campaign. But we do not know how much an anti-smoking attitude is fostered by exposure to a campaign and whether exposure to the campaign increases anti-smoking attitude for everyone. Perhaps, people hanging out with smokers a lot may even get a more positive attitude towards smoking from campaign exposure.\nWe can be more specific about exposure effects at different levels of contact with smokers if we pick some interesting values of the moderator and calculate the conditional effects at these levels.\nThe minimum or maximum values of the moderator are usually not very interesting. We tend to have few observations for these values, so our confidence in the estimated effect at that level is low. Instead, the values one standard deviation below and above the mean of the moderator are popular values to be picked. One standard deviation below the mean (M - SD) indicates a low value, the mean (M) indicates a central value, and one standard deviation above the mean (M + SD) indicates a high value.\nPerforming inferential statistics on the conditional effects at these values requires the use of the PROCESS macro in SPSS again. The macro calculates the conditional effects at different values of the moderator and provides confidence intervals for these effects.\n\n\n7.1.7 Common support\nIn Section 6.2.2.3, we checked the support of the predictor in the data for different groups of the moderator. The basic idea is that we can only sensibly estimate and interpret a conditional effect at a moderator level if we have observations over the entire range of the predictor. For each moderator group, we checked the distribution of the predictor.\nWith a numerical moderator we can also do this if we group moderator scores. Hainmueller et al. (2016) recommend creating three groups, each containing one third of all observations. These low, medium, and high groups correspond more or less with the minus one standard deviation/mean/plus one standard deviation values that we used for visualizing and testing conditional effects.\n\n\n\n\n\n\n\n\nFigure 7.7: Common support of the predictor variable (exposure) at three levels of the moderator variable (contact).\n\n\n\n\n\nAccording to Figure 7.7, the predictor variable exposure covers the entire range from 0 to 10 at medium and high contact levels. At low contact level, however, the lowest exposure score is 1 instead of zero. In all, we have common support for moderation of the exposure effect by contact for exposure scores from 1 to 10. This is quite a broad range but we should note that we have few observations of low exposure at the low contact level as well as few observations of high exposure at the high contact level.\n\n7.1.7.1 SPSS check common support\nThe video below (Video 7.3) shows how to check common support of the predictor variable in the data for three groups of the moderator variable.\n\n\n\n\n\n\nVideo 7.3: Checking common support with a continuous moderator.\n\n\n\n\n\n\n7.1.8 Assumptions\nThe general assumptions for regression analysis (Section 6.1.5) also apply to a regression model with a moderator (interaction effect). The checks are the same: See if the residuals are more or less normally distributed and check the residuals by predicted values plot.\nNote that the linearity assumption also applies to the interaction effect. If the interaction effect is positive, the exposure (predictor) effect must be higher for higher values of contact with smokers (moderator). More precisely, a unit difference on the moderator should result in a fixed increase (or decrease) of the effect of the predictor. You may have noticed this linear change in the effect size in Figure 7.1 at the beginning of this section on numerical moderators.\nIt is difficult to check this assumption, so let us not pursue this here. Just remember that the interaction effect is assumed to be linear: a gradually increasing or decreasing effect of the predictor at higher moderator values.\n\n\n7.1.9 Higher-order interaction effects\nAn interaction effect with one moderator, whether numerical or categorical, is called first-order interaction or two-way interaction. It is possible to have a moderated effect that is moderated itself by a second moderator. For example, the change in the exposure effect due to a person’s contact with smokers may be different for smokers than for non-smokers. This is called a second-order interaction or three-way interaction. We can include more moderators, yielding even higher-order interactions, such as three or four moderators.\nAn interaction variable that is the product of the predictor and two moderators can be used to include a second-order interaction in a regression model. If you include a second-order interaction, you must also include the effects of the variables involved in the interaction as well as all first-order interactions among these variables in the regression model. All in all, these models become very complicated to interpret and they are beyond the scope of the current course.\n\n\n7.1.10 SPSS moderation analysis in PROCESS\nAs our previous example for the effect of exposure on attitude towards smoking moderated by contact with smokers did not show a significant interaction effect in Table 7.1, we will look at another example for running the moderation analysis in SPSS using the PROCESS macro.\nThe example covers the effects of playing video games on aggressive behavior moderated by callous unemotional traits.\n\nVideo games are among the favourite online activities for young people: two-thirds of 5–16-year-olds have their own video games console, and 88% of boys aged 8–15 own at least one games console. Although playing violent video games can enhance visuospatial acuity, visual memory, probabilistic inference, and mental rotation, compared to games such as Tetris, these games have also been linked to increased aggression in youths Another predictor of aggression and conduct problems is callous-unemotional traits such as lack of guilt, lack of empathy, and callous use of others for personal gain.\nImagine a scientist wanted to look at the relationship between playing violent video games such as Grand Theft Auto, MadWorld and Manhunt and aggression. She gathered data from 442 youths. She measured their aggressive behaviour, callous unemotional traits, and the number of hours per week they play video games (Field, 2017, sec. 11.3.1)\n\nFor this example we again use the PROCESS macro for SPSS. Installation instructions can be found in Section 2. We use the agression.sav data file for this analysis.\nIn SPSS we go to Analyze &gt; Regression &gt; PROCESS ..., in the PROCESS dialog, we select the Model Number 1 for a simple moderation model with one moderator. We select gaming as the predictor X variable, agress as the dependent Y variable, and unemot as the dependent Moderator variable W.\nUnder Options, we select Generate code for visualizing interactions, we can mean center our variables by selecting all variables that define products, and for Moderation and conditioning we select -1SD, Mean, +1SD and tick the Johnson-Neyman output box. click continue to set these options.\nThe PROCESS macro does not like long variable names, so we recommend to use short variable names in the data file. If you have long variable names, you can change them in the SPSS data editor. You can also accept the risk of incorrect output by indicating this under long variable names.\nFinally, we click OK to run the analysis. The output will contain the regression coefficients for the predictor, moderator, and interaction variable, as well as the Johnson-Neyman output for the moderator. Video 7.4 below shows how to run the analysis in SPSS.\n\n\n\n\n\n\nVideo 7.4: Running a moderation analysis with PROCESS in SPSS.\n\n\n\nPROCESS produces the output in plain text. It covers the Model Summary, the Model, a seperate ANOVA test for the interaction, the conditianal effects of the predictor at one standard deviation below the mean, at the mean, and one standard deviation above the mean of the moderator, and the Johnson-Neyman output. Video 7.5 shows you how to interpret the output of the PROCESS macro in SPSS.\n\n\n\n\n\n\nVideo 7.5: Running a moderation analysis with PROCESS in SPSS.\n\n\n\nThe model summary indicates the amount of variance explained by the model, the F-test for the overall model, and the significance of the model.\n\nModel Summary \n          R       R-sq        MSE          F        df1        df2          p \n      .8289      .6871     9.5143    70.2656     3.0000    96.0000      .0000\n\nNext the model coefficients are reported. The coefficients for the predictor, moderator, and interaction variable are reported in the Model section. The coefficients are unstandardized regression coefficients, their standard errors, t-values, p-values, and confidence intervals. Just like we get from a regular SPSS regression analysis. In these results we see that the interaction is significant, as the p-value is below .05. PROCESS therefore also reports the conditional effects.\n\nModel \n              coeff         se          t          p       LLCI       ULCI \nconstant     5.3788      .3829    14.0491      .0000     4.6189     6.1388 \ngaming        .7027      .1546     4.5446      .0000      .3958     1.0096 \nunemot       -.2163      .1089    -1.9869      .0498     -.4324     -.0002 \nInt_1         .3699      .0265    13.9729      .0000      .3174      .4225 \n\nThe higher order unconditional interaction effect is reported in the Test(s) of highest order unconditional interaction(s) section. This is the F-test for change in explained variance of the interaction term.\n\nProduct terms key: \n Int_1    :        gaming   x        unemot \n \nTest(s) of highest order unconditional interaction(s): \n       R2-chng          F        df1        df2          p \nX*W      .6364   195.2424     1.0000    96.0000      .0000 \n---------- \n    Focal predict: gaming   (X) \n          Mod var: unemot   (W) \n\nThe conditional effects of the predictor at one standard deviation below the mean, at the mean, and one standard deviation above the mean of the moderator are reported in the Conditional effects of focal predictor at values of the moderator(s) section. The conditional effect is the effect (regression slope) of the predictor on the dependent variable at these particular values of the moderator. The p-value indicates whether this effect is significantly different from zero.\nBecause we asked PROCESS to mean center, the moderator values in the first column are not the original values of the moderator variable, but the mean-centered values. To interpret the results, we can add the mean of the moderator to the mean-centered values. We need to use SPSS to calculate the mean for the variable unemot in the data file through Analysis &gt; Descriptive Statistics &gt; Descriptives..., select the variable unemot, and make sure the mean is ticked in the Options tab. The mean of unemotional traits is 7.81, we can use this to describe the results below as the effect at one standard deviation below the mean (7.81 - 4.12 = 3.69), at the mean (7.81), and one standard deviation above the mean (7.81 + 4.12 = 11.93).\n\nConditional effects of the focal predictor at values of the moderator(s): \n \n     unemot     Effect         se          t          p       LLCI       ULCI \n    -4.1229     -.8226      .1821    -4.5179      .0000    -1.1840     -.4612 \n      .0000      .7027      .1546     4.5446      .0000      .3958     1.0096 \n     4.1229     2.2280      .1962    11.3546      .0000     1.8385     2.6175 \n\nThe Johnson-Neyman output is reported in the Johnson-Neyman significance regions section. This section reports the values of the moderator that define the significance regions for the predictor. The significance region is the range of values of the moderator for which the conditional effect of the predictor is significantly different from zero.\n\nModerator value(s) defining Johnson-Neyman significance region(s): \n      Value    % below    % above \n    -2.7902    29.0000    71.0000 \n    -1.0680    40.0000    60.0000 \n\nThe next section of the PROCESS output reports the conditional effect of the predictor at many values of the moderator, it includes the standard error, t-value, p-value, and confidence intervals. The Effect column provides the beta coefficient for the predictor at the corresponding moderator value as indicated in the first column on the left side of the effect value. For example the first row indicates that the effect of gaming on aggression is -2.4446 at a value of -8.5075 for unemotional traits. This means that for people with such a low amount of unemotional traits, the effect of gaming on aggression is negative, i.e., more gaming leads to less aggression. The columns on the right side test whether this effect is significantly different from zero, and provides the lower and upper bound of the confidence interval. This is provided for the entire range of mean centered moderator values in each row. Note that if you choose not to mean center the moderator, the values in the first column will be the original values of the moderator variable.\n\nConditional effect of focal predictor at values of the moderator: \n     unemot     Effect         se          t          p       LLCI       ULCI \n    -8.5075    -2.4446      .2629    -9.2983      .0000    -2.9665    -1.9228 \n    -7.5128    -2.0766      .2421    -8.5787      .0000    -2.5571    -1.5961 \n    -6.5180    -1.7086      .2224    -7.6829      .0000    -2.1501    -1.2672 \n    -5.5233    -1.3406      .2042    -6.5644      .0000    -1.7460     -.9352 \n    -4.5286     -.9726      .1880    -5.1734      .0000    -1.3458     -.5994 \n    -3.5338     -.6046      .1743    -3.4695      .0008     -.9505     -.2587 \n    -2.7902     -.3295      .1660    -1.9850      .0500     -.6590      .0000 \n    -2.5391     -.2366      .1636    -1.4459      .1515     -.5615      .0882 \n    -1.5443      .1314      .1568      .8381      .4041     -.1798      .4426 \n    -1.0680      .3076      .1550     1.9850      .0500      .0000      .6152 \n     -.5496      .4994      .1542     3.2395      .0016      .1934      .8054 \n      .4451      .8674      .1560     5.5600      .0000      .5577     1.1771 \n     1.4399     1.2354      .1622     7.6181      .0000      .9135     1.5573 \n     2.4346     1.6034      .1722     9.3125      .0000     1.2616     1.9452 \n     3.4293     1.9714      .1854    10.6322      .0000     1.6033     2.3394 \n     4.4241     2.3394      .2013    11.6243      .0000     1.9399     2.7389 \n     5.4188     2.7074      .2191    12.3561      .0000     2.2725     3.1423 \n     6.4136     3.0754      .2386    12.8919      .0000     2.6019     3.5489 \n     7.4083     3.4434      .2592    13.2841      .0000     2.9289     3.9579 \n     8.4030     3.8114      .2808    13.5722      .0000     3.2540     4.3688 \n     9.3978     4.1794      .3032    13.7851      .0000     3.5776     4.7812 \n    10.3925     4.5474      .3261    13.9432      .0000     3.9000     5.1948 \n\nFinally, the output contains the data for visualizing the conditional effect of the focal predictor. The data can be used to create a scatterplot with the predictor on the x-axis, the dependent variable on the y-axis. By pasting this into a syntax editor and executing it, we can create a plot that visualizes the regression slopes (conditional effects) at the 3 levels of the moderator, one standard deviation below the mean, at the mean, and one standard deviation above the mean of the moderator.\n\nData for visualizing the conditional effect of the focal predictor: \nPaste text below into a SPSS syntax window and execute to produce plot. \n \nDATA LIST FREE/ \n   gaming     unemot     agress     . \nBEGIN DATA. \n    -2.9068    -4.1229     8.6616 \n      .0000    -4.1229     6.2706 \n     2.9068    -4.1229     3.8796 \n    -2.9068      .0000     3.3362 \n      .0000      .0000     5.3788 \n     2.9068      .0000     7.4215 \n    -2.9068     4.1229    -1.9893 \n      .0000     4.1229     4.4871 \n     2.9068     4.1229    10.9634 \nEND DATA. \nGRAPH/SCATTERPLOT= \n gaming   WITH     agress   BY       unemot   .",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression Analysis With A Numerical Moderator</span>"
    ]
  },
  {
    "objectID": "09-moderation-continuous.html#sec-RegressionContModSPSS",
    "href": "09-moderation-continuous.html#sec-RegressionContModSPSS",
    "title": "7  Regression Analysis With A Numerical Moderator",
    "section": "7.2 Numerical and categorical moderators",
    "text": "7.2 Numerical and categorical moderators\nIn the smokers dataset we not only have the results from the anti-smoking campaign, and contact with smokers, but also information about the smoking status of the participants, which could be non-smoker, former smoker, or smoker. We can use this information to test whether the effect of exposure to the anti-smoking campaign on attitude towards smoking is not only moderated by the amount of contact with smokers but also by the smoking status. In other words, we want to know whether the moderating effect of contact with smokers on the effect of exposure on attitude is different for smokers, former smokers, and non-smokers.\n\n\n\n\n\n\n\n\nFigure 7.8: Conceptual diagram of the estimated moderation model.\n\n\n\n\n\n\n7.2.1 SPSS numerical and categorical moderator with PROCESS\nFor this example we again use the PROCESS macro for SPSS. Installation instructions can be found in Section 2. We use the smokers.sav data file for this analysis. We will look at the effect of campaign exposure on attitude towards smoking moderated by the amount of contact that people have with smokers and their own smoking status being non-smoker, former smoker or smoker.\nIn SPSS we go to Analyze &gt; Regression &gt; PROCESS ..., in the PROCESS dialog, we select Model Number 2 for a moderation model with two moderators and a second order interaction. We select exposure as the predictor X variable, and attitude as the dependent Y variable. We set contact as the continuous numerical Moderator variable W, and status3 as the Moderator variable Z. The variable status3 is a categorical variable with three levels: non-smoker (0), former smoker (1), and smoker (2). The PROCESS macro will create dummy variables for the categorical moderator, but we do have to indicate that this variable is categorical by clicking the Multicategorical butten and ticking the Multicategorical for Variable Z. You can leave the Coding system at Indicator.\nUnder Options, we select Generate code for visualizing interactions, we can mean center our variables by selecting all variables that define products, and for Moderation and conditioning we select -1SD, Mean, +1SD, but we can leave the Johnson-Neyman output un-ticked. Click continue to set these options.\nThe PROCESS macro does not like long variable names, so we recommend to use short variable names in the data file. If you have long variable names, you can change them in the SPSS data editor. You can also accept the risk of incorrect output by indicating this under long variable names.\nFinally, we click OK to run the analysis. The output will contain the regression coefficients for the predictor, moderator, and interaction variable, as well as the Johnson-Neyman output for the moderator. Video 7.6 will show how to run the analysis in SPSS.\n\n\n\n\n\n\nVideo 7.6: Moderation analysis with a continuous and a categorical moderator in SPSS using the PROCESS macro.\n\n\n\nThe results produced by the PROCESS macro are similar to the results of the previous example, but now we have two moderators. The output contains the regression coefficients for the predictor, moderators, and interaction variables, as well as the conditional effects of the predictor at one standard deviation below the mean, at the mean, and one standard deviation above the mean.\nBecause we have a categorical moderator, we have to identify the reference group for the categorical moderator. The PROCESS macro creates the two required dummy variables Z1 and Z2 for the categorical moderator status3. We can use the first section of the output to determine the reference group. The matrix below shows the coding of the categorical moderator variable status3 for the analysis. Dummy variables Z1 has the value 1 in the second row, indicating the status category 1 to be active. This category corresponded to being a former smoker. The dummy variable Z2 has the value 1 in the third row, indicating the status category 2 to be active. This category corresponds to being a smoker. The first row is the reference group, which is non-smoker (status3 = 0). Hence Z1 represents the former smoker group, and Z2 represents the smoker group.\n\nModel  : 2 \n    Y  : attitude \n    X  : exposure \n    W  : contact \n    Z  : status3 \n \nSample \nSize:  85 \n \nCoding of categorical Z variable for analysis: \n status3      Z1      Z2 \n    .000    .000    .000 \n   1.000   1.000    .000 \n   2.000    .000   1.000\n\nThe Product terms key section indicates the interaction variables that are created by PROCESS. The interaction variable Int_1 is the product of the predictor exposure and the continuous moderator contact. The interaction variables Int_2 and Int_3 are the products of the predictor exposure and the dummy variables Z1 (former smoker) and Z2 (smoker), respectively. These interaction variables represent the interaction effect of the predictor with the categorical moderator.\n\nProduct terms key: \n Int_1    :        exposure x        contact \n Int_2    :        exposure x        Z1 \n Int_3    :        exposure x        Z2 \n\nThe Model Summary section provides the overall model fit statistics, including the R-squared value, which indicates the proportion of variance explained by the model, and the F-statistic for the overall model significance. The Model section provides the regression coefficients for the predictor, moderators, and interaction variables, along with their standard errors, and confidence intervals. Along with the t-values, p-values testing the for the null hypothesis that the beta coefficients are zero. The coefficients indicate the effect of each variable on the dependent variable attitude.\n\nModel Summary \n          R       R-sq        MSE          F        df1        df2          p \n      .8063      .6502     1.0719    20.4440     7.0000    77.0000      .0000 \n \nModel \n              coeff         se          t          p       LLCI       ULCI \nconstant     -.8151      .1749    -4.6594      .0000    -1.1634     -.4667 \nexposure     -.1715      .0584    -2.9374      .0044     -.2878     -.0552 \ncontact       .2165      .0666     3.2513      .0017      .0839      .3491 \nInt_1         .0802      .0251     3.1968      .0020      .0302      .1301 \nZ1          -1.5775      .2451    -6.4358      .0000    -2.0656    -1.0894 \nZ2            .0524      .3428      .1530      .8788     -.6302      .7351 \nInt_2        -.2210      .0880    -2.5126      .0141     -.3961     -.0459 \nInt_3        -.0285      .1098     -.2598      .7957     -.2471      .1901\n\nThe Test(s) of highest order unconditional interaction(s) section provides the F-test for the interaction effect, which tests the increase in explained variance by the interactions.\n\nTest(s) of highest order unconditional interaction(s): \n        R2-chng          F        df1        df2          p \nX*W       .0464    10.2197     1.0000    77.0000      .0020 \nX*Z       .0311     3.4235     2.0000    77.0000      .0376 \nBOTH      .0718     5.2675     3.0000    77.0000      .0023\n\nFinally, the Conditional effects of focal predictor at values of the moderator(s) section provides the conditional effects of the predictor at the mean, one standard deviation below the mean, and one standard deviation above the mean of the continuous moderator contact, for each level of the categorical moderator status3. The conditional effects are reported for each combination of the categorical moderator levels and the continuous moderator values. The p-values indicate whether the conditional effect is significantly different from zero.\n\n\n\n\n\n\nNote\n\n\n\nThe PROCESS macro only outputs the conditional effects if the interaction effect is statistically significant.\n\n\n\nConditional effects of the focal predictor at values of the moderator(s): \n \n    contact    status3     Effect         se          t          p       LLCI       ULCI \n    -1.8271      .0000     -.3180      .0640    -4.9663      .0000     -.4455     -.1905 \n    -1.8271     1.0000     -.5390      .0823    -6.5490      .0000     -.7029     -.3751 \n    -1.8271     2.0000     -.3465      .1083    -3.1989      .0020     -.5623     -.1308 \n      .0000      .0000     -.1715      .0584    -2.9374      .0044     -.2878     -.0552 \n      .0000     1.0000     -.3925      .0672    -5.8391      .0000     -.5263     -.2586 \n      .0000     2.0000     -.2000      .0912    -2.1927      .0314     -.3817     -.0184 \n     1.8271      .0000     -.0250      .0832     -.3004      .7647     -.1906      .1406 \n     1.8271     1.0000     -.2460      .0804    -3.0595      .0031     -.4061     -.0859 \n     1.8271     2.0000     -.0535      .0954     -.5606      .5767     -.2436      .1365\n\nThe output for visualizing the conditional effect of the focal predictor is similar to the previous example. The text can be copied to a syntax file, to create a scatterplot with the predictor on the x-axis, the dependent variable on the y-axis for each level of the categorical moderator. This is shown in (id-SPSSintModContCat?) below.\n\n\n\n\n\n\nVideo 7.7: Interpreting the results of a moderation analysis with a continuous and a categorical moderator in SPSS using the PROCESS macro.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe graphs produced by PROCESS, for categorical moderators, are only visualized correctly if the Measure in the Variable View in SPSS is set to Nominal for the categorical moderator variable.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression Analysis With A Numerical Moderator</span>"
    ]
  },
  {
    "objectID": "09-moderation-continuous.html#sec-reportmoderation",
    "href": "09-moderation-continuous.html#sec-reportmoderation",
    "title": "7  Regression Analysis With A Numerical Moderator",
    "section": "7.3 Reporting Results",
    "text": "7.3 Reporting Results\nIf we report a regression model, we first present the significance test and predictive power of the entire regression model. We may report that the regression model is statistically significant, F (7, 77) = 20.44, p &lt; 0.001, so the regression model very likely helps to predict attitude towards smoking in the population.\nHow well does the regression model predict attitude towards smoking? The effect size of a regression model or its predictive power is summarized by \\(R^2\\) (R Square), which is the proportion of the variance in the dependent variable scores (attitude towards smoking) that can be predicted with the regression model. In this example, \\(R^2\\) is 0.65, so the regression model predicts 65% of the variance in attitude towards smoking among the respondents. In communication research, \\(R^2\\) is usually smaller.\n\\(R^2\\) tells us how well the regression model predicts the dependent variable in the sample. Every predictor that we add to the regression model helps to predict results in the sample even if the predictor does not help to predict the dependent variable in the population. For a better idea of the predictive power of the regression model in the population, we may use Adjusted R Square. Adjusted R Square is usually slightly lower than R Square. In the example, Adjusted R Square is 0.62 (not reported in Table 7.2).\n\n\n\n\nTable 7.2\n\n\n\n\n\n\n\n\n\nB\n\n\n95% CI\n\n\n\n\n\n\nConstant\n\n\n-0.82***\n\n\n[-1.16, -0.47]\n\n\n\n\nExposure\n\n\n-0.17**\n\n\n[-0.29, -0.06]\n\n\n\n\nContact\n\n\n0.22**\n\n\n[ 0.08, 0.35]\n\n\n\n\nFormer smoker\n\n\n-1.58***\n\n\n[-2.07, -1.09]\n\n\n\n\nSmoker\n\n\n0.05\n\n\n[-0.63, 0.74]\n\n\n\n\nExposure * Contact\n\n\n0.08**\n\n\n[ 0.03, 0.13]\n\n\n\n\nExposure * Former smoker\n\n\n-0.22*\n\n\n[-0.40, -0.05]\n\n\n\n\nExposure * Smoker\n\n\n-0.03\n\n\n[-0.25, 0.19]\n\n\n\n\nR2\n\n\n0.65\n\n\n\n\n\n\nF (7, 77)\n\n\n20.44***\n\n\n\n\n\n\n\n\nNote.  N = 150. CI = confidence interval.\n\n\n\n\n * p &lt; .05. ** p &lt; .01. *** p &lt; .001.\n\n\n\n\nPredicting attitude towards smoking with smoking status and contact with smokers as moderators. Results in APA style. Exposure and contact are mean-centered.\n\n\n\n\n\nAs a next step, we discuss the size, statistical significance, and confidence intervals of the regression coefficients. If a predictor is involved in one or more interaction effects, we must be very clear about the reference value or reference group to which the effect applies. In the example below, non-smokers are the reference group on the smoking status variable because they are not represented by a dummy variable. Average number of contacts with smokers is the reference value on the contact variable because this variable is mean-centered.\nExposure, in our example, has a negative predictive effect on attitude towards smoking (b = -0.17) for non-smokers with average contacts with smokers, t = -2.94, p = 0.004, 95% CI [-0.29, -0.06]. Note that SPSS does not report the degrees of freedom for the t test on a regression coefficient, so we cannot report them.\nInstead of presenting the numerical results in the text, we may summarize them in an APA style table, such as Table 7.2. Note that t and p values are not reported in this table, the focus is on the confidence intervals. The significance level is indicated by stars.\nA sizable and statistically significant interaction effect signals that an effect is moderated. In the example reported in Table 7.2, the effect of exposure on attitude seems to be moderated by contact with smokers (b = 0.08, p = 0.002) and by smoking status (b = -0.22, p = 0.014 for former smoker).\nThe regression coefficients for interaction effects must be interpreted as effect differences. For a categorical moderator, the coefficient describes the effect size difference between the category represented by the dummy variable and the reference group. The negative effect of exposure is stronger for former smokers than for the reference group non-smokers. The average difference is -0.22.\nFor a numerical moderator, we can interpret the general pattern reflected by the interaction effect. A positive interaction effect, such as 0.08 for the interaction between exposure and smoker contact, signals that the effect of exposure is more strongly positive or less strongly negative at higher levels of contact with smokers.\nThis interpretation in terms of effect differences remains difficult to understand. It is recommended to select some interesting values for the moderator and report the size of the effect for each value. For a categorical moderator, each category is of interest. For a numerical moderator, the mean and one standard deviation below and above the mean are usually interesting values. The regression coefficients show whether the effect is positive, negative, or nearly zero at different values of the moderator.\nVisualize the regression lines for different values of the moderator in addition to presenting the numerical results. If the regression model contains covariates, mention the values that you have used for the covariates. Select one of the categories for a categorical covariate. For numerical covariates, the mean is a good choice. If you are working with mean-centered predictors, be sure to use the mean-centered predictor for the horizontal axis (as in Figure 7.9), not the original predictor.\n\n\n\n\n\n\n\n\n\n\n\n(a) Effects for groups with different smoking status (at average contact with smokers).\n\n\n\n\n\n\n\n\n\n\n\n(b) Effects at different levels of contact with smokers (effects for non-smokers).\n\n\n\n\n\n\n\nFigure 7.9: The effect of exposure on attitude towards smoking.\n\n\n\nThe left panel in Figure 7.9 clearly shows that the effect of exposure on attitude is more or less the same for non-smokers and smokers. The effect is different for former smokers, for whom the exposure effect is more strongly negative. It is more difficult to communicate this conclusion with the table of regression coefficients.\nCheck that the predictor has good support at the selected values of the moderator. In the left-hand plot of Figure 7.9, the groups (colours) vary nicely over the entire range of the predictor exposure, so that is okay. We need histograms to check common support for the right-hand plot.\nDo not report that common support of the predictor at different moderator values is good. If it is bad, warn the reader that we cannot fully trust the estimated moderation because we do not have a nice range of predictor values within each level of the moderator. If the predictor is supported only within a restricted range, you may report this range.\nFinally, inspect the residual plots but do not include them in the report. Warn the reader if the assumptions of the linear regression model have not been met. Do not mention the assumptions if they have been met.\nCommunicate the results of a numerical moderator in a scatterplot with regression lines for the effect of the predictor at three moderator values: the mean value of the moderator, one standard deviation below the mean and one standard deviation above the mean (Figure 7.10). The PROCESS macro provides the syntax for these plots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.10: Plot of the effect of exposure on attitude towards smoking for three levels of the moderator variable: contact with smokers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression Analysis With A Numerical Moderator</span>"
    ]
  },
  {
    "objectID": "09-moderation-continuous.html#take-home-points",
    "href": "09-moderation-continuous.html#take-home-points",
    "title": "7  Regression Analysis With A Numerical Moderator",
    "section": "7.4 Take-Home Points",
    "text": "7.4 Take-Home Points\n\nAn interaction variable represents moderation in a regression model also if the moderator is numerical.\nAn interaction variable is the product of the predictor and moderator.\nThe effect of the predictor in a model with an interaction variable does not represent a main or average effect. It is a conditional effect: The effect for cases that score zero on the moderator. The same applies to the effect of the moderator, which is the conditional effect for cases scoring zero on the predictor.\nThe unstandardized regression coefficient of the interaction variable specifies the predicted change in the effect of the predictor on the dependent variable for a one unit increase in the moderator variable.\nWe recommend to mean-center a numerical moderator and a numerical predictor that are involved in an interaction effect. Observations with a mean score on the moderator are a substantively interesting reference group.\nTo interpret moderation, describe the effects (slopes, unstandardized regression coefficients) and visualize the regression lines for some interesting levels of the moderator, such as the mean and one standard deviation below or above the mean.\n\n\n\n\n\nHainmueller, Jens, Jonathan Mummolo, and Yiqing Xu. 2016. “How Much Should We Trust Estimates from Multiplicative Interaction Models? Simple Tools to Improve Empirical Practice.” https://doi.org/10.2139/ssrn.2739221.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression Analysis With A Numerical Moderator</span>"
    ]
  },
  {
    "objectID": "10-confounding.html",
    "href": "10-confounding.html",
    "title": "8  Regression Analysis And Confounders",
    "section": "",
    "text": "Summary\nWatch this micro lecture on confounders for an overview of the chapter (Video 8.1).\nIf we analyze the effects of two or more predictors on a dependent variable in a regression model, the effect of a predictor is adjusted for the effects of other predictors. Each predictor only predicts the part of the scores on the dependent variable that cannot be predicted by the other predictors. If we predict newspaper reading time, for example, from age and interest in politics, age predicts the part of newspaper reading time that interest in politics cannot predict.\nBecause the effects of predictors are adjusted for the effects of other predictors, the effects of predictors may change if a new predictor is added to a regression model. The effects can become stronger (the new variable was a suppressor) or weaker and even change direction (the new variable was a reinforcer). For example, adding respondents’ news site use to the regression model predicting newspaper reading time from age may change the effect of age on newspaper reading time.\nIndirect correlations play a central role here: the correlation between a predictor (age) and the dependent variable (newspaper reading time) due to a third variable (news site use) that is correlated both with the predictor and the dependent variable. The size of the indirect correlation is the product of the correlation between the predictor (age) and third variable (news site use) and the correlation between the third variable (news site use) and the dependent variable (newspaper reading time).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis And Confounders</span>"
    ]
  },
  {
    "objectID": "10-confounding.html#sec-controlling",
    "href": "10-confounding.html#sec-controlling",
    "title": "8  Regression Analysis And Confounders",
    "section": "8.1 Controlling for Effects of Other Predictors",
    "text": "8.1 Controlling for Effects of Other Predictors\nIn a regression model, we use the variation in scores on independent variables to predict the variation of scores on the dependent variable: Does a person with a higher score on an independent variable also have a higher score or, on the contrary, a lower score on the dependent variable? A simple regression model contains only one independent variable but a multiple regression model includes more than one.\nFor example, European citizens who are more interested in politics spend more time on reading newspapers and so do citizens who are older. We have two independent variables (interest in politics and age) to predict the dependent variable (newspaper reading time). The two independent variables can be correlated: Older citizens tend to be more interested in politics. How does the regression model decide which independent variable is responsible for which part of the variation in the dependent variable?\n\n\n\n\n\n\n\nFigure 8.1: How do regression coefficients change if new predictors for reading time are added to the model?\n\n\n\n\n8.1.1 Partial effect\nHow does a multiple regression model control the effect of an independent variable for the effects of all other independent variables? Conceptually, the regression model first removes the variation in the dependent variable that is predicted by all other independent variables. Then it determines how well the remaining independent variable predicts the variation that is left (residual variation). This is the variation in outcome scores that can be predicted by this particular independent variable but not by any of the other independent variables in the model.\nIn this sense, a regression coefficient in a multiple regression model expresses the unique contribution of a variable to the prediction of the dependent variable. It is the contribution to the prediction of the dependent variable over and above the predictions that we can make with all other independent variables in the model. This is called a partial effect. This is what we mean if we say that we are controlling for all other independent variables in our interpretation of a regression model.\n\n\n8.1.2 Confounding variables\nIt is important to note that the effect is only unique in comparison to the other independent variables that are included in the model. It may well be that we did not include variables in the model that are actually responsible for part of the effects that are attributed to the independent variables in the model. Such left-out variables are called confounding variables or, for short, confounders.\nIf we include a confounder as a new independent variable in the model, the partial effects of other independent variables in the model change. In Figure 8.1, for instance, this happens if you add news site use to a model containing age as a predictor for newspaper reading time. The effects of other independent variables are adjusted to a new situation, namely a situation with news site use as a new independent variable. News site use helps to predict variation in the dependent variable, so the variation left to be explained by age changes. In Section 8.3, we will learn that regression coefficients can increase and decrease if confounders are included in the model.\nIf we want to interpret regression coefficients as causal effects, for example, whether news site use causes people to spend less time on reading newspapers, we must ensure that there are no important confounders. We will discuss this in Chapter 9 (Section 9.1.1).\n\n8.1.2.1 Confounders are not included in the regression model\nFinally, it is important to remember that a confounder, such as age in the present example, is a variable that is not included in our regression model. As long as it is not included, the indirect correlation between predictor (political interest) and outcome (newspaper reading time) due to the confounder (age) is not controlled for when the effect of the predictor is estimated. The estimated effect is confused (confounded) with the effect of the confounder.\nOnce the confounder (age) is added to the regression model, however, the estimated effects are controlling for the variable formerly known as a confounder. The effects no longer partly represent the effect of the former confounder. In other words, they are no longer confounded by the effect of that variable. The former confounding variable now is a predictor or, if we are not interested in its effects, a covariate or control variable in the regression model.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis And Confounders</span>"
    ]
  },
  {
    "objectID": "10-confounding.html#sec-indirectcorrelation",
    "href": "10-confounding.html#sec-indirectcorrelation",
    "title": "8  Regression Analysis And Confounders",
    "section": "8.2 Indirect Correlation",
    "text": "8.2 Indirect Correlation\n\n\n\n\n\n\n\nFigure 8.2: What happens to the regression coefficient for the effect of political interest if we add a confounder to the model? Numbers represent correlations (lines) or standardized regression coefficients (arrows).\n\n\n\nWhen is a variable a confounder and when does it change the effect of another predictor a lot if it is added to the regression model? The answer to the first part of this question is easy: A confounder is a variable that is correlated with both the predictor and dependent variable but is not (yet) included in the regression model. Because of the correlations, a confounder establishes an indirect correlation between the predictor and dependent variable.\n\n\n\n\n\n\nA confounder is a variable that is correlated with both the predictor and dependent variable but is not included in the regression model.\n\n\n\nThe size of the indirect correlation equals the product of the correlation between confounder and predictor and the correlation between confounder and dependent variable. In Figure 8.2, the correlation between age and political interest is .12 and the correlation between age and newspaper reading time is .88, the indirect correlation between interest in politics and reading time established by age is .12 * .88 = .11.\n\n8.2.1 Indirect correlation and size of confounding\nIn Figure 8.2, we start with a simple regression model with political interest as the only predictor of newspaper reading time. Respondent’s age, however, is correlated both with political interest (\\(r\\) = 0.12) and with newspaper reading time (\\(r\\) = 0.88). Age creates a positive indirect correlation between political interest and reading time.\nAs long as age is not included in the regression model, the model believes that the indirect correlation due to age is part of the effect of political interest. It assigns the indirect correlation due to age to the effect of political interest, that is, it includes the indirect correlation in the regression coefficient of political interest. In this situation, the regression coefficient for political interest expresses both the effect of political interest itself and the effect of age (the confounder).\nOnce we add age as a new predictor to the regression model, the indirect correlation due to age is removed from the effect of political interest. The effect of age on newspaper reading time is now correctly assigned to age. As a result, the value of the regression coefficient for political interest changes if we add age as a new predictor.\nThe size of the change is related to the size of the indirect correlation. The larger the indirect correlation, the more the regression coefficient of political interest changes if age (the former confounder) is included as a new predictor. This answers the second part of the question with which we started Section 8.2: When is a variable a stronger confounder?\nIf you love the details: The size of the change in the standardized regression coefficient is not exactly the same as the size of the indirect correlation. It is equal to the correlation between the confounder (age) and the predictor (political interest) times the standardized regression coefficient of the effect of the confounder (age) on the dependent variable (newspaper reading time) that controls for the effect of the predictor (political interest).\n\n\n8.2.2 Randomization for avoiding confounders\nThere is a very important way to minimize the chance of having any confounders at all, namely, randomization in an experiment. Remember the example of Chapter 5, where participants saw a video clip with Angelina Jolie, George Clooney, or no celebrity endorsing a charity. The video clip with or without a celebrity endorser is the experimental treatment here. If we let chance decide which video clip a participant sees, we randomize the experimental treatment.\nHow does this help us to avoid having confounders? The example research aimed to find out whether the celebrity endorser affects the willingness to donate to a charity. The experimental treatment (celebrity endorser video) is the independent variable or predictor variable in the model. If participants’ scores on this variable — in the example, seeing Jolie, Clooney, or no celebrity endorser — are random, the variable is expected not to correlate with any other characteristic of the participants when the experiment starts.\nFor example, female and male participants would have the same chance to see Jolie, Clooney, or no endorser. We expect one third of all females and one third of all males to see Jolie, to see Clooney, and to see no endorser. If there is no systematic difference between females and males in this respect, participant’s experimental treatment is not correlated with sex of the participant. The same reasoning applies to every other characteristic of the participant at the start of the experiment: age, hair colour, favourite movie star, and so on. We expect that all of these participant characteristics are not correlated with the experimental treatment variable.\nLet us now turn to the definition of a confounder: A confounder is a variable that is correlated with both the predictor and dependent variable but is not included in the regression model. A confounder must be correlated with the predictor, which is the experimental treatment here. Thanks to experimental randomization, we expect that all participant characteristics that are not included in the experiment do not meet this criterion. There should not be any confounders!\nWe have learned about probabilities and expectations in previous chapters. These principles also apply to experimental randomization. Even if we may expect to have equal numbers of females and males seeing Angelina Jolie in our example experiment, we can end up with more females than males seeing Jolie in our experiment due to chance. In this situation, the experimental treatment variable is correlated with the sex of the participant, so participant sex is a confounder if it is also correlated with the dependent variable (willingness to donate) and not included in the analysis. Experimental randomization does not guarantee that there are no confounders but it is our best instrument to minimize the chance of having confounders.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis And Confounders</span>"
    ]
  },
  {
    "objectID": "10-confounding.html#sec-confounders",
    "href": "10-confounding.html#sec-confounders",
    "title": "8  Regression Analysis And Confounders",
    "section": "8.3 Two Types of Confounders",
    "text": "8.3 Two Types of Confounders\n\n\n\n\n\n\n\nFigure 8.3: When is a regression effect too large and when is it too small due to a confounder? Numbers represent correlations (lines) or standardized regression coefficients (arrows).\n\n\n\nIn the preceding section, we learned that a partial effect expressed by a regression coefficient may change if a new predictor is added to the regression model. The partial effect of a predictor changes if the added variable is a confounder: It is correlated both with the predictor and dependent variable. In other words, there is an indirect correlation between the predictor and dependent variable due to the confounding variable.\nThe partial effect of a predictor can become stronger, weaker, or even change direction if we add a confounder to the regression model. The following sections describe the two types of confounders that are responsible for these changes: suppressors and reinforcers.\n\n8.3.1 Suppression\nA predictor’s effect becomes stronger (more strongly positive or more strongly negative) if we include a confounding variable that is responsible for an indirect correlation that points in the opposite direction of the effect of the predictor in the model without the confounder. Here, the indirect correlation contradicts the effect of the predictor and as a result, the effect of the predictor is underestimated (suppressed) if the confounder is not included in the model. The confounder is a suppressor variable. If we add it to the model, it no longer suppresses the effect of the predictor, so this effect becomes stronger.\nThere are two situations in which an indirect correlation can have the opposite sign of the effect of a predictor:\n\nThe indirect correlation is negative but the effect of the predictor is positive.\nThe indirect correlation is positive but the effect of the predictor is negative.\n\nWe start with the first situation and discuss the second situation later on in this section.\n\n\n\n\n\n\n\n\nFigure 8.4: News site use as a confounder of the effect of interest in politics on newspaper reading time.\n\n\n\n\n\nLet us assume that political interest has a positive effect on reading newspapers. People who are more interested in politics tend to spend more time on reading newspapers than people who are less interested in politics. The use of news sites confounds this effect if it is correlated with both political interest and newspaper reading time. What happens if people interested in politics use news sites more often (positive correlation) because they offer the latest political news but using news sites decreases newspaper reading time (negative correlation) because most of the political information has already been provided by the news sites?\nIn this situation, the indirect correlation between political interest and newspaper reading time due to news site use is negative: Positive times negative yields a negative. The indirect correlation tells us that people interested in politics use news sites more frequently but people who frequently use news sites read newspapers less often. The indirect correlation clearly contradicts the regression effect of political interest on newspaper reading time, which is positive: People who are more interested in politics spend more time on reading newspapers.\nIf news site use is not included in the regression model, the standardized regression effect of political interest more or less adds the indirect correlation to the effect of political interest. Adding a negative amount (indirect correlation), however, is equal to subtracting this amount from the standardized regression coefficient. The positive effect of political interest on reading time is underestimated. In this example, the effect of political interest is suppressed (masked) by the confounder news site use. News site use is a suppressor variable.\nIf we include this suppressor variable (news site use) in our regression model, we eliminate its suppression of the effect of political interest on newspaper reading time. The negative effect of news site use on reading time is now captured by the regression coefficient for the news site use predictor. The effect of political interest on newspaper reading time is now controlled for the effect of news site use; it no longer includes the indirect correlation due to news site use. In this example, the effect of political interest on newspaper reading time becomes more strongly positive.\n\n\n\n\n\n\n\n\nFigure 8.5: Interest in politics as a confounder of the effect of news site use on newspaper reading time.\n\n\n\n\n\nNow, let us have a look at the situation in which the indirect correlation is positive but the regression effect of the predictor is negative. Just reverse the example and make news site use the predictor and political interest the confounder. The regression effect of news site use on newspaper reading time is negative if people tend to use news sites instead of newspapers as sources of information. The indirect correlation due to political interest, however, is positive if politically interested people use news sites more and spend more time on reading newspapers. In this scenario, the negative effect of news site use on newspaper reading is underestimated if we do not control for political interest.\n\n\n\n\n\n\nA variable is a suppressor (1) if it is not included in the regression model and (2) it establishes an indirect correlation between predictor and dependent variable that has the opposite sign of the current effect of the predictor on the dependent variable.\n\n\n\nSuppression can have surprising effects. If the predictor’s original effect was close to zero, adding a suppressor variable to the model will strengthen the effect. An effect that we initially believed to be absent may turn out to be substantial and statistically significant. If our regression model tells us that our predictor does not have an effect, we cannot rule out that it does have an effect that is suppressed by a suppressor variable.\nIn addition, indirect correlations due to other predictors can add so much to the original partial effect of a predictor that the standardized regression coefficient becomes higher than 1 or lower than -1. This illustrates that standardized regression coefficients are not correlations in multiple regression models because correlations can never be higher than 1 or lower than -1. In contrast, the standardized regression coefficient in a simple regression model is equal to the correlation between predictor and outcome. This is an important difference between simple and multiple regression models.\n\n\n8.3.2 Reinforcement and spuriousness\nAdding a new predictor to a regression model may weaken the effects of other predictors or even change the direction of effects. This happens if the indirect correlation due to a confounder has the same direction (sign) as the regression effect of the predictor in the model without the confounder. Either the indirect correlation and regression effect are both positive or they are both negative.\nIn both situations, regression effects are initially overestimated because the predictors cover part of the effect of an important variable that has not yet been added to the regression model. The part of the effect that is due to the confounding variable is called spurious. The confounding variable is called a reinforcer because it makes an effect appear more strongly positive or more strongly negative than it really is as long as the confounder has not been added to the regression model.\n\n\n\n\n\n\n\n\nFigure 8.6: Age as a confounder of the effect of interest in politics on newspaper reading time.\n\n\n\n\n\nAs an example, the effect of political interest on newspaper reading time may include the effect of age on newspaper reading when age is not (yet) included in the regression model. If older people are more interested in politics and do more newspaper reading, age creates a positive indirect correlation between political interest and newspaper reading.\nIf age is not included as a predictor in the regression model, the indirect correlation is attributed to the effect of interest in politics. The estimated effect is too strong. Once we include age as a predictor, the effect of political interest is cleansed of the age effect, so the effect size decreases.\nIn Figure 8.6, age is positively correlated with both political interest and newspaper reading. But a confounder that is negatively correlated with predictor and outcome has the same impact as a confounder that is positively correlated with predictor and outcome. Political cynicism, for instance, can be negatively correlated with both interest in politics and newspaper reading time (Figure 8.7). People who are less cynical about politics are more interested in politics and spend more time on reading newspapers. As a result, it looks like political interest strongly increases newspaper reading time but higher newspaper reading time is at least partly due to less political cynicism. Similar scenarios are available if the regression effect and the indirect correlation are negative.\n\n\n\n\n\n\n\n\nFigure 8.7: Political cynicism as a confounder of the effect of interest in politics on newspaper reading time.\n\n\n\n\n\nAs with suppression, spuriousness can have surprising results. It may happen that the entire estimated effect of a predictor is spurious. Adding a reinforcer variable to the regression model may make the entire effect of a predictor disappear. In other words, an effect that we initially thought was substantial may turn out to be too weak to be of interest.\nActually, the indirect correlation between a predictor and dependent variable due to a confounding variable can be so strong that a positive effect in a model without the confounder changes into a negative effect in a model that includes the variable. Adding the reinforcer to the model, the effect of the predictor not only moves towards zero (becoming weaker), but it moves beyond zero into a negative effect. It may even move so far beyond zero that the new negative effect is stronger than the reinforced positive effect.\nThe opposite may happen as well: An initially negative effect may become positive if a strong reinforcer variable is added to the model. This would be the case if the indirect correlation between political interest and newspaper reading time via news site use is strongly negative, resulting in a negative effect of political interest on reading time if news site use is left out of the model. Adding news site use to the model may then result in a positive effect of political interest.\n\n\n\n\n\n\nA variable is a reinforcer (1) if it is not included in the regression model and (2) it establishes an indirect correlation between predictor and dependent variable that has the same sign as the current effect of the predictor on the dependent variable.\n\n\n\nTo summarize the two types of confounders:\n\n\n\n\n\n\n\nIf we add a suppressor to the model, the suppressed effect moves away from zero because suppression disappears. A positive effect becomes more strongly positive, a negative effect becomes more strongly negative.\nIf we add a reinforcer to the model, the reinforced effect moves towards the opposite side because reinforcement disappears. A positive effect becomes less strongly positive or even negative and a negative effect becomes less strongly negative or even positive.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis And Confounders</span>"
    ]
  },
  {
    "objectID": "10-confounding.html#sec-compmodelSSPSS",
    "href": "10-confounding.html#sec-compmodelSSPSS",
    "title": "8  Regression Analysis And Confounders",
    "section": "8.4 Comparing Regression Models in SPSS",
    "text": "8.4 Comparing Regression Models in SPSS\n\n8.4.0.1 Essential Analytics\nWe can detect confounders by adding each independent variable as a separate Block in a linear regression model (the Linear option in the Regression submenu). The SPSS output estimates a regression model for each block (Figure 8.8).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.8: SPSS table of regression models with the independent variables interest in politics, age, and news site use added one by one.\n\n\n\nIn the first model, interest in politics is the only predictor. One additional unit of interest in politics adds 1.13 to the predicted newspaper time per day (Figure 8.8, red box). If age is added as a predictor in the second model, the effect of interest in politics decreases from 1.13 to 0.32 (Figure 8.8, green box). Age reinforced the effect of interest in politics in the first model. The third model adds the predictor variable news site use. Now, the effect of interest in politics increases from 0.32 to 0.53 (Figure 8.8, blue box); news site use suppressed the effect of interest in politics in the second model.\n\n\n8.4.1 Instructions\nIn the video below we will demonstrate how to identify confounders by comparing regression models in SPSS.\nLet us imagine the situation where we are part of a political communication research team. Elections are coming up and we are interested in whether people are reading the news, getting information on the elections and campaigns, and which factors might be of influence there. Our main interest is the relationship between newspaper reading time and news site use. We will be adding education and age as possible confounders in our model. By adding them into the model, these possible confounders become covariates.\nWe will conduct what we call a stepwise regression. This means we will add the predictors to the SPSS model one by one, as we discussed in Section 8.4.0.1. We perform a regression as always, through analyse &gt; regression &gt; linear. We add readingtime as the dependent variable. In the independent(s) window, we add newssite - click next, we add education - click next - and we add age. In the Statistics dialog, we click confidence interval, 95%. Then we run the analysis, please remember to first click paste and then run the analysis through the syntax file.\nIn the video the assumptions of regression analysis are not getting checked since this is not the focus of this chapter. Which is why we are not getting into those settings at the moment. Please refer to Chapter 6 and 7 if you want to get into the assumptions.\nIn the output we can see the table Variables Entered/Removed, here we can see if we entered the variables in the correct order. In the next table, the Model Summary we can collect details on the R and R-squared of the different models. R-squared is the explained variance. The table Coefficients provides us with the other regression output; the unstandardized regression coefficient, the standard error, the standardized regression coefficient, the t-value, the p-value and the confidence interval.\nIn the Coeffcients table we can compare the models to explore whether our added variables were confounders. Please note that we can only compare subsequent models. Meaning in this output we can compare model 2 to model 1 and we can compare model 3 to model 2. We are unable to compare model 3 to model 1. If we look at the results of model 3 compared to model 2, we see that the unstandardized regression coefficients of both news site use and education level (in years) move closer to zero. The unstandardized regression coefficient of News site use become less negative (the unstandardized regression coeffcient of -5.770 becomes -1.297) and the unstandardized regression coefficient of Education level becomes less positive (the unstandardized regression coefficient of .463 becomes .176). In other words, the individual effects of both news site use and education level become weaker once we add the variable age. Hence, based on comparing the results of model 3 to model 2, we can conclude that the predictor age (a covariate in model 3) was a reinforcing confounder in model 2.\nWatch the video below for the step by step instructions, more details and additional information.\n\n\n\n\n\n\nVideo 8.2: Stepwise regression.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis And Confounders</span>"
    ]
  },
  {
    "objectID": "10-confounding.html#take-home-points",
    "href": "10-confounding.html#take-home-points",
    "title": "8  Regression Analysis And Confounders",
    "section": "8.5 Take-Home Points",
    "text": "8.5 Take-Home Points\n\nIn a multiple regression model, a regression coefficient represents the (predictive) effect of a variable while controlling for the effects of all other predictors. It is called a partial effect: It predicts variance of the dependent variable that cannot be predicted by the other predictors.\nIf a new predictor is added to a regression model, the regression coefficient of an old predictor changes if the new predictor is correlated with both the old predictor and the dependent variable. If the old predictor’s effect becomes stronger, the new predictor was a suppressor. If it becomes weaker (the old effect was—partially—spurious) or changes direction (sign), the new predictor was a reinforcer.\nRandom assignment of participants to experimental treatments (the independent variable/predictor in an experiment) is meant to create (near) zero correlations between the predictor and any other variable not included in the experiment. As a result, we expect that there are no confounders.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression Analysis And Confounders</span>"
    ]
  },
  {
    "objectID": "11-mediation.html",
    "href": "11-mediation.html",
    "title": "9  Mediation with Regression Analysis",
    "section": "",
    "text": "Summary\nWatch this micro lecture on mediation for an overview of the chapter (Video 9.1).\nIf we add a causal order among the predictors of our regression model, we obtain a causal model or path model. The causal model includes an indirect effect: The first predictor affects the scores on the second predictor, which affects the scores on the dependent variable that we usually call the outcome variable. For example, age affects news site use, which affects newspaper reading time. In this path model, the second predictor (news site use) mediates the effect of the first predictor on the outcome variable. The second predictor is called a mediator.\nWe can estimate a path model as a series of regression models. The size of an indirect effect equals the product of the direct effects that constitute the indirect effect. With additional software, we can estimate the confidence interval of an indirect effect.\nMediation models help us to think about the different ways in which a variable may cause another variable. But we must realize that the causal order underlying the model is an assumption that we make. A regression model shows the predictive effects, which do not have to be causal. We cannot prove that the predictive effects are causal. We can only think of arguments that make a causal interpretation of a predictive effect plausible.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#mediation-as-causal-process",
    "href": "11-mediation.html#mediation-as-causal-process",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.1 Mediation as Causal Process",
    "text": "9.1 Mediation as Causal Process\n\n\n\n\n\n\n\nFigure 9.2: How does a common cause affect regression coefficients? The values in this path diagram represent standardized regression coefficients.\n\n\n\n\n9.1.1 Criteria for a causal relation\nResearchers are usually interested in causal effects, so let us theorize a causal order between age and reading newspapers. From previous research and personal experience, we strongly suspect that older people spend more time on reading newspapers than young people. In statistical language, we expect a positive correlation between age and newspaper reading time. Can age be a cause of current newspaper reading?\nCorrelation is the first criterion. A causal relation implies correlation or another type of statistical association. If newspaper reading is not correlated with age, it is hard to imagine that age affects newspaper reading. But correlation does not imply causation, as the saying goes. Correlated variables do not have to be causally related. We need additional arguments to add plausibility to a causal relation.\nThe second criterion is the time order between cause and consequence. A cause must appear before the consequence. In our example, a person’s age must be fixed before she displays the behaviour that we want to explain, namely reading newspapers. The time order is very plausible here because age stands for the moment a person was born, which must be prior in time to reading newspapers. If there is a causal relation between age and reading newspapers, age must be the cause and newspaper reading the consequence.\nA third criterion for causality is that the correlation is not spurious. In Section 8.3.2, we have encountered a spurious effect as an effect that incorrectly includes the effect of a confounder (reinforcer).\nIn the context of causality, spuriousness is linked to a confounder that is a common cause to both the assumed cause (predictor) and consequence (dependent variable). Age, for instance, can be a common cause both to having (grand)children and reading newspapers. Older people tend to have more (grand)children and they read more newspapers. If we do not control for age in a regression model predicting newspaper reading time from the number of (grand)children a person has, we may find a positive effect.\nThis effect is probably not causal: We do not spend more time reading newspapers because we have more (grand)children. Unless we use newspaper reading to ignore our children and grandchildren when they are around. Most if not all of the effect of (grand)children on newspaper reading is spurious because it results from a common cause, namely age or the habits and opportunities represented by age.\nTo interpret an effect that we find as causal, then, we must ensure that there are no confounding variables that are common causes to both our predictors and dependent variable. Including them as controls in the regression model is a way to solve the problem (Chapter 8). With non-experimental data, for example data on newspaper reading habits gathered in a survey, we cannot be sure that we have included all common causes in our model. In this situation, we should always keep in mind that the effects that we find may be caused by variables not included in our regression model. In an experiment, however, we can use experimental randomization to minimize the chance of having any confounders at all (Section 8.2.2).\n\n\n9.1.2 Mediation as indirect effect\nA common cause does not have to remove the entire effect between a predictor and dependent variable. Even if part of newspaper reading is caused by age, another part can be caused by a variable related to age, for example, interest in politics. During their lifetime, people may gain more experience with politics and, for that reason, become more interested in reading about politics. This may cause them to invest more time in reading newspapers for collecting information.\nNot all people become more interested in politics as they grow older and their interest in politics does not have to increase regularly during all of their lifetime. The relation between age and interest in politics, therefore, will not be perfect. This allows us to technically distinguish between the effect of age and the effect of interest in politics.\nIf we include both age and interest in politics as predictors in a regression model for newspaper reading time, the partial effect (Section 8.1.1) of interest in politics is corrected for the spurious correlation between interest in politics and newspaper reading caused by age as their common cause. The partial effect of political interest can be interpreted as causal if current interest in politics was attained before the newspaper readings that we measure (very plausible) and age is the only common cause of interest in politics and newspaper reading (highly questionable).\n\n\n\n\n\n\n\n\nFigure 9.3: Causal diagram for the effects of age and interest in politics on newspaper reading time.\n\n\n\n\n\nNow let us draw the causal diagram for this simple example (Figure 9.3). A causal diagram contains the names of the variables with arrows pointing from causes to consequences. The causal order of variables is represented from left to right. In Figure 9.3, the very first cause (age) is at the left, the final consequence (newspaper reading time) is at the right, and interest in politics is placed in the middle. In this layout, the arrows always point to the right.\nIn the causal order that we theorize, age is causally prior (antecedent) to interest in politics, which is causally prior to current newspaper reading time. We have an indirect effect of age on newspaper reading by way of interest in politics. When adults grow older, they tend to be more interested in politics and because of this, they tend to spend more time on reading newspapers. We say that interest in politics mediates the effect of age on newspaper reading time. Interest in politics is a mediator, an intermediary variable, or an intervening variable in this causal diagram.\nA causal diagram like Figure 9.3 is also called a path diagram. Each indirect effect is a sequence of direct effects. Each direct effect is a “step” from one variable to another variable, represented by an arrow. An indirect effect, then, can be regarded as a path that we can follow to “travel” from one variable to another variable.\n\n\n\n\n\n\n\n\nFigure 9.4: Causal diagram for the effect of age on newspaper reading time mediated by interest in politics and news site use.\n\n\n\n\n\nAn indirect effect may contain more than one step or mediator. If we include news site use in the model (Figure 9.4), we would have an indirect effect of age via interest in politics via news site use on newspaper reading time.\n\n\n\n\n\n\n\nMediation: A causal relationship between a predictor, one or more mediators, and an outcome variable.\nIndirect effect: An effect in which three (or more) variables affect each other in a causal order: the predictor affects the mediator, the mediator affects another mediator or the outcome variable.\n\n\n\n\n\n\n9.1.3 Causal process\nIn our example (Figure 9.4), age has a direct effect on newspaper reading time. What does the direct effect mean? If we start thinking about why older people spend more time on reading newspapers, we soon realize that this is probably not some biological process. It is hard to believe that an ageing human body requires more newspaper reading time. The effect is more likely to be social.\nIn the middle of the 20th century, newspapers were among the most important sources of information. A person who was born and grew up in that period is accustomed to using newspapers as main information source. For later generations, however, news sites on the internet have become important sources of information. Newspapers being less important to them, they are less oriented and accustomed to reading newspapers.\nThis line of reasoning shows us two things. First, we discover that our common cause may actually represent different things. Age, for instance, refers to life experience in its effect on interest in politics. In contrast, it relates to the period of coming of age in its direct effect on newspaper reading time.\nOur second discovery is that we usually look for mediators if we want to understand a direct effect. Date of birth affects exposure to people using newspapers as information sources, which affects the habit of reading newspapers, which finally affects the time spent on reading newspapers later on. Exposure and habit are mediators here. A direct effect of age on newspaper reading represents a causal process that may contain many intermediary steps. Adding mediators to our model is a way of getting more insight in the causal process.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#path-model-with-regression-analysis",
    "href": "11-mediation.html#path-model-with-regression-analysis",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.2 Path Model with Regression Analysis",
    "text": "9.2 Path Model with Regression Analysis\nMediation or, more generally, path models can be estimated with a series of regression models. Every variable in the path diagram with at least one predictor (or incoming arrow) is a dependent variable, so for each of them, we estimate a regression model. The regression model contains all variables as predictors that may cause changes in the dependent variable. In other words, all variables that are causally prior to the outcome are used as predictors. In a well-designed causal diagram, all variables to the left of the outcome are causally prior to it.\n\n\n\n\n\n\n\n\nFigure 9.5: Path diagram with unstandardized effect sizes and their 95% confidence intervals.\n\n\n\n\nIn the path diagram displayed in Figure 9.5, we would regress newspaper reading time, the final dependent variable, on age and political interest. As a next step, we would predict political interest as outcome from age.\n\n9.2.1 Requirements\nWe can estimate mediation and path models with regression analysis if we meet the following requirements:\n\nEach variable used as a dependent variable is numeric. This is a general requirement of a linear regression model. In a path diagram, it means that all mediators and dependent variables must be numeric.\nFor detail lovers: Variables with only incoming arrows may be dichotomous but that requires logistic regression, which we do not discuss.\nEach variable used only as a predictor must be a numeric or dichotomous (dummy) variable. Again, this is a general requirement of regression models.\nThere are no causal feedback loops. Causality must work in one direction. It must be impossible to travel from a variable back to it while following the direction of the arrows. Note that it can be difficult to assign a causal order. For example, does political interest cause (low) political cynicism or the other way around? Or are they not causally related at all?\nAll regression models meet the assumptions for regression analysis. Check if the residuals are normally distributed, centered around zero for all levels of the predicted outcome scores, and that scores are predicted equally well at all outcome levels (see Section 6.1.5).\n\n\n\n9.2.2 Size of indirect effects\nThe regression results tell us the sizes and statistical significance of all direct effects on the dependent variable. Both unstandardized and standardized regression coefficients can be used to interpret effects in the usual way. But how do we obtain the size, confidence interval, and statistical significance of indirect effects?\nThe size of an indirect effect is calculated in exactly the same way as the size of indirect correlations (Section 8.2): Just multiply the size of direct effects. This can be done with either the standardized regression coefficients or the unstandardized regression coefficients.\n\n\n\n\n\n\n\n\nFigure 9.6: Path diagram with unstandardized effect sizes and their 95% confidence intervals.\n\n\n\n\nIt may sound weird that we can multiply the unstandardized regression coefficients but it really works. In Figure 9.6, for instance, the unstandardized partial effect of age (measured in tens of years) on interest in politics is 0.1. This means that an additional 10 years of life predict an average increase in interest in politics of 0.1.\nIn its turn, interest in politics has an unstandardized effect of 0.32 on reading time (in minutes). An additional unit of interest in politics predicts an average increase in reading time of 0.32 minutes.\nTen additional years of life only predict an increase of 0.1 in political interest, not a full unit increase. The predicted increase of 0.1 in political interest predicts 0.1 * 0.32 = 0.03 minutes of additional newspaper reading time. As a result, an additional ten years of life predict 0.1 * 0.32 = 0.03 minutes of additional newspaper reading time as an indirect effect via political interest.\nNote that the indirect effect is interpreted in terms of the measurement units of the initial predictor (age in tens of years) and the final outcome (reading time in minutes): A difference in (tens of) years predicts a difference in reading time in minutes. As a consequence, we can directly compare unstandardized indirect effect sizes of different paths between the same predictor and outcome, as we will see in Section 9.2.5.\n\n\n9.2.3 Direction of indirect effects\nMultiplication of direct effects assigns the right direction (positive or negative) to indirect effects. In the example above, age has a positive effect on interest in politics, which has a positive effect on newspaper reading time. If age goes up, interest in politics goes up and if interest in politics goes up, reading time increases. Thus, higher age is indirectly associated with more reading time through interest in politics: Plus times plus yields a plus.\n\n\n9.2.4 Parallel and serial mediation\nIf each indirect effect in a path model contains at most one mediator, we have single mediation or parallel mediation. Figure 9.7 illustrates single and parallel mediation.\n\n\n\n\n\n\n\n\n\n\n\n(a) Single mediation.\n\n\n\n\n\n\n\n\n\n\n\n(b) Parallel mediation.\n\n\n\n\n\n\n\nFigure 9.7: Causal diagrams for:.\n\n\n\nIf at least one of the indirect effects in a path model contains two or more mediators, we are dealing with serial mediation. Figure 9.8 illustrates serial mediation. It contains an indirect effect from age on reading time with two mediators: Age &gt; Political Interest &gt; News Site Use &gt; Reading Time. The distinction between parallel and serial mediation is relevant to the software (PROCESS) that we will use to estimate indirect and total effects (Section 9.5).\n\n\n\n\n\n\n\n\nFigure 9.8: Causal diagram for serial mediation.\n\n\n\n\n\n\n\n9.2.5 Partial and full mediation\nThe unstandardized direct effect and indirect effects between a predictor and outcome can be compared directly because they are all expressed in the same measurement units, namely the predicted change in the dependent variable (reading time in minutes) for a difference of one unit in the predictor (ten additional years of life) (Section 9.2.2). Because of this, we can sum the unstandardized direct and indirect effects to obtain the total unstandardized effect.\nWith this in mind, we see that the relation between age and newspaper reading time is dominated by the positive direct effect (b = 5.99) and the positive indirect effect via news site use (b = 0.03). The remaining indirect effects are relatively small as indirect effects usually are.\nSumming all effects, we obtain a total effect of age on newspaper reading time around 6 (b = 6.03). A person who is ten years older but in other respects the same as another person, is predicted to spend on average 6 additional minutes on reading newspapers per day.\nIf the direct effect of a predictor on the outcome is zero in a model with mediators, the predictor’s effect is fully mediated. This clearly is not the case in our example: There still is a substantial direct effect of age on newspaper reading time. This is what we usually encounter; it is called partial mediation.\nSometimes, researchers decide that an effect is fully mediated if the direct effect is no longer statistically significant once a mediator is added to the model. This strategy is contestable because a statistically non-significant direct effect does not mean that the effect is absent (zero) in the population. It can be absent but it is much more likely to be present but just too small to be picked up by our significance test (see Section 4.2.7).\nThe distinction between full and partial mediation is a little bit problematic. From a substantive point of view, we may argue that direct effects are probably always mediated. As we have seen in Section 9.1.2, a direct effect usually summarizes a causal process that consists of intermediary steps, which is mediation. We may wonder whether it makes theoretical sense to talk about unmediated effects. Do we really believe that age can directly affect newspaper reading time?\nIf the variables that we entered in the model as mediators do not create any indirect effects, the direct effect is equal to the total effect. We may conclude that the direct effect is not mediated by the mediators that are included in the model. For example, if education, political interest, and news site use do not create indirect effects from age on newspaper reading time (Figure 9.6), we only have the direct effect of age on reading time in our model. However, this effect is very likely to be mediated by other variables that we did not include in the model. We should not conclude that the effect is unmediated because we have not found mediation yet.\n\n\n9.2.6 Significance of indirect effects\nSPSS does not calculate the size of indirect effects for us or their confidence intervals and p values. It is easy to calculate the sizes of indirect effects, as we have seen in a preceding section: just take the product of direct effects.\nIn contrast, it is not possible to calculate the confidence interval or p value of an indirect effect in a reliable way from the confidence intervals or p values of the direct effects (see Hayes 2013: Section 4.4 for a detailed and critical discussion of approaches that try to calculate the p value of an indirect effect from p values of direct effects).\nWe use bootstrapping to create the sampling distribution of the size of an indirect effect. We have learned the principles and limitations of bootstrapping in Section 2.5, so we do not have to go into details here. Suffice it to repeat that our original sample must not be too small and it must be quite representative of the population if we apply bootstrapping.\n\n\n\n\nTable 9.1\n\n\n\n\n\n\n\n\n\nEffect\n\n\nBoot SE\n\n\nBootLLCI\n\n\nBootULCI\n\n\n\n\n\n\nTotal indirect effect\n\n\n1.47\n\n\n.42\n\n\n.62\n\n\n2.25\n\n\n\n\nAge - Pol. Interest - Reading Time\n\n\n.05\n\n\n.03\n\n\n.01\n\n\n.14\n\n\n\n\nAge - Pol. Interest - News Site Use - Reading Time\n\n\n-.02\n\n\n.01\n\n\n-.05\n\n\n.00\n\n\n\n\nAge - News Site Use - Reading Time\n\n\n1.44\n\n\n.42\n\n\n.60\n\n\n2.23\n\n\n\n\nBootstrap results for unstandardized indirect effects in a model with two mediators. Effect size, standard error, lower and upper levels of the 95% confidence interval.\n\n\n\n\n\nThe confidence interval of an indirect effect can be calculated from its bootstrapped sampling distribution. Table 9.1 shows bootstrap results for the indirect effects in a model with age as predictor, newspaper reading time as dependent variable, and interest in politics and news site use as mediators.\nIn total, there is a substantial indirect effect of age on newspaper reading time in this model. We are confident that this effect is positive [b = 1.47, 95% CI [0.62, 2.25] (bootstrapped)]. It is easy to see that the indirect effect of age via news site use on reading time is by far the most important indirect effect [b = 1.44, 95% CI [0.60, 2.23] (bootstrapped)]. On its own, it is responsible for almost the entire total indirect effect.\nIt may happen that an indirect effect is not statistically significant (the confidence interval includes zero) whereas all direct effects that constitute the indirect effect are statistically significant. In Figure 9.9, for example, both the effect of age on political interest (b = 0.30, 95% CI [0.22, 0.38]) and the effect of political interest on reading time (b = 0.20, 95% CI [0.12, 0.28]) are statistically significant at the .05 level. The indirect effect of age via political interest on reading time, however, is not statistically significant at this level [b = 0.30 * 0.20 = 0.06, 95% CI [-0.02, 0.14], (bootstrapped)].\n\n\n\n\n\n\n\n\nFigure 9.9: Causal diagram for the effects of age and interest in politics on newspaper reading time: unstandardized estimates with 95% confidence intervals.\n\n\n\n\n\nThis sounds like a paradox but it should not upset you. The unstandardized indirect effect tends to be weaker than the direct effects, that is, closer to zero. With a weaker effect, it is more difficult to reject the null hypothesis that the effect is zero in the population. We need a larger sample to reject null hypotheses for smaller effects (see Section 4.2.3 on power). In this case, report that all direct effects that create the indirect effect are statistically significant, so non-significance is likely to arise from low test power rather than from absence of an indirect effect in the population.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#sec-mediationcovariate",
    "href": "11-mediation.html#sec-mediationcovariate",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.3 Controlling for Covariates",
    "text": "9.3 Controlling for Covariates\nWe usually have theoretical reasons to expect that a variable mediates an effect, for example, that political interest mediates the effect of age on newspaper reading time. At the same time, we know that our dependent variable and perhaps our mediator may depend on other variables. Newspaper reading time, for instance, may also depend on education. In this situation, we would use the other variables as covariates (or control variables) for which we want to control statistically.\n\n\n\n\n\n\n\n\nFigure 9.10: Causal diagram for interest in politics as mediator between age and newspaper reading time with education as covariate.\n\n\n\n\n\nFigure 9.10 presents a model in which education is used as a covariate in a model with political interest mediating the effect of age on newspaper reading time. Education is probably causally prior to both political interest and newspaper reading time, so it is allowed to have an effect on both variables. In this way, we control for education and remove spurious correlation between political interest and newspaper reading time due to education as a common cause.\nIf education predicts political interest and political interest predicts newspaper reading time as in Figure 9.10, political interest mediates the effect of education on newspaper reading time. We are, however, not interested in mediation in the case of a covariate, so we do not estimate or report the indirect effects of education on newspaper reading time. In the context of a mediation model, a covariate is a predictor for which we do not investigate if its effect is mediated.\nNote that covariates should only be allowed to have an effect on variables that can be caused by the covariate. We should not include effects of a covariate on a variable that is causally prior to it. If a covariate is a consequence rather than a cause of a mediator, it had better be used as another mediator in the model. If, for instance, political cynicism may affect newspaper reading time but it is a consequence of political interest, it should be included as a second (serial) mediator instead of as a covariate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#reporting-mediation-results",
    "href": "11-mediation.html#reporting-mediation-results",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.4 Reporting Mediation Results",
    "text": "9.4 Reporting Mediation Results\nWe analyse a path model as a series of regression models, so the general rules for reporting mediation are the same as for reporting regression analyses (see Section 7.3). If you summarize results in a table, make sure that the table includes:\n\nThe unstandardized regression coefficients for all direct and indirect effects tested in the regression models.\nThe confidence intervals and significance levels of the unstandardized effects.\nThe F test and measure of model fit (\\(R^2\\)) for each regression model.\n\n\n\n\n\nTable 9.2\n\n\n\n\n\n\n\n\n\nB\n\n\n\n\n95% CI\n\n\n\n\n\n\nOutcome: News Site Use\n\n\n\n\n\n\n\n\n\n\nconstant\n\n\n6.62\n\n\n***\n\n\n[5.92; 7.31]\n\n\n\n\nage\n\n\n-0.93\n\n\n***\n\n\n[-0.97; -0.88]\n\n\n\n\neducation\n\n\n0.06\n\n\n*\n\n\n[0.01; 0.11]\n\n\n\n\npol.interest\n\n\n0.12\n\n\n***\n\n\n[0.06; 0.17]\n\n\n\n\nR2\n\n\n0.86\n\n\n\n\n\n\n\n\nF (3, 308)\n\n\n617.40\n\n\n***\n\n\n\n\n\n\nOutcome: Newspaper Reading Time\n\n\n\n\n\n\n\n\n\n\nconstant\n\n\n13.59\n\n\n**\n\n\n[5.26; 21.93]\n\n\n\n\nage\n\n\n4.54\n\n\n***\n\n\n[3.62; 5.47]\n\n\n\n\neducation\n\n\n0.06\n\n\n\n\n[-0.34; 0.46]\n\n\n\n\npol.interest\n\n\n0.52\n\n\n*\n\n\n[0.07; 0.96]\n\n\n\n\nnewssite\n\n\n-1.55\n\n\n**\n\n\n[-2.47; -0.64]\n\n\n\n\nR2\n\n\n0.79\n\n\n\n\n\n\n\n\nF (4, 307)\n\n\n290.85\n\n\n***\n\n\n\n\n\n\nIndirect Effect\n\n\n\n\n\n\n\n\n\n\nAge &gt; News Site Use &gt; Reading Time\n\n\n1.44\n\n\n\n\n[0.61; 2.17]\n\n\n\n\n\n\nNote.  * p &lt; .05. ** p &lt; .01. *** p &lt; .001.\n\n\n\n\nUnstandardized effects in a model regressing newspaper reading time on age with one mediator (News Site Use) and two covariates (Education, Political Interest). Theoretical approximation for direct effects, bootstrap results for indirect effects, using 5,000 bootstraps.\n\n\n\n\n\nA path model may yield a lot of direct effects, so it is good practice to present results as a path diagram with the values of the standardized or unstandardized regression coefficients as labels to the arrows. A path model conveniently summarizes the results for the reader (Figure 9.11). Remember that we don’t use standardized regression coefficients if the predictor or a covariate is dichotomous variable or a set of dummy variables (see Section 6.1.3).\n\n\n\n\n\n\n\n\nFigure 9.11: Unstandardized direct effects for a path model with one mediator and two covariates.\n\n\n\n\nIf effect mediation is central to your report, focus your presentation and interpretation on the indirect effects and compare them to the direct effects. Report the size and confidence interval of each indirect effect. If possible, add both the direct and indirect effect to a diagram such as Figure 9.11.\nInterpret an unstandardized indirect effect just like any unstandardized regression effect, namely, as the predicted difference in the outcome for a one unit difference in the predictor. It is usually interesting to compare the sizes of the direct and indirect effects. Is the effect predominantly mediated in the model or is only a minor part of the effect mediated in the model?\nInform the reader that you bootstrapped the indirect effect and report the number of bootstrap samples and the method used for the confidence intervals (see Section 9.5). For a more elaborate discussion of reporting mediation, see Hayes (2013: 198-202).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#sec-SPSSPROCESS",
    "href": "11-mediation.html#sec-SPSSPROCESS",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.5 Mediation with SPSS and PROCESS",
    "text": "9.5 Mediation with SPSS and PROCESS\n\n9.5.1 Instructions\nTo perform mediation analysis in SPSS the PROCESS macro by Hayes (2013), makes it a lot easier.\n\n\n\n\n\n\nInstructions on how to add PROCESS to your SPSS installation can be found in Appendix 2.\n\n\n\n\n\n\n\n\n\n\nVideo 9.2: Estimating a single or parallel mediation model with PROCESS.\n\n\n\n\n\n\n\n\n\n\nVideo 9.3: Estimating a serial mediation model with PROCESS (Model 6).\n\n\n\n\n\n\n\n\n\n\nVideo 9.4: Estimating a (single mediator) mediation model including covariates with PROCESS.\n\n\n\n\n\n\n\n\n\n\nVideo 9.5: Getting standardized direct effects and checking assumptions in SPSS,",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#criticisms-of-mediation",
    "href": "11-mediation.html#criticisms-of-mediation",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.6 Criticisms of Mediation",
    "text": "9.6 Criticisms of Mediation\nIf we think of causality, we usually think of a process in which one thing leads to another thing, which leads to something else, and so on. This is apparent if we want to explain why we think that one phenomenon causes another (see Section 9.1.2). Mediation, however, is difficult to establish with regression analysis and, as some argue, perhaps impossible to establish.\n\n9.6.1 Causal order assumed\nIt is paramount to note that the regression approach to mediation and path models does not tell us anything about the causal order of the variables. The causal order is purely an assumption that we make. The plausibility of the assumptions depends on how well we can justify the time order of the variables and the absence of common causes for cause-consequence pairs (see Section 9.1.1).\n\n\n9.6.2 Time order\nTo establish the time order of variables, we must think about the time at which the behaviours or opinions that we measure took place. This is what matters, not the time at which we measure them. We can collect information on behaviour a long time after the fact, for example by asking respondents when they started using news sites or checking internet use logs.\nIf cause and consequence appear very closely in time, it may be difficult to argue that one variable precedes the other. This may also apply to the time at which measurement takes place. If we measure cause and consequence nearly at the same time, it can be difficult to establish the time order of the two.\n\n\n9.6.3 Causality or underlying construct?\nFor causes and consequences that appear nearly simultaneously, we should take into account that the two variables may measure the same underlying construct. Think of the way we construct a scale from items: We assume that the items measure the same underlying attitude, for instance, political cynicism.\nThe indicators of a scale are correlated because they have a common cause, namely, the underlying attitude. But it does not make sense to interpret the correlation as a sign of mediation. One item does not trigger another item, and so on. A mediator must be theoretically and conceptually different from both the predictor and outcome. We have to provide arguments that they are really different.\n\n\n9.6.4 Every effect in a path model can be confounded\nIn Chapter 8, you learned that the estimated regression coefficients can be too small, too large, or have the wrong sign (direction) if there are confounders: variables not included in the regression model that are correlated with the predictor and outcome variable. If we analyze a path model with a series of regression models, there can be confounders for each regression model. Every estimated direct effect can be wrong. As a consequence, every indirect effect, which is the product of direct effects, can be wrong.\nThe surest way to get rid of a confounder is adding it to the regression model. In a path model, we can add a variable that we expect to be a confounder as a covariate (Section 9.3) or as an additional mediator. If a confounder comes after the outcome variable in the causal order of the path model, it cannot be a common cause to both a predictor and the outcome variable. In this situation, the confounder can be ignored. This underlines the importance of choosing a correct causal order when we construct a path model. Unfortunately, we can never be sure about this.\nIn practice, we do not know all confounders and we cannot include all of them in our regression models. We can minimize the risk of having confounders if we use randomization in an experiment. Section 8.2.2 explained how randomization of the experimental treatment variable helps to eliminate confounders for the effect of the experimental treatment (predictor variable) on the dependent variable. We expect that this effect is not confounded.\nIn a path model, a mediator also serves as a predictor, so we also have to randomize the mediator variable to get correct estimates for the causal effect of the mediator on the outcome variable. With randomized predictor and mediator variables, the direct effects are probably not confounded, so the indirect effects calculated from the direct effects are also unlikely to be confounded.\nIt is difficult to manipulate a mediator in an experiment (Bullock and Ha 2011). If we hypothesize, for example, that political interest mediates the effect of age on newspaper reading time, how can we assign a random level of political interest to a participant in an experiment? By the way, it will also be impossible to randomize participant age in this example.\n\n\n9.6.5 Recommendations\nAll in all, mediation is an intuitively simple and appealing concept. Unfortunately, it is very difficult to substantiate the claim that indirect effects in path models represent mediation. Mediation assumes causal effects and causality is difficult to establish.\nIf you plan to investigate mediation:\n\nJustify that the mediator is theoretically and conceptually different from the predictor and outcome.\nMotivate the time order of variables in the model.\nInclude variables that are likely to confound the effects of the predictor or mediator(s) in your research project and in the regression models that you are going to estimate.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#combining-mediation-and-moderation",
    "href": "11-mediation.html#combining-mediation-and-moderation",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.7 Combining Mediation and Moderation",
    "text": "9.7 Combining Mediation and Moderation\nMediation and moderation (Chapters Chapter 6 and Chapter 7) can occur in the same model. For example, the effect of age on newspaper reading time mediated by interest in politics can be different for females and males. In other words, the indirect effect is different for females and males.\nIf the indirect effect is different for females and males, at least one of the two direct effects (predictor on mediator or mediator on dependent variable) must be different for females and males. In Figure 9.12, the direct effect of age on interest in politics is moderated and as a consequence, indirect effects including this effect are moderated. This is called moderated mediation. In this example, sex is the moderator and interest in politics is the mediator.\n\n\n\n\n\n\n\n\nFigure 9.12: Causal diagram for interest in politics as mediator between age and newspaper reading time with sex as moderator of the effect of age on interest in politics.\n\n\n\n\n\nSeveral models with more than one mediator or with moderated mediation can be estimated with PROCESS. For an overview of the models, see Appendix A in Hayes (2013). The models, however, are quite complex, so we leave them for enthusiasts.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "11-mediation.html#take-home-points",
    "href": "11-mediation.html#take-home-points",
    "title": "9  Mediation with Regression Analysis",
    "section": "9.8 Take-Home Points",
    "text": "9.8 Take-Home Points\n\nA causal or path model without causal feedback loops can be estimated as a series of regression models: one regression model for each variable that has at least one predictor in the path model.\nUnstandardized regression coefficients, standardized regression coefficients, and correlations can be multiplied to obtain indirect effects and indirect correlations.\nAn indirect effect is a mediated effect. Variables that are at the same time predicted and predictors in an indirect effect are called mediators, intermediary variables, or intervening variables.\nStatistical inference on an indirect effect—its confidence interval and significance level—requires a sampling distribution of the size of the indirect effect. This distribution can be bootstrapped with the PROCESS macro (Hayes 2013).\nMediation is an intuitively appealing concept but it is difficult to establish. A causal interpretation of a regression coefficient requires a clear time order between predictor, mediator, and dependent variable, a clear theoretical and conceptual difference between these three variables, and the inclusion of all variables that may confound the effects of the predictor and mediator(s) in the regression models.\n\nRead the little but very helpful book on the logic of causal order by James A. Davis (1985) for more information on causality and correlational analysis.\n\n\n\n\nBullock, John G., and Shang E. Ha. 2011. “Mediation Analysis Is Harder Than It Looks.” In Cambridge Handbook of Experimental Political Science, edited by James N. Druckman, Donald P. Green, James H. Kuklinski, and ArthurEditors Lupia, 508–22. Cambridge University Press. https://doi.org/10.1017/CBO9780511921452.035.\n\n\nDavis, James A. 1985. The Logic of Causal Order. Beverly Hills, CA: Sage.\n\n\nHayes, Andrew F. 2013. Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach. Guilford Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Mediation with Regression Analysis</span>"
    ]
  },
  {
    "objectID": "12-Appendix.html",
    "href": "12-Appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Flow chart statistical test selection\nThe flow chart in Figure 1 helps you select the appropriate statistical test based on the type of dependent and independent variables.\nIt helps in the learning proces to think of different research designs for each test. Try to think of one for the independent samples t-test. Work your way from right to left to determine the type of variables and think of some research that conforms to this.\nFigure 1: Flow chart for statistical test selection.\nAn interactive tool for test selection made by Matt Jackson-Wood can be found here.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#all-spss-tutorial-videos-list",
    "href": "12-Appendix.html#all-spss-tutorial-videos-list",
    "title": "Appendix",
    "section": "All SPSS Tutorial Videos List",
    "text": "All SPSS Tutorial Videos List\nChapter 2  Probability Models\n\nVideo 2.4: Bootstrapping in SPSS.\nVideo 2.5: Interpreting bootstrap results in SPSS.\nVideo 2.2: Performing an exact test in SPSS.\nVideo 2.3: Interpreting exact test results in SPSS.\n\nChapter 3  Estimating a Parameter\n\nVideo 3.1: Setting the confidence level in SPSS.\n\nChapter 4  Hypothesis testing\n\nVideo 1: A binomial test on a single proportion in SPSS.\nVideo 3: A chi-squared test on a frequency distribution in SPSS.\nVideo 2: A one-sample t test in SPSS.\n\nChapter 5  Moderation with Analysis of Variance (ANOVA)\n\nVideo 5.2: One-way analysis of variance (ANOVA) in SPSS.\nVideo 5.5: Two-way analysis of variance with moderation in SPSS.\nVideo 5.8: Calculating eta2 from SPSS output.\n\nChapter 6  Regression Analysis And A Categorical Moderator\n\nVideo 6.3: Executing and interpreting regression analysis in SPSS.\nVideo 6.4: Creating dummy variables in SPSS.\nVideo 6.5: Using dummy variables in a regression model in SPSS.\nVideo 6.2: Checking assumptions for regression models in SPSS.\nVideo 6.8: Creating categorical by numerical interaction predictors for regression in SPSS.\n?vid-SPSSregcatmod: Estimating categorical by numerical moderation with regression in SPSS.\nVideo 6.12: Representing moderation by regression lines in a scatterplot in SPSS.\nVideo 6.11: Checking common support for a predictor at different moderator values in SPSS.\n\nChapter 7  Regression Analysis With A Numerical Moderator\n\nVideo 7.2: Mean-centering variables for regression analysis in SPSS.\n?vid-SPSSreglines2: Regression lines for a numerical moderator in a scatterplot in SPSS.\nVideo 7.3: Checking common support with a numerical moderator in SPSS.\n\nChapter 8  Regression Analysis And Confounders\n\nVideo 8.2: Identifying confounders with regression in SPSS.\n\nChapter 9  Mediation with Regression Analysis\n\nVideo 9.2: Estimating a single or parallel mediation model with PROCESS (Model 4).\nVideo 9.3: Estimating a serial mediation model with PROCESS (Model 6).\nVideo 9.4: Estimating a mediation model including covariates with PROCESS.\nVideo 9.5: Estimating a path model in SPSS.\n\nAppendix: Variance / Association\n\nVideo 4: Levene’s F test on equal variances in SPSS.\nVideo 5: Independent samples t test on two means.\nVideo 6: Paired samples t test on two means.\nVideo 7: Test on a correlation.\nVideo 8: Chi-squared test on a contingency table (crosstab).\nVideo 9: Obtaining Cohen’s d with SPSS.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#formulating-statistical-hypotheses",
    "href": "12-Appendix.html#formulating-statistical-hypotheses",
    "title": "Appendix",
    "section": "Formulating Statistical Hypotheses",
    "text": "Formulating Statistical Hypotheses\nA research hypothesis is a statement about the empirical world that can be tested against data. Communication scientists, for instance, may hypothesize that:\n\na television station reaches half of all households in a country,\nmedia literacy is below a particular standard (for instance, 5.5 on a 10-point scale) among children,\n\nopinions about immigrants are not equally polarized among young and old voters,\n\nthe celebrity endorsing a fundraising campaign makes a difference to people’s willingness to donate,\n\nmore exposure to brand advertisements increases brand awareness,\nand so on.\n\nAs these examples illustrate, research hypotheses seldom refer to statistics such as means, proportions, variances, or correlations. Still, we need such statistics to test a hypothesis. The researcher must translate the hypothesis into a new hypothesis specifying a statistic in the population, for example, the population mean. The new hypothesis is called a statistical hypothesis.\nTranslating the research hypothesis into a statistical hypothesis is perhaps the most creative part of statistical analysis, which is just a fancy way of saying that it is difficult to give general guidelines stating which statistic fits which research hypothesis. All we can do is give some hints.\nResearch questions usually address shares, score levels, associations, or score variation. If a research question talks about how frequent some characteristic occurs (How many candies are yellow?) or which part has a particular characteristic (Which percentage of all candies are yellow?), we are dealing with one or two categorical variables. Here, we need a binomial, chi-squared, or exact test (see Figure 1).\nIf a research question asks how high a group scores or whether one group scores higher than another group, we are dealing with score levels. The variable of central interest usually is numerical (interval or ratio measurement level) and we are concerned with mean or median scores. There is a range of tests that we can apply, depending on the number of groups that we want to compare (one, two, three or more): t tests or analysis of variance.\nInstead of comparing mean scores of groups, a research question about score levels can address associations between numerical variables, for example, Are heavier candies more sticky? Here, the score level on one variable (candy weight) is linked to the score level on another variable (candy stickiness). This is where we use correlations or regression analysis.\nFinally, a research question may address the variation of numeric scores, for example, Does the weight of yellow candies vary more strongly than the weight of red candies? Variance is the statistic that we use to measure variation in numeric scores.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#proportions-shares",
    "href": "12-Appendix.html#proportions-shares",
    "title": "Appendix",
    "section": "Proportions: shares",
    "text": "Proportions: shares\nA proportion is the statistic best suited to test research hypotheses addressing the share of a category or entity in the population. The hypothesis that a television station reaches half of all households in a country provides an example. All households in the country constitute the population. The share of the television station is the proportion or percentage of all households watching this television station.\nIf we want to use a statistic, we need to know the variable and cases (units of analysis) for which the statistic must be calculated. In this example, a household does or does not watch the television station, so our variable is a dichotomy with the two categories (“No, does not watch this station”, “Yes, watches this station”) usually coded as 0 versus 1 or 1 versus 2.\nEach household provides an observation, namely either the score 0 or the score 1 on this variable or no score if there are missing values. To test the research hypothesis that a television station reaches half of all households in a country, we have to formulate a statistical hypothesis about the proportion—of households viewing this television station—in the population—all households in this country. For example, the researcher’s statistical hypothesis could be that the proportion in the population is 0.5.\nWe can also be interested in more than two categories, for instance, does the television station reach the same share of all households in the north, east, south, and west of the country? This translates into a statistical hypothesis containing three or more proportions in the population. If 30% of households in the population are situated in the west, 25 % in the south and east, and 20% in the north, we would expect these proportions in the sample if all regions are equally represented. Our statistical hypothesis is actually a relative frequency distribution, such as, for instance, in Table 1.\n\n\n\n\nTable 1\n\n\n\n\n\n\n\nRegion\n\n\nHypothesized Proportion\n\n\n\n\n\n\nNorth\n\n\n0.20\n\n\n\n\nEast\n\n\n0.25\n\n\n\n\nSouth\n\n\n0.25\n\n\n\n\nWest\n\n\n0.30\n\n\n\n\nStatistical hypothesis about four proportions as a frequency table.\n\n\n\n\n\nA test for this type of statistical hypothesis is called a one-sample chi-squared test. It is up to the researcher to specify the hypothesized proportions for all categories. This is not a simple task: What reasons do we have to expect particular values, say a region’s share of thirty per cent of all households instead of twenty-five per cent?\nThe test is mainly used if researchers know the true proportions of the categories in the population from which they aimed to draw their sample. If we try to draw a sample from all citizens of a country, we usually know the frequency distribution of sex, age, educational level, and so on for all citizens from the national bureau of statistics. With the bureau’s information, we can test if the respondents in our sample have the same distribution with respect to sex, age, or educational level as the population from which we tried to draw the sample; just use the official population proportions in the null hypothesis.\nIf the proportions in the sample do not differ more from the known proportions in the population than we expect based on chance, the sample is representative of the population in the statistical sense (see Representative sample). As always, we use the p value of the test as the probability of obtaining our sample or a sample that is even more different from the null hypothesis, if the null hypothesis is true. Note that the null hypothesis now represents the (distribution in) the population from which we tried to draw our sample. We conclude that the sample is representative of this population in the statistical sense if we can not reject the null hypothesis, that is, if the p value is larger than .05. Not rejecting the null hypothesis means that we have sufficient probability that our sample was drawn from the population that we wanted to investigate. We can now be more confident that our sample results generalize to the population that we meant to investigate.\n\nTesting proportions in SPSS\n\n\n\n\n\n\nVideo 1: A binomial test on a single proportion.\n\n\n\n\n\n\n\n\n\nVideo 2: Execute a one sample t test in SPSS,\n\n\n\nFor a binomial test, see the video in Video 1. A one-sample chi-squared test is explained in the video of Video 3.\n\n\n\n\n\n\nVideo 3: One-sample chi-squared test.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#mean-and-median-level",
    "href": "12-Appendix.html#mean-and-median-level",
    "title": "Appendix",
    "section": "Mean and median: level",
    "text": "Mean and median: level\nResearch hypotheses that focus on the level of scores are usually best tested with the mean or another measure of central tendency such as the median value. For example, the hypothesis that media literacy is below a particular standard (e.g., 5.5 on a 10-point scale) among children refers to a level: the level of media literacy scores.\nThe hypothesis probably does not argue that all children have a media literacy score below 5.5. Instead, it means to say that the overall level is below this standard. The center of the distribution offers a good indication of the general score level.\nFor a numeric (interval or ratio measurement level) variable such as the 10-point scale in the example, the mean is a good measure of the distribution’s center. In this example, our statistical hypothesis would be that average media literacy score of all children in the population is (below) 5.5.\n\nTesting one mean or median in SPSS\nThe one-sample t test in SPSS is explained in Video 2.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#variance-disagreement",
    "href": "12-Appendix.html#variance-disagreement",
    "title": "Appendix",
    "section": "Variance: (dis)agreement",
    "text": "Variance: (dis)agreement\nAlthough rare, research hypotheses may focus on the variation in scores rather than on score level. The hypothesis about polarization provides an example. Polarization means that we have scores well above the center and well below the center rather than all scores concentrated in the middle. If voters’ opinions about immigrants are strongly polarized, we have a lot of voters strongly in favour of admitting immigrants as well as many voters strongly opposed to admitting immigrants.\nFor a numeric variable, the variance or standard deviation—the latter is just the square root of the former—is the appropriate statistic to test a hypothesis about polarization. The research hypothesis concerns the variation of scores in two groups, for instance, young versus old voters. The statistical hypothesis would be that the variance in opinions in the population of young voters is different from the variance in the population of old voters.\n\nTesting two variances in SPSS\n\nInstructions\n\n\n\n\n\n\nVideo 4: Levene’s F test in SPSS\n\n\n\n\n\nExercises",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#association-relations-between-characteristics",
    "href": "12-Appendix.html#association-relations-between-characteristics",
    "title": "Appendix",
    "section": "Association: relations between characteristics",
    "text": "Association: relations between characteristics\nFinally, research hypotheses may address the relation between two or more variables. Relations between variables are at stake if the research hypothesis states or implies that one (type of) characteristic is related to another (type of) characteristic. The statistical name for a relation between variables is association.\nTake, for example, an analysis of the effect of a celebrity endorser on the willingness to donate. Here, the endorser to whom a person is exposed (one characteristic) is related to this person’s willingness to donate (another characteristic). Another example: If exposure to the campaign increases willingness to donate, a person’s willingness to donate is positively related to this person’s exposure to the campaign.\n\nScore level differences\nAssociation comes in two related flavors: a difference in score level between groups or the predominance of particular combinations of scores on different variables.\nThe relation between the endorser’s identity and willingness to donate is an example of the first flavor. All people are confronted with one of the celebrities as endorser of the fund-raising campaign. This is captured by a categorical variable: the endorsing celebrity.\nThe categorical variable clusters people into groups: One group is confronted with Celebrity A, another group with Celebrity B, and so on. If the celebrity matters to the willingness to donate, the general level of donation willingness should be higher in the group exposed to one celebrity than in the group exposed to another celebrity.\nThus, we return to statistics needed to test research hypotheses about score levels, namely measures of central tendency. If willingness to donate is a numeric variable, we can use group means to test the association between endorsing celebrity (grouping variable) and willingness to donate (score variable). The statistical hypothesis would then be that group means are not equal in the population of all people.\nIf you closely inspect the choice diagram in Figure 4.17, you will see that we prefer to use a t distribution if we compare two different groups (independent-samples t test) or two repeated observations for the same group (paired-samples t test). By contrast, if we have three or more groups, we use analysis of variance with an F distribution.\n\n\nComparing means in SPSS\n\nInstructions\n\n\n\n\n\n\nVideo 5: Association as level differences between groups.\n\n\n\n\n\n\n\n\n\n\nVideo 6: Association as score level change.\n\n\n\n\nFor an instruction and exercises on one-way analysis of variance, see Video 5.2 and One-Way Analysis of Variance in SPSS. For two-way analysis of variance, see Two-Way Analysis of Variance in SPSS (instructions and exercises).\n\n\nExercises\n\n\n\nCombinations of scores\nThe other flavor of association represents situations in which some combinations of scores on different variables are much more common than other combinations of scores.\nThink of the hypothesis that brand awareness is related to exposure to advertisements for that brand. If the hypothesis is true, people with high exposure and high brand awareness should occur much more often than people with high exposure and low brand awareness or low exposure and high brand awareness.\nThe two variables here are exposure and brand awareness. One combination of scores on the two variables is high exposure combined with high brand awareness. This combination should be more common than high exposure combined with low brand awareness.\nMeasures of association are statistics that put a number to the pattern in combinations of scores. The exact statistic that we use depends on the measurement level of the variables. For numerical variables, measured at the interval or ratio level, we use Pearson’s correlation coefficient or the regression coefficient. For ordinal variables with quite a lot of different scores, we use Spearman’s rank correlation.\nFor categorical variables, measured at the nominal or ordinal level, chi-squared indicates whether variables are statistically associated. The larger chi-squared, the more likely we are to conclude that the variables are associated in the population. If variables are not associated, they are said to be statistically independent.\nSeveral measures exist that express the strength of the association between two categorical variables. We use Phi and Cramer’s V (two nominal variables, symmetric association), Goodman & Kruskals tau (two nominal variables, asymmetric association), Kendalls tau-b (two categorical ordinal variables, symmetric association), and Somers’ d (two categorical ordinal variables, asymmetric association).\n\n\nTesting associations in SPSS\n\nInstructions\n\n\n\n\n\n\nVideo 7: Association as combinations of scores.\n\n\n\n\n\n\n\n\n\n\nVideo 8: Association as relatively frequent/infrequent category combinations.\n\n\n\n\nFor regression analysis (instructions and exercises), see Regression Analysis in SPSS.\n\n\nExercises",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#sec-CohenCalculations",
    "href": "12-Appendix.html#sec-CohenCalculations",
    "title": "Appendix",
    "section": "Cohen’s d calculatons",
    "text": "Cohen’s d calculatons\n\nThese are the formulas for Cohen’s d for a one-sample t test, a paired-samples t test, and an independent-samples t test (they will be provided if needed):\n\n\\[\\begin{equation}\n  d_{one_-sample} = \\frac{M - \\mu_0}{SD}\n\\end{equation}\\]\n\\[\\begin{equation}\n  d_{paired_-samples} = \\frac{M_{diff} - \\mu_{0_-diff}}{SD_{diff}}\n\\end{equation}\\]\n\\[\\begin{equation}\n  d_{independent_-samples} = \\frac{2*t}{\\sqrt(df)}\n\\end{equation}\\]\n\nWhere:\n\n\\(M\\) is the sample mean, \\(\\mu_0\\) is the hypothesized population mean, and \\(SD\\) is the standard deviation in the sample,\n\\(M_{diff}\\) is the difference between the two means in the sample, \\(\\mu_{0_-diff}\\) is the hypothesized difference between the two means in the population mean, which is zero in case of a nil hypothesis, and \\(SD_{diff}\\) is the standard deviation of the difference in the sample,\n\\(t\\) is the test statistic value and \\(df\\) is the number of degrees of freedom of the t test.\n\n\nThe sample outcome can be a single mean, for instance the average weight of candies, but it can also be the difference between two means, for example, the difference in colourfulness of yellow candies at the beginning and end of a time period. In the latter case, the standard deviation that we need is the standard deviation of colourfulness difference across all candies (Dependent samples). In the case of independent samples, such as average weight of red versus yellow candies, we need a special combined (pooled) standard deviation for yellow and red candy weight that is not reported by SPSS. Here, we use the t value and degrees of freedom to calculate Cohen’s d.\n\nObtaining Cohen’s d with SPSS\n\n\n\n\n\n\nVideo 9: Obtaining Cohen’s d with SPSS\n\n\n\nIt is, relatively easy to calculate Cohen’s d by hand from SPSS output. Remember that we must divide the unstandardized effect by the standard deviation, though the latest versions of SPSS can also produce this in the output.\nFor a t test on one mean, the unstandardized effect is the difference between the sample mean and the hypothesized mean. SPSS reports this value in the column Mean Difference of the table with test results. Drop any negative signs! Divide it by the standard deviation of the variable as given in Table One-Sample Statistics.\nIn the example, Cohen’s d is 0.036 / 0.169 = 0.21. This is a weak effect.\nFor a paired-samples t test, the unstandardized effect size is reported in the column Mean in the Table Paired Samples Test. The standard deviation of the difference can be found in column Std. Deviation in the same table. Divide the first by the second, for instance, 1.880 / 1.033 = 1.82. This is a strong effect.\nFor an independent-samples t test, the situation is less fortuitous because SPSS does not report the pooled sample standard deviation that we need. The pooled sample standard deviation takes a sort of average of the outcome variable’s standard deviations in the two groups. As an approximation, we can calculate Cohen’s d as follows: Double the t value and divide it by the square root of the degrees of freedom.\nIn the example, Cohen’s d equals \\((2 * 0.651) / \\surd(18) = 0.31\\). This is a moderate effect size.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "12-Appendix.html#sec-installPROCESS",
    "href": "12-Appendix.html#sec-installPROCESS",
    "title": "Appendix",
    "section": "Installing PROCESS",
    "text": "Installing PROCESS\nSPSS cannot apply statistical inference to indirect effects, so we use the PROCESS macro developed for this purpose (Hayes 2013). If correctly installed (see below), the macro can be used from within the SPSS Regression menu. Please note that you had better not paste the PROCESS commands to the SPSS syntax because it produces a lot of code that is difficult to understand. Instead, run the PROCESS command directly from the menu and manually add a comment to your SPSS syntax file reminding yourself of the model that you estimated with PROCESS.\nDownload the PROCESS macro and install the SPSS custom dialog file. Check the FAQ at the PROCESS website if installation is not successful. If PROCESS is successfully installed, it can be found in the Analyze &gt; Regression menu.\nVideo 10 shows how to install the PROCESS macro. The installation is straightforward, but you need to know where to find the right files. The video shows two ways of installing the macro. We recommend the latter method, which is more reliable on different operating systems. This requires you to add the costum dialog through the SPSS menu Extensions &gt; Utilities &gt; Install Custom Dialog.\n\n\n\n\n\n\nVideo 10: How to install the PROCESS macro to SPSS.\n\n\n\nIf installation does not work, it is always possible to use the University of Amsterdam’s virtual destop environment for students, to use SPSS. The PROCESS macro is already installed there. You can find the virtual desktop environment at apps.uva.nl. Instructions on how to use the virtual desktop environment can be found here.\n\n\n\n\nHayes, Andrew F. 2013. Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach. Guilford Press.",
    "crumbs": [
      "Appendix"
    ]
  },
  {
    "objectID": "13-colophon.html",
    "href": "13-colophon.html",
    "title": "Colophon",
    "section": "",
    "text": "Disclaimer\nThe example data sets have been generated for the purpose of demonstrating statistical techniques. These are not real data and no conclusions should be drawn from the results obtained from the data.\n\n\n2016 Acknowledgements\nBy the original author: Wouter de Nooy\nAdam Sasiadek developed the more complicated Shiny apps in this book. The College of Communication at the University of Amsterdam generously supported the creation of the apps whereas this university’s Grassroots Project for ICT in education refused to support it. Renske van Bronswijk corrected my English. Any remaining errors result from changes and additions that I applied afterwards. My colleague Peter Neijens commented on a draft of this text. Among the first tutors using this book, Chei Billedo, Marcel van Egmond, Matthijs Elenbaas, Andreas Goldberg, Bregje van Groningen, Luzia Helfer, Rhianne Hoek, Laura Jacobs, Jeroen Jonkman, Fam te Poel, Sanne Schinkel, Christin Scholz, Ragnheiður Torfadóttir, Philipp Mendoza, and she whose name I am not allowed to mention offered many suggestions that improved the text substantially. Among the students who helped to improve the book, Alissa Hilbertz stands out for her careful language corrections and suggestions.\n\n\n2023 Technical changes\nThis book is a modification of the original work by Wouter de Nooy. His publication, licensed under CC-SA 4.0 and hosted from his github repository, forms the basis for this publication. Some technical changes have been made to the book. Hosting is now managed through GitHub pages and the source material is maintained from this repository. The Shiny apps are now available through Shinyapps.io and maintained at a separate GitHub repository.\nThis version of the book is hosted by Sharon Klinkenberg. Modifications are collaboratively done by assigned teacher from the statistical modelling course of the College of Communication Science at the University of Amsterdam.\n\n\n2024 Rewrite\nIn 2024 the book has been partially rewriten. Through the quality agreements funds, the College of Communication at the University of Amsterdam has been able to support the further development of the this opensource book.\nThe following changes have been made:\n\nRewrite of Reverse reasoning from one sample mean by Xin Gao to more thoroughly explain the concept and reasoning behind confidence intervals. Including the creation of a new Shiny app to demonstrate the concept.\nMerging and rewrite of the previous chapters on Hypothesis testing, Sample size, and Critical discussion by Sharon Klinkenberg. This to place the discussion on power, and how this is related to sample size firmly in the context of null hypothesis significance testing.\n\nMany thanks to Christin Scholz for her elaborate feedback on the rewrite of 4  Hypothesis testing.\nThe 2023 version of the book is archived in the pre 2024 branch of the GitHub repository\n\n\n2025 Bookdown to quarto migration\nIn 2025 we migrated the book from Bookdown to Quarto. Because we were running into technical issues with incompatible packages, we decided to future proof the book by making the transition to quarto. With a lot of regular expressions, we managed to convert many of the bookdown syntax to quarto.\nThanks to the efforts of Saurabh Khanna, Sten Paffen, and Sharon Klinkenberg the book is now fully migrated to quarto.\n\n\n2025 Rewrite\nIn 2025 chapter 5  Moderation with Analysis of Variance (ANOVA) has been rewritten by Jolijn van den Hout. The chapter has been made more inclusive by replacing the George Clooney and Angelina Jolie example by an example related to Vaccine acceptance. The shiny apps were updated by Christin Scholz. These changes where funded by the College of Communication’s DEI initiative at the University of Amsterdam.\nIn addition the moderation analysis with continuous variables has been rewritten by Sharon Klinkenberg. The chapter now uses the PROCESS macro by Andrew Hayes to analyse the moderation. Manual calculation of dummy variables and interaction terms has been removed. New videaos have been recorded to explain the new approach.\nIn all chapters textual explanations have been added to the SPSS videos by Jolijn van den Hout to improve the readability and accessibility of the book.\n\n\nContribute\nIf you would like to contribute to the book, you can fork the GitHub repository and submit a pull request. The book is written in quarto markdown and compiled using the quarto package. The book is hosted on GitHub pages and the source material is maintained from his github repository. If you find any typo’s or errors, please submit an issue on the GitHub repository.\n\n\nLicense\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "crumbs": [
      "Colophon"
    ]
  },
  {
    "objectID": "14-references.html",
    "href": "14-references.html",
    "title": "References",
    "section": "",
    "text": "Bullock, John G., and Shang E. Ha. 2011. “Mediation Analysis Is\nHarder Than It Looks.” In Cambridge Handbook of Experimental\nPolitical Science, edited by James N. Druckman, Donald P. Green,\nJames H. Kuklinski, and ArthurEditors Lupia, 508–22. Cambridge\nUniversity Press. https://doi.org/10.1017/CBO9780511921452.035.\n\n\nCohen, Jacob. 1969. Statistical Power Analysis for the Behavioral\nSciences. San Diego, CA: Academic Press.\n\n\nCumming, Geoff. 2012. Understanding the New Statistics: Effect\nSizes, Confidence Intervals, and Meta-Analysis. New York:\nRoutledge.\n\n\nDavis, James A. 1985. The Logic of Causal Order. Beverly Hills,\nCA: Sage.\n\n\nde Groot, Adrianus Dingeman. 1969. Methodology:\nFoundations of Inference and\nResearch in the Behavioral Sciences.\nBook, Whole. The Hague: Mouton.\n\n\nDeci, Edward L., and Richard M. Ryan. 2013. Intrinsic\nMotivation and Self-Determination\nin Human Behavior. Springer Science &\nBusiness Media.\n\n\nEfron, B. 1979. “Bootstrap Methods: Another Look at the\nJackknife.” Ann.Statist. 7 (1): 1–26.\n\n\nEfron, Bradley. 1987. “Better Bootstrap Confidence\nIntervals.” Journal of the American Statistical\nAssociation 82 (397): 171–85. https://doi.org/10.1080/01621459.1987.10478410.\n\n\nFishbein, M., and J. N. Cappella. 2006. “The Role of Theory in\nDeveloping Effective Health Communications.” Journal of\nCommunication 56 (s1): S1–17. http://onlinelibrary.wiley.com/doi/10.1111/j.1460-2466.2006.00280.x/full.\n\n\nFisher, R. A. 1919. “The Correlation Between Relatives on the\nSupposition of Mendelian Inheritance.” Transactions of the\nRoyal Society of Edinburgh 52 (2): 399–433. https://doi.org/10.1017/S0080456800012163.\n\n\nFisher, Ronald Aylmer. 1955. “Statistical Methods and Scientific\nInduction.” Journal of the Royal Statistical Society.Series B\n(Methodological) 17 (1): 69–78. http://www.jstor.org/stable/2983785.\n\n\nHainmueller, Jens, Jonathan Mummolo, and Yiqing Xu. 2016. “How\nMuch Should We Trust Estimates from Multiplicative Interaction Models?\nSimple Tools to Improve Empirical Practice.” https://doi.org/10.2139/ssrn.2739221.\n\n\nHayes, Andrew F. 2013. Introduction to Mediation, Moderation, and\nConditional Process Analysis: A Regression-Based Approach. Guilford\nPress.\n\n\nHoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan\nWagenmakers. 2014. “Robust Misinterpretation of Confidence\nIntervals.” Psychonomic Bulletin & Review 21:\n1157–64.\n\n\nHolbert, R. Lance, and Esul Park. 2019. “Conceptualizing,\nOrganizing, and Positing Moderation in\nCommunication Research.” Communication\nTheory, April. https://doi.org/10.1093/ct/qtz006.\n\n\nLehmann, E. L. 1993. “The Fisher, Neyman-Pearson Theories of\nTesting Hypotheses: One Theory or Two?” Journal of the\nAmerican Statistical Association 88 (424): 1242–49. https://doi.org/10.1080/01621459.1993.10476404.\n\n\nNeyman, Jerzy. 1937. “Outline of a Theory of Statistical\nEstimation Based on the Classical Theory of Probability.”\nPhilosophical Transactions of the Royal Society of London.Series A,\nMathematical and Physical Sciences 236 (767): 333–80.\n\n\nO’Keefe, Daniel J. 2007. “Brief Report: Post Hoc Power, Observed\nPower, a Priori Power, Retrospective Power, Prospective Power, Achieved\nPower: Sorting Out Appropriate Uses of Statistical Power\nAnalyses.” Communication Methods and Measures 1 (4):\n291–99. https://doi.org/10.1080/19312450701641375.\n\n\nRogers, R. W. 1975. “A Protection\nMotivation Theory of Fear\nAppeals and Attitude Change1:\nThe Journal of Psychology:\nVol 91, No 1.” THe Journal of\nPsychology 91 (1): 93–114. https://www.tandfonline.com/doi/abs/10.1080/00223980.1975.9915803?casa_token=tnl4hHdJsB0AAAAA:T1RIciCaIhUSOxB2By3Ah9nZPbufTPANLbbdgCujfE-yZr6cugamG3pBECgsGroUDXf02-k1XlEzOw.\n\n\nSawilowsky, Shlomo. 2009. “New Effect Size Rules of\nThumb.” Journal of Modern Applied Statistical\nMethods 8 (2). https://doi.org/10.22237/jmasm/1257035100.\n\n\nWasserstein, Ronald L., and Nicole A. Lazar. 2016. “The ASA\nStatement on p-Values: Context,\nProcess, and Purpose.” The American\nStatistician 70 (2): 129–33. https://doi.org/10.1080/00031305.2016.1154108.\n\n\nWilkinson, Leland. 1999. “Statistical Methods in Psychology\nJournals: Guidelines and Explanations.” American\nPsychologist 54 (8): 594.",
    "crumbs": [
      "References"
    ]
  }
]